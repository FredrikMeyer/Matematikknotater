\documentclass[11pt, english]{article}
%\documentclass[12pt]{article}  
%\usepackage[papersize={108mm,144mm},margin=2mm]{geometry}  
\sloppy 
%\pagestyle{empty} 
%\usepackage[scaled]{helvet}
%\renewcommand{\familydefault}{\sfdefault}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}   % S P R A A K

\usepackage{amssymb, amsmath, amsthm, amssymb} % symboler, osv
\usepackage{mathrsfs,calligra}
\usepackage{url}
\usepackage{thmtools}
\usepackage{enumerate}  % lister $
\usepackage{float}
\usepackage{tikz}

\usepackage{young} 
\usepackage{youngtab} 

\usepackage[all]{xy}   % for comm.diagram
\usepackage{wrapfig} % for float right
\usepackage[colorlinks=true]{hyperref}
\usepackage{mystyle} % stilfilen      


\title{Representation theory}
\author{Fredrik Meyer} 
\date{}
\begin{document}
\maketitle

\abstract{These are notes from the course MAT4270 on representation theory, autumn 2015. The lectures were held by Sergey Neshyevey. The notes are mine, as well as any mistakes. The first half is about representations of finite groups, the second half about representations of compact Lie groups.
}

\section{Introduction to representation theory}

In the first part, $G$ is always a \emph{finite group} and $V$ a finite-dimensional vector space over the complex numbers. Most results will hold over any algebraically closed field of characteristic zero.

\subsection{Motivating example}

In 1896 Dedekind made the following observation. Let $\C[x_g \mid g \in G]$ be the free $\C$-algebra with basis the elements of $G$. Then one can form the following polynomial:
\[
P_G(x_{\rho_1},\ldots,x_{\rho_n}) = \det \left( (x_{\rho_i g_j})_{i,j=1}^n \right).
\]

Note that the matrix is just the multiplication table for the group $G$ if we identify the variable $x_g$ with the group element $g$. The problem is to decompose $P_G$ into irreducible polynomials. 

\begin{example}
Let $G=S_3$. Then $G$ is generated by two elements $r$ and $s$ with the relation $srs=r^2$. If we write the elements of $G$ as $\{e,r,r^2,s,sr,sr^2\}$, then the multiplication table looks like
 $$ \bgroup\begin{pmatrix}
e&     r&     r^2&     s&     sr&     sr^2\\
     r&    r^2&     e&    sr^2&     s&     sr\\ 
    r^2&     e&     r&     sr&     sr^2&     s\\ 
    s&     sr&    sr^2&     e&     r&    r^2\\ 
    sr&     sr^2&     s&    r^2&    e&     r\\
  sr^2&     s&   sr&     r&    r^2&     e\\
     \end{pmatrix}\egroup.$$
The determinant is quite long, so I won't write it out. Suffice it to say it is a degree $6$ polynomial with 146 terms. Using a computer algebra system such as \verb|Macaulay2|, one can decompose it. It decomposes into three factors: one quadratic and two linear. They are the following (we change notation to avoid confusion about exponents):
\[
x_e+x_r+x_{r^2}+x_s+x_{sr}+x_{sr^2}
\]
and 
\[
x_e+x_r+x_{r^2}-x_s-x_{sr}-x_{sr^2}
\]
and 
\[
x_e^2-x_ex_r+x_{r^2}-x_e x_{r^2}-x_r x_{r^2} + x_{r^2}^2-x_s^2+x_s x_{sr} - x_{sr^2}^2 + x_s x_{sr^2}-x_{sr^2}^2-x_{sr^2}^2.
\]
We will see later that the first factor corresponds to the trivial representation, the second to the alternating representation, and the third is the so-called standard representation of $G$.
\end{example}

Frobenius proved the following theorem:
\begin{thm}
  Let $P_G= \prod_{i=1}^r P_i^{m_i}$ be the decomposition of $P_G$ into irreducibles. Then
  \begin{enumerate}
  \item $m_i=\deg P_i$ for every $i$.
\item $r$ is the number of conjugacy classes in $G$. 
  \end{enumerate}
In particular, since $r=\lvert G \rvert $ if and only if $G$ is abelian, $P_G$ decomposes into linear factors if and only if $G$ is abelian.
\end{thm}

In trying to prove the above theorem, Frobenius basically  had to develop representation theory.

\subsection{Representations}

A \textbf{representation of $G$ on $V$} is a homomorphism $\pi:G \to \GL(V)$. We will denote a representation interchangably by $\pi, (\pi,V), V_\pi$ or $V$, depending upon the situation. We sometimes also say that $V$ is a \textbf{$G$-module}.

Thus a representation is a \emph{linear} action of $G$ on $V$, so that the action of $G$ on $V$ always can be represented as some group of matrices. This matrix group is isomorphic to $G$ if the action is \textbf{faithful}.

\begin{example}
  The \textbf{trivial representation} $\epsilon$ of any group $G$ is given by letting $V= \C$ and $\epsilon(g)=e$ for any $g \in G$.
\end{example}

\begin{example}
We define the \textbf{left regular representation}. Let $\C[G]$ be the space of functions on $G$. It is a finite-dimensional vector space under pointwise addition. Define $\lambda:G \to \GL(\C[G])$ by
\[
g \mapsto \left(f \mapsto (h \mapsto f(g^{-1}h)) \right)
\]
This vector space have a basis consisting of the characteristic functions $e_g$ for $g \in G$. On these elements on sees that the action is given by $g \cdot e_h = e_{gh}$. 

Thus any group have at least one non-trivial representation.
\end{example}

\begin{example}
Similarly, one has the \textbf{permutation representation}. Let $X$ be a $G$-set, that is, a set on which $G$ act by permutations. Then one forms the space $\C[X]$ of $\C$-valued functions on $X$. It has a basis $e_x$ of characteristic functions, and the action of $G$ on $\C[X]$ is given by $g e_x = e_{gx}$. 
\end{example}

As in any reasonable category, there is a notion of \emph{isomorphism} of representations. An \textbf{equivalence} of representations $(\pi,V)$ and $(\theta,W)$ is given by a vector space isomorphism $T:V \to W$ such that $T(g \cdot v) = g \cdot T(v)$ for all $g \in G$.

A map $T:V \to W$ satisfying the last condition above is called an \textbf{intertwiner} of $V$ and $W$. The set of intertwiners is denoted by $\Mor(\pi,\theta)$ or $\Hom_G(V,W)$.

A subspace $W \subset V$ is \textbf{invariant} if $g \cdot W \subset W$ for all $g \in G$. Letting $\theta(g) = \restr{\pi(g)}{W}$, we get another representation of $G$, called a \textbf{subrepresentation} of $G$. We write $\restr{\pi}{W}$ for $\theta$. 

If we have two representation $(\pi,V)$ and $(\pi',V')$, we can form the \textbf{direct sum representation} by letting $g \in G$ act on $V \oplus V'$ componentwise. Note that $\pi$ is a subrepresentation of $V \oplus V'$.

The following proposition has important consequences. Note that the proof works for any field of characteristic not dividing $\lvert G \rvert$. 

\begin{prop}[Mascke's theorem]
\label{propmaschke}
Let $(\pi,V)$ be a representation of $G$ and $W \subset V$ an invariant subspace. Then there exists a complementary invariant subspace $W^\perp$. That is, $W$ is also invariant and such that $V = W \oplus W^\perp$.
\end{prop}

\begin{proof}
  We prove this by ``averaging'', a process we will do again and again. Let $P:V \to W$ be \emph{any} projection from $V$ to $W$. Then define 
\[
\Pi(v) = \frac 1{\lvert G \rvert} \sum_{g \in G} g P(g^{-1}v).
\]
This is a $G$-linear morphism, because
\begin{align*}
\Pi(h\cdot v) &= \frac 1{\lvert G \rvert} \sum_{g \in G} g P(g^{-1} h \cdot v) \\  
&=  \frac 1{\lvert G \rvert} \sum_{g \in G} g P((h^{-1}g) \cdot v) \\
&=  \frac h{\lvert G \rvert} \sum_{g \in G} h^{-1}g P((h^{-1}g) \cdot v) = h \cdot \Pi(v).
\end{align*}
It is also surjective, since if $v \in W$, then clearly $\Pi(v)=v$. Hence $\Pi$ is a $G$-linear surjection onto $W$. Then it is clear that $\ker \Pi$ is a $G$-invariant subspace of $V$ complementary to $W$.
\end{proof}

We say that a representation is \textbf{irreducible} if it has no proper invariant subspaces. A representation is \textbf{completely reducible} if it decomposes into a direct sum of irreducible representations.

\begin{example}
  Let $G = (\R,+)$ act on $\R^2$ by the matrices
\[
\begin{pmatrix}
1 & a \\ 
0 & 1
\end{pmatrix}.
\]
This leaves the $x$-axis fixed, so the representation is not irreducible. But it does not split into irreducible representations, since there is no complementary subspaces.
\end{example}

\begin{example}
 The same example with $G=(\F_p, +)$ and $V=\F_p^2$ works. Again, the space spanned by the vector $(1,0)^T$ is invariant, so $V$ is not irreducible. The same computation as in the previous example shows that are no complementary invariant subspace.
\end{example}

\begin{example}
Let $G=\Z/2$ act on $V=\F_p^2$ by $(x,y) \mapsto (y,x)$. Then the diagonal $\Delta$ is an invariant subspace. Letting $P:\F_p^2 \to \F_p^2$ be the map $(x,y) \mapsto (x,x)$ be a projection onto $\Delta$, going through the proof, we see that 
$$
\Pi((x,y)) = \frac 12 \left( (x,x) + (y,y) \right) = \frac 12 \left(x+y, x+y \right)
$$
is an intertwiner. The kernel is spanned by $(1,-1)^T$ if $p \neq 2$, so we see that $V$ is completely reducible if and only if $p \neq 2$. 
\end{example}

However, we don't worry, because we work over $\C$. Here things are nicer:

\begin{thm}
Any finitely-dimensional representation of a finite group $G$ is completely reducible.
\end{thm}
\begin{proof}
The proof is a direct consequence of Maschke's theorem \eqref{propmaschke}.  Just inductively find invariant subspaces. Since the representation is finite-dimensional, the process must stop.
\end{proof}

We next prove Schur's lemma, which although it has a very simple proof, has important consequences.

\begin{prop}[Schur's lemma]
Let $(V,\pi)$ and $(W,\theta)$ be irreducible representations. Then
\begin{enumerate}
\item If $\pi \not \sim \theta$, then $\Hom_G(V,W)=0$.
\item If $\pi \sim \theta$, then $\Hom_G(V,W)$ is $1$-dimensional. 
\end{enumerate}
\end{prop}
\begin{proof}
Both the kernel and the image of a morphism of representations is a representation. But since $V$ and $W$ are irreducible, it follows that $\ker \varphi = 0$ or $\ker \varphi  = V$ for any $\varphi:V \to W$. This proves i). 

Now suppose $\Pi:V \to W$ is an equivalence of representations. Let $\varphi \in \Hom_G(V,W)$ be non-zero. Then $\varphi$ must be an equivalence as well, since the neither the kernel nor image can be non-zero. Consider $T=\varphi^{-1} \circ \Pi: V \to V$. This is a linear map, and since we work over $\C$, it has an eigenvalue $\lambda$. Now consider $T-\lambda \cdot \id_V$. This has non-zero invariant kernel, hence $T-\lambda \id_V=0$. 
\end{proof}

\begin{prop}
 If $G$ is abelian, then any irreducible representation of $G$ is 1-dimensional.
\end{prop}
\begin{proof}
Each $g \in G$ gives a map $\pi(g):V \to V \in \End(V)$. This is an intertwining map, because $\pi(g)\pi(h)=\pi(gh)=\pi(hg)=\pi(h)\pi(g)$.

If $\pi$ is not trivial, by Schur's lemma, this map is just multiplication by some constant. But any 1-dimensional subspace of $V$ is invariant under this action, so $V$ must be 1-dimensional in order to be irreducible.
\end{proof}

Thus, up to equivalence, every irreducible representation of a finite abelian group is just a homomorphism $\chi:G \to \C$. But since $G$ is finite, this actually maps into $\mathbb T := \{ z \in \C \mid \lvert z \rvert = 1 \}$. Such maps are called \textbf{characters} of $G$.

Denote by $\widehat G$ the \emph{set} of irreducible representations of $G$. We call  $\widehat G$ the \textbf{Pontryagin dual} of $G$. It is an abelian group under pointwise multiplication.

\begin{remark}
  If $G$ is non-abelian, there will always exist irreducible representations of dimension $\geq 2$. This follows because the regular representation $G \to \GL \left( \C[G] \right)$ is injective.
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Density and orthogonality}


\subsection{Density theorems}

Let $(V,\pi)$ be an irreducible representation. The set of operators $\pi(g)$ for $g \in G$ is quite large. By definition, if $x \neq 0$, then
$$
V = \mathrm{Span} \{ \pi(g)x \mid g \in G \}.
$$
By Schur's lemma, we also know that 
$$
\End(\pi) = \{ T \in \End(V) \mid T\pi(g) = \pi(g) T \, \forall \, g \in G \} = \C \cdot \id_V.
$$

\begin{remark}
Over any field, $\End(\pi)$ is a division algebra over that field. But if the field is algebraically closed, any division algebra is isomorphic to the field itself.
\end{remark}



\begin{thm}[The density theorem]
\label{thmdensity}
 Assume $\pi_1,\ldots,\pi_n$ are pairwise inequivalent irreducible representations, with $V_i = V_{\pi_i}$. Consider the direct sum representation $V_1 \oplus \ldots \oplus V_n$.

Then
\[ 
\Span {\pi(G)} = \End(V_1) \oplus \ldots \oplus \End(V_n).
\]
\end{thm}
\begin{proof}
Read the proof of Theorem \ref{seconddensity} first. First break $(V,\pi)$ into irreducible representations $V_i$. It follows directly from Schur's lemma that $\pi(G)' = \C \id_{V_1} \oplus \ldots \oplus \C \id_{V_n}$. 

Now, the elements of $\C \id_{V_1} \oplus \ldots \oplus \C \id_{V_2}$ are block diagonal scalar matrices. It is an easy exercise with matrices to see that the commutator of this set is just
$$
\End(V_1) \oplus \ldots \oplus \End(V_n).
$$
This proves the theorem.
\end{proof}

First we need some notation. Let $V$ be some vector space and $X \subset \End(V)$ a subset of the endomorphisms of $V$. Let 
\[
X' \stackrel{\Delta}{=}  \{ T \in \End(V) \mid TS = ST \text{ for all } S \in X \}.
\]
We call $X'$ the \textbf{commutant} of $X$. Thus if $(V,\pi)$ is a representation, then by definition:
\[
\End(\pi) = \pi(G)'.
\]

\begin{thm}
\label{seconddensity}
 For any finite-dimensional representation $(V,\pi)$ we have 
\[
\pi(G)'' = \Span{\pi(G)}.
\]
\end{thm}
\begin{proof}
The inclusion $\supseteq$ is clear, since $\pi(G)''=\End(\pi)'$, and we have $\pi(g) S(v) = \pi(g) (Sv) = S (\pi(g)v)$ for all $S \in \End(\pi)$.

For the other inclusion, let $T \in \pi(G)''$. Let $x_1,\ldots,x_n$ be a basis of $V$. Consider the representation $\theta = \pi \oplus \ldots \oplus \pi$ (n times) on $W = \oplus_{i=1}^n V$. Let $S$ be the operator $T \oplus \ldots \oplus T$ on $W$. 

Then $S \in \theta(G)''$, because 
\begin{align*}
  (S\theta )(v) &= S( \theta \cdot (v_1,\ldots,v_n)) \\
&= S( \pi(v_1),\ldots, \pi(v_n)) \\
&= (T\pi(v_1),\ldots, T\pi(v_n)) \\
&= (\pi(Tv_1), \ldots, \pi(Tv_n)) \\
&= \theta \cdot ( Tv_1,\ldots, Tv_n) \\
&= \theta  Sv.
\end{align*}

Let $x=(x_1,\ldots,x_n) \in W$. Then $\Span{\theta (G) x }$ is an invariant subspace of $W$. Then there exists an invariant complementary subspace. Let $P:W \to \Span{\theta(G)x}$ be the projection with kernel that invariant subspace, so that $P \in \Theta(G)'$ (that is, $P$ is a $G$-morphism). Then
$$
PSx = SPx = Sx
$$
since $P x \in \Span{\theta(G)x}$. Hence $Sx \in \Span { \theta(G)x }$ as well. This means that there exist $\alpha_g \in \C$ such that
$$
Sx = \sum_g \alpha_g \theta(g)x.
$$
This is the same as saying that
$$
Tx_i = \sum_g \alpha_g \pi(g)x_i,
$$
hence $T = \sum \alpha_g \pi(g)$, so $T \in \Span \pi(G)$.
\end{proof}

\begin{remark}
\label{remarksum}
Note that any irreducible representation is contained in the regular representation. Indeed, let $(V,\pi)$ be irreducible, and pick some non-zero $x \in V$. Then the map
$$
\C[G] \xrightarrow{\cdot x} V
$$
must be surjective, and by for example Schur's lemma it has a section. Thus the density theorem says that 
$$
\lvert G \rvert ^2  = \sum_{\overline{\pi} \in \widehat G} \lvert V_i \rvert^2.
$$
In particular, there are only finitely many isomorphism classes of representations of $G$.
\end{remark}

\subsection{Orthogonality relations}

Let $(V,\pi)$ be a representation and let $\rho \in V^\ast = \Hom_\C(V,\C)$. The function $\alpha_{\rho,x}^\pi$ on $G$ defined by $\alpha_{\rho,x}^\pi(g)=\rho(\pi(g)x)$ is called a \textbf{matrix coefficient} of $\pi$. If we fix a basis $e_1,\ldots,e_n$ of $V$ and consider the dual basis $e_1^\ast,\ldots, e_n^\ast$ of $V^\ast$, then we write $\alpha_{ij}^\pi$ instead of $\alpha_{e_i^\ast,e_j}^\pi$.

In the basis $e_1,\ldots,e_n$ we have
$$
\pi(g) = \left( \alpha_{ij}^\pi(g) \right)_{ij} \in GL(V).
$$

The following theorem is usually called the \textbf{orthogonality relations}:
\begin{thm}
 We have:
 \begin{enumerate}
 \item If $\pi$ and $\theta$ are inequivalent irreducible representations, then
\[
\frac 1{\lvert G \rvert} \sum_{g \in G} \alpha_{f,x}^\pi(g) \alpha_{\rho,y}^\theta(g^{-1}) = 0
\]
for all $f \in V_\pi^\ast$, $\rho \in V_\theta^\ast$, $x \in V_\pi$ and $y \in V_\theta$.
\item If $\pi$ is irreducible, then
\[
\frac{1}{\lvert G \rvert} \sum_g \alpha_{f,x}^\pi(g) \alpha_{\rho,y}^\pi (g^{-1}) = \frac{f(y) \rho(x)}{\dim \pi}.
\]
 \end{enumerate}
\end{thm}
\begin{proof}
It is the averaging trick again. First observe that if $T:V_\theta \to V_\pi$ is any linear operator, then 
\[
S = \frac{1}{\lvert G \rvert} \sum_{g \in G} \pi(g) T \theta(g^{-1}) 
\]
is in $\Hom_G(\theta,\pi)$ by averaging. Thus, since $\pi$ and $\theta$ are inequivalent, it follows from Schur's lemma that this must be zero.

Now let $T(v) = \rho(v)x$. This is a linear operator. Then
\begin{align*}
0 = f(Sy) &= f\left( \frac{1}{\lvert G \rvert} \sum_{g \in G} \pi(g) T \theta(g^{-1})(y) \right) \\
&= \frac{1}{\lvert G \rvert} \sum_{g \in G} f(\pi(g)\rho(\theta(g^{-1})y)x) \\
&= \frac{1}{\lvert G \rvert} \sum_{g \in G} f(\pi(g)x) \rho(\theta(g^{-1})y) \\
&= \frac{1}{\lvert G \rvert} \sum_{g \in G} \alpha_{f,x}^\pi (g) \alpha_{\rho,y}^\theta (g^{-1})
\end{align*}
This proves part $1$.

Now suppose $\pi=\theta$. Then $S \in \End(\pi) = \C \cdot \id_V$. Thus $S = \alpha \cdot \id_V$ for some $\alpha \in \C$. Taking traces on both sides, we get $\alpha = \tr T/\dim \pi$ (note that $T$ and $S$ have the same trace).

Then
$$
\frac 1{\lvert G \rvert} \sum_g \alpha_{f,x}^\pi(g) \alpha_{\rho,y}^\pi(g^{-1}) = f(S(y)) = f(y) \frac{\tr T}{\dim \pi} = \frac{f(y)\rho(x)}{\dim \pi},
$$
since $\tr T = \rho(x)$. 
\end{proof}

Let $\C[G]$ denote the vector space of functions $G \to \C$. Instead of writing expressions like $\sum_g \alpha_g \pi(g)$ it is convenient to introduce the following notation. Define the \textbf{convolution} of two functions $f_1,f_2 \in \C[G]$ by
$$
(f_1 \ast f_2)(g) = \sum_{g=g_1g_2} f_1(g_1)f_2(g_2) = \sum_{h \in G} f_1(h)f_2(h^{-1}g).
$$
We have $\delta_{g_1} \ast \delta_{g_2} = \delta_{g_1g_2}$ (easy check!). The convolution product is associative and makes $\C[G]$ into an algebra, called the \textbf{group algebra} of $G$.

Note that with this product, $\C[G]$ could just as well have been defined as the associative algebra $\C\langle x_g  \, \mid \, g \in G \rangle $ module the relations defined by $x_gx_h = x_{gh}$. 

Given a representation $\pi:G \to \GL(V)$ we can define an algebra homomorphism $\C[G] \to \End(V)$ by $\pi(f) = \sum_{g \in G} f(g) \pi(g)$. Thus $V$ becomes a left $\C[G]$-module. Conversely, given any unital algebra homomorphism $\pi:\C[G] \to \End(V)$, we get a representation by $g \mapsto \pi(\delta_g)$. Thus we have a correspondence between representations of $G$ and left $\C[G]$-modules. Using these notions, the Density Theorem (Theorem \ref{thmdensity}) becomes $\pi(\C[G]) = \oplus_i \End(V_i)$.

Formalizing Remark \ref{remarksum}, we get:

\begin{thm}
Let $\widehat G$ be the set of equivalence classes of irreducible representations of $G$. Then $\widehat G$ is finite and $\sum_{[\pi] \in \widehat G} (\dim \pi)^2 = \lvert G \rvert$. Furthermore, we have
$$
\lambda \sim  \bigoplus_{[\pi] \in \widehat G} \pi^{\oplus \dim \pi}.
$$
Here $\lambda$ is the left regular representation of $G$.
\end{thm}
\begin{proof}
The first two statements are already proved in the Remark. The only thing to be proved is the part about the multiplicity of $\pi$ in $\lambda$. 

Note that as a representation, $\C[G]$ is just the left regular representation of $G$. Note also that $\End V_i \simeq V^{\dim V}$: let $\{ v_1, \ldots , v_n\}$ be a basis of $V$. Then the map $\End(V_i) \ni \varphi \mapsto (\varphi(v_1), \ldots, \varphi(v_n))$ is an isomorphism of representations. 

Then the density theorem implies the statement.
\end{proof}

\begin{example}
Let $G=S_3$, the symmetric group on $3$ elements. We have that $\lvert G \rvert = 6$. We have two one-dimensional representations: the trivial representation and the sign representation $g \mapsto sign(g) \in \{ \pm 1 \}$. Hence $6=1^2+1^2+n_1^2+n_2^2+\ldots$, where $n_i$ is the dimension of the $i$th irreducible representation. This is only possible if $n_1=2$ and $n_i=0$ for $i >1$. Thus there are only three irreducible representations of $S_3$.

Let $G=\langle \rho, \sigma \rangle$ with $\rho^3=e$, $\sigma^2=e$ and $\sigma \rho \sigma = \rho^{-1}$. The unique irreducible $2$-dimensional representation of $S_2$ can be realized as the action on $\C^2$ given by 
$$
\rho \mapsto \begin{pmatrix}
\sin\left( \frac{2 \pi}{3} \right) & - \sin \left( \frac{2 \pi}{3} \right) \\
\cos \left( \frac{2\pi}{3} \right) & \cos \left( \frac{2 \pi}{3} \right)
\end{pmatrix}
$$
and 
$$
\sigma \mapsto \begin{pmatrix}
-1 & 0 \\
0 & 1 
\end{pmatrix}.
$$
The density theorem here says that 
$$
\C[G] = e \C \oplus \ldots
$$
[[sett inn dekomposisjon av $\C[G]$ som algebra]]
\end{example}

We will now motivate the name "orthogonality relations". First of all, note that if $V_\pi$ is a representation of a finite group $G$, and $\langle,\rangle'$ is \emph{any} Hermitian scalar product on $V_\pi$, then 
$$
\langle v, w \rangle := \frac{1}{\langle G \rangle} \sum_{g \in G} \langle \pi(g) v, \pi(g) w \rangle'
$$
is another Hermitian scalar product on $V_\pi$, making the representation \textbf{unitary}, meaning that the inner product is invariant under the action of the group.

If $\pi$ is any unitary representation, choose an orthonormal basis $e_1,\ldots,e_n \in V$. Then the matrices $\pi(g) = \left( a_{ij}^\pi(g) \right)$ are unitary matrices. Since for unitary matrices $U$ we have $U^{-1}=U^\star$ (conjuge transpose), we have $\overline{a_{ij}^\pi(g)} = a_{ji}^\pi(g^{-1})$.

There is a standard scalar product on $\C[G]$ defined by
$$
(f_1,f_2) = \frac 1{\lvert G \rvert} \sum_{g \in G} f_1(g) \overline{f_2(g)}.
$$

Then the orthogonality relations take the form:

\begin{thm}
 For every irreducible representation $\pi$ choose an invariant scalar product and an orthonormal basis $\{e_1^\pi, \ldots, e_n^{\dim \pi}\}$ and corresponding matrix coeffecients $a_{ij}^\pi$. Then the functions $a_{ij}^\pi$ are mutually orthogonal with
$$
\left (a_{ij}^\pi, a_{ij}^\pi\right) = \frac{1}{\dim \pi}.
$$
\end{thm}
\begin{proof}
This is just a reformulation of the Theorem. 
\end{proof}

Let $(V,\pi)$ be an irreducible representation of $G$ and $(W,\theta)$ any finite-dimensional representation. Denote by $W(\pi)$ the space spanned by elements $Tx$ for $T \in \Mor(\pi,\theta)$. It is an invariant subspace of $W$, called the \textbf{isotypic component} of $\theta$ corresponding to $\pi$. 


\begin{lemma}
The operator $P:W \to W$ defined by
$$
P = \frac{\dim \pi}{\lvert G \rvert} \sum_{g \in G} \chi_\pi(g^{-1})\theta(g),
$$
where $\chi_\pi(g)=\tr \pi(g)$, is a projection onto $W(\pi)$ along $\sum_{[\pi'] \in  \widehat G  \bs \{ [\pi] \}} W(\pi')$. In particular, $$
W = \bigoplus_{[\pi] \in \widehat G} W(\pi)
$$
and 
$$
\restr{\theta}{W(\pi)} \sim \pi^{\oplus n_\pi},
$$
where $n_\pi$ is the \emph{multiplicity} of $\pi$ in $\theta$.
\end{lemma}

\begin{proof}
For now, see Theorem 8, page 2.6 in \cite{serre_linrep}.
\end{proof}

\subsection{Decomposition of the regular representation}

We want to decompose the left regular representation $\lambda$ into irreducibles.

It is sometimes more convenient to work with $\rho$, the right regular representation. Note that the map $\C[G] \to \C[G]$ defined by $\delta_g \mapsto \delta_{g^{-1}}$ defines an equivalence (check!)\footnote{Recall that the right regular representation is defined by $(f \cdot h )(k) = f(hk)$}. 

The followig theorem was already proved using the density theorem, but we now get a more explicit proof.

\begin{thm}
Every irreducible representation $\pi$ of $G$ has multiplicity $\dim \pi$ in the regular representation, so
$$
\lambda \sim \rho \sim \bigoplus_{[\pi] \in \widehat G} \pi^{\oplus \dim \pi}.
$$
\label{thmspan}
\end{thm}

\begin{proof}
Fix a basis $v_1,\ldots,v_n$ of $V_\pi$. In matrix form the identity $\rho(gh)=\rho(g)\rho(h)$ takes the form 
$$
a_{ij}^\pi(gh) = \sum_k a_{ik}^\pi(g) a_{kj}^\pi (h). 
$$
Or equivalently,
$$
\rho(h) a_{ij}^\pi  = 	\sum_k a_{kj}^\pi (h) a_{ik}^\pi.
$$
Therefore, for every $i$, the map 
$$
V_\pi \to \C[G], \qquad e_j \mapsto a_{ij}^\pi  \quad (1 \leq j \leq n)
$$

is an intertwiner between $\pi$ and $\rho$.

Assume now that $V \subset \C[G]$ is a $\rho$-invariant subspace such that $\restr{\rho}{V} \sim \pi$. Choose a basis $f_1,\ldots, f_n$ on $V$ such that 
$$
\rho(h) f_j = \sum_k a_{kj}^\pi(h) f_k,
$$
so 
$$
f_j(gh) = \sum_k a_{kj}^\pi(h) f_k(g).
$$
Thus
$$
f_j = \sum_k f_k(e) a_{kj}^\pi
$$
by setting $g=e$.

We thus conclude that the isotypic component corresponding to $\pi$ equals $\Span \{ a_{ij}^\pi \}_{i,j=1}^n$. The functions $a_{ij}^\pi$ are linearly independent by the orthogonality relations, hence their span have dimension $n^2$, and the multiplicity of $\pi$ in $\rho$ is $n$.
\end{proof}

\begin{corr}
There are only finitely many equivalence classes of representations of $G$, and 
$$
\sum_{[\pi] \in \widehat G} (\dim \pi )^2 = \lvert G \rvert.
$$
\end{corr}



%%%%%%%%%%%%%%%
\newpage
\section{Character theory}

Let $(\pi,V)$ be a representation. 

The function $\chi_\pi:G \to \C$ defined by $\tr \pi(g)$ is called the \textbf{character of $\pi$}.

We say that a function $f:G \to \C$ is \textbf{central} if $f(hgh^{-1})=f(g)$ for all $h,g \in G$. Note that characters are central, by standard properties of traces. Also note that $\chi_\pi(e)=\dim \pi$. 

\begin{thm}
 The characters $\chi_\pi$ (for $[\pi] \in \widehat G$) form an orthonormal basis for the space of central functions on $G$.
\end{thm}
\begin{proof}
 We have
$$
\chi_\pi(g) = \sum_{i=1}^{\dim \pi} a_{ii}^\pi(g),
$$
and the orthogonality relations show that $\{ \chi_\pi \mid [\pi] \in \widehat G \}$ is an orthonormal system:
$$
(\chi_\pi, \chi_\pi) = \sum_{ij}^{\dim \pi} (a_{ii}^\pi, a_{jj}^\pi) = 
\sum_{i=1}^{\dim \pi} \frac{1}{\dim \pi} = 1.
$$

Thus we need to show that the characters \emph{span} the space of central functions. Consider the projection $P$ from $\C[G]$ to the space of central functions defined by
$$
f \mapsto (Pf)(g) = \frac{1}{\lvert G \rvert }\sum_{h \in G} f(hgh^{-1}).
$$
It is obviously a projection. Now consider $P(a_{ij})(g)$ (we skip the upper index $\pi$). Then this is equal to
$$
\frac{1}{\lvert G \rvert} \sum_{h \in G} a_{ij}(hgh^{-1}) = \frac{1}{\lvert G \rvert} \sum_{h \in G} \sum_k \sum_l a_{ik}(h) a_{kl}(g) a_{lj}(h^{-1}).
$$
But after using that the orthogonality relations, this simplifies to
$$
\frac{1}{\dim \pi} \delta_{ij} \chi_\pi(g).
$$
But the $a_{ij}^\pi$ constitute a basis of $\C[G]$, by the proof of Theorem \ref{thmspan}.

Thus $\mathrm{image}(P)= \Span \{ \chi_\pi \}$.
\end{proof}

The dimension of the space of central functions coincides with the number of conjugacy classes $c(G)$ in $G$. We thus have:

\begin{corr}
Let $c(G)$ be the number of conjugacy classes in $G$. We have
$$
\lvert \widehat G \rvert = c(G).
$$
\end{corr}



\begin{corr}
 Two finite-dimensional representations $\pi$ and $\theta$ are equivalent if and only if $\chi_\pi = \chi_\theta$.
\end{corr}
\begin{proof}
  You can read off the multiplicities from the character.
\end{proof}

\begin{corr}
 For any $[\pi] \in \widehat G$, the multiplicity of $\pi$ in the regular representation $\lambda$ equals $\dim \pi$.
\end{corr}
\begin{proof}
 We are interested in $(\chi_\lambda, \chi_\pi)$. But note that
$$
\chi_\lambda(g) = \sum_{h \in G} \tr ( \delta_h \mapsto \delta_{hg} ) = \begin{cases} 0 & \text{ if } g \neq e \\
\lvert G \rvert & \text{ if } g = e.
\end{cases}
$$
Then $(\chi_\lambda,\chi_\pi) = \frac{1}{\lvert G \rvert} \chi_\lambda(e) \cdot \overline{\chi_\pi(e)} = \dim \pi$.
\end{proof}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{The Frobenius determinant problem}

Let $G$ be a finite group and $\C[x_g \mid g \in G]$ the algebra on generators indexed by $g \in G$. Then one can form the following determinant:
$$
P_G  = \det \left( (x_{gh})_{gh} \right).
$$
It is the determinant of the multiplication table of $G$. The problem is to decompose $P_G$ into irreducible polynomials.

It is conventient to instead work with $\widetilde{P_G}=\det( {(x_{gh^{-1}})}_{gh})$. This has the same determinant as $P_G$ up to multiplication by $\pm 1$ since this just corresponds to permuting some of rows of the multiplication table.

Now here comes the \emph{key} observation: the matrix $(x_{gh^{-1}})$ is the matrix of the operator 
$$
\sum_{g \in G} x_g \lambda(g)
$$
in the basis $\delta_g \in \C[G]$. Here you are supposed to view elements of $\C[x_g \mid g \in G]$ as \textbf{functions} on $G$.

That this operator actually has matrix $(x_{gh^{-1}})$ can be seen by applying the operator to basis elements:
$$
\left(\sum_{g \in G} x_g \lambda(g)\right)(\delta_h) = \sum_{g \in G} x_g \delta_{gh}
$$
by definition of the left regular action. Reparametrizing this last sum by setting $g' = gh$ we get
$$
\sum_{g \in G} x_g \delta_{gh} = \sum_{g' = gh} x_{gh^{-1}} \delta_g,
$$
as wanted.

Now, the regular representation decomposes as
$$
\lambda \sim \bigoplus_{[\pi] \in \widehat G}  \pi^{\oplus \dim \pi}
$$
Hence the determinant decomposes as well (since the operator is $G$-invariant) and we get
$$
\widehat{P_G} = \prod_{[\pi] \in \widehat G} \det\left( \sum_{g \in G} x_g \pi(g) \right)^{\dim \pi}
$$

The question is now of course if we can decompose further. The answer is no:

\begin{thm}[Frobenius]
The polynomials $\det\left(\sum_{g \in G} x_g \pi(g) \right)$ ($[\pi] \in \widehat G$) are irreducible and no two of them are associates.
\end{thm}

We begin with a lemma:
\begin{lemma}
 The polynomial $p=\det (x_{ij})$ is irreducible.
\end{lemma}

\begin{proof}
Let $X=(x_{ij})$. Note that since $p$ is linear in each variable, $p$ is a sum of monomials with no square terms. Suppose for contradiction that $\det X = fg$ and that any determinant of a $(n-1) \times (n-1)$ matrix is irreducible (by induction).

Then one of $f,g$ must be independent of $x_{nn}$, say $f$. Then $g$ is linear in $x_{nn}$, hence can be written as $g=x_{nn}a+b$, where $a,b$ are polynomials in which $x_{nn}$ does not appear. Hence
$$
\det X = fg = x_{nn}af+bf.
$$
Now let $Y$ be the square matrix obtained from $X$ by removing the last row and column. By induction $\det Y$ is irreducible, and have coefficient $x_{nn}$ in the expansion of $\det X$. Hence $af = \det Y$. Hence either $a=1$ and $f=\det Y$ or $a=\det Y$ and $f = 1$. The last option would show that $\det X$ is irreducible. So assume that $a=1$ and $f = \det Y$. Then $\det X = x_{nn} \det Y + b\det Y $.

This leads to a contradiction: let $M$ be the matrix $(\delta_{i,n-i})$. Then $\det Y=0$ but $\det X \neq 0$.
\end{proof}

\begin{proof}[Proof of theorem:]
  
Suppose that $P_\pi=p_1p_2$. The polynomials $p_1,p_2$ are homogeneous. Fix a basis $e_1,\ldots,e_n$ in $V_\pi$. Consider the corresponding matrix units $m_{ij}$ defined by
$$
m_{ij} e_k = \delta_{jk}e_i.
$$
Since $\pi(\C[G]) = \End(V_\pi)$ by the density theorem,we can find $a_{ij}(g) \in \C$ such that
$$
m_{ij} = \sum_{g \in G} \alpha_{ij}(g) \pi(g).
$$

Consider the ring map $\C[x_g \mid g \in G] \to \C[x_{ij}]$ defined by
$$
x_g \mapsto \sum_{ij} \alpha_{i,j}(g) x_{ij}.
$$

Under this map the operator $\widehat{P_G}$ becomes
$$
\sum_{g \in G} x_g \pi(g) \mapsto \sum_{g \in G} \sum_{i,j} \alpha_{ij}(g) x_{ij} \pi(g) = \sum_{i,j} m_{ij} x_{ij} = (x_{ij}).
$$

Since the determinant is functorial with respect to ring maps, a decomposition $P_G=p_1p_2$ would induce a decomposition of $\det (x_{ij})$, which was just proved to be impossible.

We still have to check that no two of the determinants are equal (up to associates). But note that $\restr{P_\pi}{x_g = 0, g \neq e}=x_e^{\dim \pi}$. So we can recover the dimension of $\pi$ from the determinant. Also note that if we fix $g \in G$, then
$$
\restr{P_\pi}{x_e=1, x_h =0 \text{ for } h \neq g} = \det(1+x_g\pi(g))=1+\tr \pi(g)x_g + h.o.t.
$$
so we can recover the trace as well. But a representation is determined by its character, so if the polynomials are equal, then the representations are equivalent. The converse is similar.
\end{proof}

\subsection{Two constructions on representations}

Recall that the tensor product of $V$ and $W$ have a basis $v_i \otimes v_j$. Hence $\dim_\C V \otimes W = \dim_\C V \cdot \dim_\C W$.

Assume $(V,\pi)$ is a finite-dimensional representation and consider $V^\ast = \Hom_\C(V,\C)$. Then we define the \textbf{contragradient representation} $\pi^c$ to be the representation of the dual space defined by 
$$
(\pi(g)f)(v) = f(\pi(g^{-1})v).
$$

If $v_1,\ldots,v_n$ is a basis for $V$ and $\pi(g)=(a_{ij}(g))$, then in this basis 
$$
\pi^c(g) = (a_{ji}(g^{-1}))_{ij}.
$$
In particular, the character  $\chi_{\pi^c}(g)=\chi_\pi(g^{-1})=\overline{\chi_\pi(g)}$. 

Also, we can define the \textbf{tensor product of two representations} $\pi$ and $\theta$ by $(\pi \otimes \theta)(g)(v \otimes w) = \pi(g)v \otimes \theta(g)w$. 

Also: $\Hom(U, W \otimes V^\ast)= \Hom(U \otimes V, W)$. 

Frobenius reciprocity:
$$
\Mor(\pi, \eta \otimes \theta^c) \simeq \Mor(\pi \otimes \theta, \eta).
$$

Denote by $R(G)$ the abelian group generated by classes $[\pi] \in \widehat G$ of finite-dimensional representations and relations $[\pi]+[\pi'] = [\pi \oplus \pi']$.  Then it is an exercise to show that $R(G)$ is a free abelian group with basis $[\pi] \in \widehat G$.

Via the tensor product we can form a product on $R(G)$, making it into a ring. See the Appendix for an explicit example.

%%%%%%%%%%%%%%%%%%%
\newpage
\section{Dimension of irreducible representations}

Recall that $$
\sum_{[\pi] \in \widehat G} (\dim \pi)^2 = \lvert G \rvert. $$

In this lecture we will prove the following theorem:

\begin{thm}
\label{thmzg}
 The dimension of any irreducible representation divides the number $\lvert G/Z(G) \rvert$, where $Z(G)$ is the center of $G$. 
\end{thm}
Later we will strengthen this and show that $Z(G)$ can be replaced by any normal abelian subgroup of $G$.

In order to prove this, we need to introduce some results from commutative algebra. Let $R$ be a unital commutative ring and $S \subset R$ a unital subring. Then we say that an element $a \in R$ is \textbf{integral over $S$} if it satisfies a monic polynomial $a^n+s_1a^{n-1}+\ldots+s_{n-1}=0$ with coefficients in $S$.

We say that a complex number integral over $\Z$ is an \textbf{algebraic integer}. An integral domain $S$ is called \textbf{integrally closed} if any element in the fraction field of $S$ integral over $S$ is already in $S$. It is an easy exercise to see that $\Z$ is integrally closed (and so is any UFD). 

\begin{lemma}
\label{lemmafingen}
 Let $S \subset R$ be as above. Then $a \in R$ is integral over $S$ if and only if the subring $S[a]$ is a finitely-generated $S$-module.
\end{lemma}
\begin{proof}
This is Proposition 5.1 in Atiyah-MacDonald, \cite{kommalg}.
\end{proof}

It is an exercise to see that the set of elements of $R$ integral over $S$ actually form a ring. Alternatively, consult Corollary 5.3 in Atiyah-MacDonald.

\begin{prop}
 Assume that $\pi$ is an irreducible representation. Let $g \in G$. Let $C(g)$ be the conjugacy class of $G$. Then
 \begin{enumerate}
 \item The number $\chi_\pi(g)$ is an algebraic integer.
\item The number $\frac{\lvert C(g) \rvert}{\dim \pi} \chi_\pi(g)$ is also an algebraic integer.
 \end{enumerate}
\end{prop}
\begin{proof}
i). Since $G$ is finite, we have $g^n=e$ for some $n$. Hence all eigenvalues of $g$ are $n$th roots of unity which are algebraic integers. But $\chi_\pi(g)$ is the sum of the eigenvalues, and the set of algebraic integers form a subring of $\C$, hence $\chi_\pi(g)$ is an algebraic integer as well.

The second part is more difficult. Let $p$ be the characteristic function of $C(g)$, that is the function defined as follows:
$$
p(x) = \begin{cases} 1 & \text{ if $x \in C(g)$} \\
0 & \text{ otherwise}.
\end{cases}
$$

Then $p$ lies in the center of $\C[G]$. Consider the subring $\Z[G] \subset \C[G]$ (the \textbf{group ring} of $G$). Let $R \subset \Z[G]$ be the subring of \emph{central functions}. Then $p \in R$. As $R$ is a finitely generated abelian group, any element of $R$ is integral over $\Z \cdot 1 \subset R$. 

It follows that $\pi(p)$ is integral over $\Z \cdot \id_{V_\pi} \subset \End(V_\pi)$. Since $p$ is central, $\pi(p) \in \End_G(V_\pi)=\C \cdot \id_{V_\pi}$. Hence $\pi(p) = \alpha \cdot \id_{V_\pi}$ for some complex number $\alpha$. This $\alpha$ is an algebraic integer, since $\pi(p)$ is integral over $\Z$.

But now

\begin{align*}
  \alpha \cdot \dim \pi &= \tr( \alpha \cdot \id_{V_\pi}) \\
&= \tr \pi(p) \\
&= \sum_{h \in C(g)} \tr \pi(h) \\
&= \sum_{h \in C(g)} \chi_\pi(h) \\
&= \lvert C(g) \rvert \chi_\pi(g).
\end{align*}
The last equality is because characters are constant on conjugacy classes. This proves the theorem.
\end{proof}

\begin{lemma}
 If $\pi$ is irreducible, then $\dim \pi \mid \lvert G \rvert$.
\end{lemma}
\begin{proof}
Recall that
$$
(\chi_\pi , \chi_\pi) = 1.
$$
This can be rewritten as
$$
\frac{\lvert G \rvert}{\dim \pi} = \sum_{g \in G} \frac{\chi_\pi(g) \chi_\pi(g^{-1})}{\dim \pi}.
$$
Now let $C(G)$ denote the set of conjugacy classes in $G$. $C$ is then a partition of $G$. Then the above sum can be rewritten as
$$
\sum_{C \in C(G)} \sum_{h \in C} \frac{\chi_\pi(h) \chi_\pi(h^{-1})}{\dim \pi} = \sum_{C \in C(G)}  \frac{\lvert C \rvert \chi_\pi(g_C) }{\dim \pi}  \chi_\pi(g_C^{-1}),
$$
where $g_C$ is any representative from $C$. The right-hand-side is a sum of products that we know are algebraic integers. They form a ring, hence the sum is an algebraic integer as well.

But the left hand side is a fraction, so we must have $\lvert G \rvert / \dim \pi \in \Z$, which is equivalent to what was to be proven.
\end{proof}

Now we can prove Theorem \ref{thmzg}. Recall that we want to show that $\dim \pi$ divides $\lvert G/Z(G) \rvert$. 

\begin{proof}
 For every $n \in \N$, consider the representation of $G^n = G \times \cdots \times G$ on $V_\pi \otimes \ldots \otimes V_\pi$ defined by $\pi_n(g_1,\ldots,g_n) = \pi_1(g_1) \otimes \ldots \otimes \pi_n(g_n)$. This is an irreducible representation, since any elementary tensor $v_1 \otimes \ldots \otimes v_n$ can be sent to any other elementary tensor by the action of $G^n$.

Now consider the subgroup $$Z_n = \{ (g_1,\ldots,g_n) \in Z(G)^n \mid \prod g_i = e \}.$$

It is isomorphic to $Z(G)^{n-1}$. If $g \in Z(G)$, then by Schur's lemma, $\pi(g)$ is a scalar operator. So if $(g_1,\ldots,g_n) \in Z_n$, then $\pi(g_i)=\alpha_i \cdot 1$, hence
\[
\pi_n(g_1,\ldots,g_n) = \alpha_1 \cdot \ldots \cdot \alpha_n \id_{V^{\otimes n}} = \id_{V^{\otimes n}}
\]
since $g_1\cdots g_n = e$. Therefore $Z_n \subset \ker \pi_n$. It follows that $\pi_n$ induces a representation of $G^n/Z_n$. Then by the previous lemma
$$
(\dim \pi)^n \mid \frac{\lvert G \rvert^n }{\lvert Z(G) \rvert ^{n-1}}
$$
That is:
$$
\left( \frac{\lvert G \rvert}{\dim \pi \lvert Z(G) \rvert } \right)^n \in \frac{1}{\lvert Z(G) \rvert}\Z
$$
for any $n$. This is true for any $n$, hence
$$
\Z \left[ \frac{\lvert G \rvert}{\dim \pi \lvert Z(G) \rvert} \right] \subset \frac{1}{\lvert Z(G) \rvert } \Z.
$$
This implies that the left-hand-side is a finite-generated $\Z$-module, so by Lemma \ref{lemmafingen}, the fraction $\lvert G \rvert /\dim \pi |Z(G)|$ is an algebraic integer. Hence it is an integer. This proves that $\dim \pi \mid \lvert G \rvert / \lvert Z(G) \rvert$.
\end{proof}


%%%%%%%%%%%%%%%
\newpage
\section{Representation theory of $S_n$}

It is easy to understand conjugacy classes in $S_n$. Every $\sigma \in S_n$ defines a partition of $[n] = \{ 1,\ldots n\}$ into orbits of $\sigma$. Let $O_1,\ldots,O_m$ be these orbits. Assume that they are decreasing in size. Let $n_i=\lvert O_i \rvert$. These numbers $n_i$ sum to $n$.

We say that a \textbf{partition of $n$} is a decreasing sequence of integers $n_1 \geq \ldots \geq n_m \geq 1$ such that $n=\sum n_i$. 

Thus every $\sigma \in S_n$ gives a partition of $n$. Two elements of $S_n$ are conjugate if and only if they define the same partition. To see this, note that a permutation is determined by its decomposition into disjoint cycles. If $\sigma = \sigma_1 \ldots \sigma r $ is a product into disjoint cycles, then $\rho \sigma \rho^{-1}=\rho \sigma_1 \rho^{-1} \ldots \rho \sigma_r \rho^{-1}$ is also a decomposition into disjoint cycles. Hence conjugat elements define the same partition. For the converse, write the partition $n = n_1 + n_2 \ldots + n_m$ as
$$
(1,2,\ldots,n_1)(n_{1}+1, \ldots,n_{n_2}) \cdots 
$$
Then renaming the numbers correspond to conjugating.

Our goal is to produce an irreducible representation for every partition of $n$.

Recall that for any finite group $G$, we have
$$
\C [G] \simeq \bigoplus_{[\pi] \in \widehat G} \End(V_\pi),
$$
the isomorphism sending $g$ to the corresponding endomorphism of $V = \oplus V_\pi$. Now one can ask if it is possible to recover the space $V_\pi$ from its matrix algebra? (essentially, yes)

We need a few definitions. An idempotent $e$ in an algebra $A$ is called \textbf{minimal} if $e \neq 0$ and $eAe =\C e$. 


\begin{lemma}
 Let $V$ be a finite-dimensional vector space. Show that $e \in \End(V)$ is a minimal idempotent if and only if it is a projecting onto a $1$-dimensional subspace $\C v \subset V$. Then we have an isomorphism
$$
\End(V)e \simeq V
$$
of $\End(V)$-modules given by $Te \mapsto Tv$. 
\end{lemma}
\begin{proof}
--TO COME--
\end{proof}

Therefore finding irreducible representations of $G$ is the same as finding minimal idempotents in the group algebra $\C[G]$: if $e \in \C[G]$ is a minimal idempotent, then $\C[G]e$ defines an irreducible representation ([[WHY IS IT IRREDUCIBLE??]].

Thus, returning to $S_n$, we want to to construct minimal idempotents in $\C[S_n]$ for every partition.

It is convenient to present partitions as \textbf{Young diagrams}. Given a partition $(n_1,\ldots,n_m)$ of $n$, we draw a diagram having $n_1$ boxes in the first row, $n_2$ boxes in the second, and so on. For example, for the partition $(6,5,2,1)$ of $14$, we draw the following diagram:
$$
\begin{Young}
&&&&&\cr
&&&&\cr
&\cr
\cr
\end{Young}
$$

We can also fill these with numbers: a \textbf{Young tableau} (plural \emph{tableaux}) is a Young diagram filled with numbers $i \in [n]$ without repetitions. For example:
$$
\begin{Young}
3&1\cr
2\cr
\end{Young}
$$

Two Young tableaux are said to be of the same \textbf{shape} if they arise from the same underlying Young diagram.

Note that the symmetric group act on the set of Young tableaux of the same shape $\lambda$. 

Fix a Young tableau $T$. Let $R(T) \subset S_n$ be the subgroup of elements of $S_n$ permuting numbers in the \emph{rows} of $T$. Let $C(T)$ be the same group, but permuting columns instead. Define the following two elements of $\C[S_n]$:

$$
a_T = \frac{1}{\lvert R(T) \rvert} \sum_{g \in R(T)} g 
$$
and
$$
b_T = \frac{1}{\lvert C(T) \rvert} \sum_{g \in C(T)} sgn(g) \cdot g.
$$
Then we define $c_T=a_T b_T$. The element $c_T$ is called a \textbf{Young symmetrizer}. Our main theorem is this:

\begin{thm}
  \begin{enumerate}
  \item For any $T$, the element $c_T$ is (up to a scalar) a minimal idempotent in $\C[S_n]$, so it defines an irreducible representation of $S_n$.
\item If $T_1,T_2$ are two Young tableaux, then the representations corresponding to them are equivalent if and only if $T_1$ and $T_2$ have the same shape.
  \end{enumerate}
\end{thm}

The modules $\C[S_n]c_T$ are called \textbf{Specht modules}.

\begin{proof}
  TOO LONG FOR NOW. WILL PROB. COME LATER
\end{proof}

\subsection{Tabloids}

A bit more explicitly the Specht modules can be described as follows.

Fix a Young diagram $\lambda$. Introduce an equivalence relation on Young tableaux of shape $\lambda$:
$$
T_1 \sim T_2 
$$
if $T_1 = r(T_2)$ for some $r \in R(T_2)$. The equivalence class of a Young tableau $T$ is called a \textbf{tabloid} (of shape $\lambda$) and denoted by $\{ T \}$. 

Note that $S_n$ act on the set of tabloids. Let $(M_\lambda,S_n)$ be the corresponding permutation representation. Note also that action on the set of tabloids is transitive. Thus the basis of $M_\lambda$ can be identified with $S_n/R(T)$\footnote{Every $\{T \}$ can be written as $\sigma \{ T_0\}$ for some fixed $T_0$. But the ambiguity lies in $R(T)$.}.  

Thus we get isomorphisms of $\C[S_n]$-modules:
$$
\C[S_n]a_T \simeq \C[S_n/R(T)] \simeq M_\lambda.
$$
The first isomorphism is given by sending $ga_T$ to $\partial_{gR(T)}$ (the delta function), and the second is given by sending a coset $\partial_{gR(T)}$ to $\{g(T)\}$. 

Under these isomorphisms, the image of the  submodule $\C[S_n]b_T a_T \subset \C[S_n]a_T$ is spanned by the elements $b_{g(T)} \{ g(T) \}$. In other words, $V_\lambda$ is spanned by $e_{T'} := b_{T'} \{ T' \}$ by all elements of shape $\lambda$. This gives us a description of $V_\lambda \simeq V_T$ only in terms of $\lambda$.

This is a spanning set. We would like a basis. We say that that tableau $T$ is \textbf{standard} if the numbers in every row and column are increasing.

Here's a theorem that we will not prove:
\begin{thm}
 For any Young diagram $\lambda$ the elements $e_T$ for all standard tableaux $T$ of shape $\lambda$ form a basis in $V_\lambda$.
\end{thm}

The complication is of course that $g e_T = c_{g(T)}$ is a combination of standard tableaux! 

\begin{example}
 Let $\lambda = n$, be the trivial partition. Then the corresponding Young diagram looks like:
$$
\begin{Young}
&&&...&&\cr
\end{Young}
$$
For any tableau $T$ of shape $\lambda$ we get $C(T) = \{ e\}$, since all the columns are one-element sets. Also, $R(T)=S_n$. Hence
$$
c_T = \frac{1}{n!} \sum_{g \in S_n} g.
$$
Then $gc_T = c_Tg = c_T$. Hence $\C[S_n]c_T = \C c_T$, so that $\pi_T \sim \epsilon$, the trivial representation.
\end{example}

\begin{example}
Let $\lambda = (1,\ldots,1)$ be the partition $n=1+\ldots+1$. Then the corresponding Young diagram looks like
$$
\begin{Young}
\cr
\cr \cr  \vdots \cr \cr
\end{Young}
$$
Then $C(T)=S_n$ and $R(T)=\{e \}$. Therefore
$$
c_T = \frac{1}{n!} \sum_{g \in S_n} sgn(g) \cdot g.
$$
So $g c_T = c_T g = sgn(g) c_T$. Hence $\C[S_n]=\C c_T$, and $\pi_T$ is the one-dimensional representation $sgn$. 
\end{example}

\subsection{Characters and the Hook length formula}

Denote by $\pi_\lambda$ the irreducible representation of the symmetric group corresponding to the Young diagram $\lambda$.

To describe the character of $\pi_\lambda$, it is convenient to index the conjugacy classes in $S_n$ by the number of cycles.

Let $I=(i_1,i_2,\ldots,)$ consist of non-negative integers such that
$$
\sum_{k=1}^\infty ki_k=n.
$$
Denote by $c_I$ the conjugacy class in $S_n$ consisting of elements $\sigma$ which decompose into $i_1$ orbits of length $1$, $i_2$ orbits of length two and so on. 

This correspond to the partition $ki_k+(k-1)i_{k-1}+...=n$. 

Let $\chi_{\pi_\lambda}(C_I)$ denote the value of the character of $\pi_\lambda$ on any representating of the conjugacy class of $C_I$. Then
\begin{thm}[Frobenius character formula]
  
Assume $\lambda=(\lambda_1,\ldots,\lambda_r)$. Take any number $N \geq r$. Then $\chi_{\pi_\lambda}(C_I)$ is the coefficient of $\prod_{i=1}^N x_i^{\lambda_i+N-i}$ in the polynomial
$$
\Delta(x) \prod_{k \geq 1} {\left(\sum_{i=1}^N (x_i^k)\right)}^{i_k}.
$$
where $\Delta(x) = \prod (x_i-x_j)$ is the Vandermonde determinant.
\end{thm}

\begin{proof}
  If time allows a proof will appear here.
\end{proof}

Now we quote the \textbf{hook length} formula. Let $\lambda$ be a Young diagram and let $(i,j)$ be the element in position row $i$ and column $j$. Then the \textbf{hook length} $h(i,j)$ is the number of boxes below and to the right of $(i,j)$ including $(i,j)$ itself.

Using the notation in the previous Theorem, let $l_i = \lambda_i + N - i$ (we put $\lambda_i=0$ for $i > r$). Then:

\begin{lemma}
  For any $i$, we have that
$$
\frac{l_i!}{\prod_{j=i+1}^n (l_i-l_j)} = \prod_{k=1}^{\lambda_i} h(i,k).
$$
\end{lemma}

From this we get:

\begin{thm}
  Let $\lambda$ be partition and let $\pi_\lambda$ be the corresponding irreducible representation of $S_n$. Then 
$$
\dim \pi_\lambda = \frac{n!}{\prod_{(i,j) \in \lambda} h(i,j)}.
$$
This is also the number of standard Young tableaux of shape $\lambda$.
\end{thm}

Prooof later.

\begin{example}
Let
$$
\begin{Young}
&\cr
\cr
\end{Young}
$$
be a Young diagram. Then $\dim \pi_\lambda= \frac{3!}{1 \cdot 1 \cdot 1 \cdot 3}=2$.
\end{example}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Induced representations}

Let $G$ be a finite group and $H \subset G$ a subgroup. Assume $(\pi,V)$ is a representation of $H$. We want to construct a representation of $G$ out of this.

We define the \textbf{induced representation} $\mathrm{Ind}_H^G \pi$ to be the $\C[G]$-module define by
$$
\C[G] \otimes_{\C[H]} V.
$$

It is very useful in constructing representations of larger groups from smaller ones. The idea is that if we know the representations some subgroup $H$ of a group $G$, then in nice situations all representations of $G$ can be found as subrepresentations of $\Ind_H^G \pi$ for various $[\pi] \in \hat H$.

\begin{remark}
This is exactly the concept of \emph{extension of scalars} from commutative algebra (see e.g. Atiyah-MacDonald page 28). Namely let $A=\C[H]$ and $B=\C[G]$ and let $M=V$ be an $A$-module. Then we can form the $A$-module $M_B= B \otimes_A M$, which carries a natural $B$-module structure as well: the action of $B$ on $M_B$ is given by $b(b' \otimes x)=bb' \otimes x$.
\end{remark}

We have three equivalent description of the induced representation, some of the descriptions being easier to work with than others.

\begin{thm}
Let $V$ be a representation of $H$. The following representations are isomorphic.
\begin{enumerate}
\item $\C[G] \otimes_{\C[H]} V$ where $G$ acts by $g \cdot (g' \otimes v) = gg' \otimes v$. 
\item $\Hom_H(\C[G],V) \simeq \{ f:G \to V \mid f(hg)=hf(g) \forall h \in H, g \in G \}$, where $g$ acts by $(g' \cdot f)(g) = f(gg')$.
\item $W = \bigoplus_{\sigma \in G/H} V_\sigma$, where each $V_\sigma$ is a copy of $V$. Let $x_\sigma \in V_\sigma$. Then the action of $G$ is defined as follows: Let $r_\sigma \in \sigma$ be a representative for the coset $\sigma$. Then $g$ can be written as $g=r_{\sigma'}h$ for some $h \in H$. Then we define $g \cdot  x_\sigma = h \cdot x_{g \sigma}$.
\end{enumerate}
\end{thm}

\begin{proof}
Start with 1). We want to show that this is the same as number 3). We will do this in a non-canonical way. The algebra $\C[H]$ have a vector space basis $\{e_{h_1}, \ldots, e_{h_r}\}$ where $r=\lvert H \rvert$. By properties of cosets, this implies that $\C[G]$ have a basis 
$$\{e_{h_1},\ldots, e_{h_r},e_{r_1h_1},\ldots, e_{r_1h_r},\ldots,e_{r_kh_1},\ldots,e_{r_kh_r} \}$$
where $k=[G:H]$ and the $r_i$'s are a set of fixed representatives for the cosets $G/H$. This means that we can write
$$
\C[G] = \bigoplus_{i=1}^k r_i \C[H].
$$
This implies that $\C[G] \otimes_{\C[H]} V$ decomposes as
$$
\C[G] \otimes_{\C[H]} V = \left(\bigoplus_{i=1}^k r_i \C[H]\right) \otimes_{\C[H]} V \simeq \bigoplus_{i=1}^k \left( r_i \C[H] \otimes_{\C[H]} V \right)
$$
This is exactly the description in 3), because each $r_i \C[H] \otimes_{\C[H]} V$ is isomorphic to $V$ via the map $r_ih \otimes v \mapsto h^{-1}v$ (we need the inverse in order for this to be a map of representations). The representation $V$ is endowed with a left action of $H$ (this is what a representation \emph{is}), but it also has a right action, defined by $v \cdot g := g^{-1}v$. In this way, $V$ becomes a left-right-$H$-module.

One can check that the action in 3) and the natural $G$-action on $r_i\C[H] \otimes_{\C[H]} V$ are the same.

The equivalence of $1)$ and $2)$ can be seen by standard isomorphisms from module theory. Namely, note that 
$$
\Hom_H(\C[G],V) \simeq \Hom_H(\C[G],\C) \otimes_{\C[H]} V.
$$
We also have an identifaction of representations $\C[G]$ with $\Hom(\C[G],\C)$ where we let $\delta_g$ be a basis of $\C[G]$. Then the action of $G$ on $\C[G]$ by left-multiplication is identified with the contragradient action of $G$ on $\Hom(\C[G],\C)$. But all functions $\C[G] \to \C$ are $G$-invariant (because $f(gx)=gf(x)=f(x)$ since we are considering the trivial action on $\C$). Hence $\Hom_H(\C[G],\C)=C[G]$. So the two descriptions are equal.
\end{proof}

We say that the collection $\{ V_\sigma \}\subset W$  is called a \textbf{system of imprimitivity} of the representation $V$. 

\begin{example}
Let $\lambda_H$ be the regular representation of $H$. Then $\lambda_H$ can be identified with $\C[H]$. In this case we have $\Ind_H^G \lambda_H = \lambda_G$ since $\C[G] \otimes_{\C[H]} \C[H] \simeq \C[G]$.
\end{example}

Here are some basic properties of induction. Most of them follow from basic properties of tensor products.

\begin{enumerate}
\item If $N \subset M \subset G$ are subgroups, then
$$
\Ind_H^G \Ind_N^M \pi \sim \Ind_N^G \pi
$$
\item If $\pi$ is finitedimensional, then
$$
(\Ind_H^G \pi)^\vee \simeq \Ind_H^G  \pi^\vee.
$$
Here $\pi^\vee$ is the contragradient representation of $\pi$.
\end{enumerate}

\subsection{Frobenius reciprocity}

Let $\Rep{G}$ denote the category of finite-dimensional $G$-representations where $G$ is a finite group and let $H \subset G$ be a subgroup of $G$. We have two functors in each direction: we have a functor $\Res_H^G$ sending a representation to its restriction to $H$, and we have a functor $\Ind_H^G$ in the other direction, sending a representation of $H$ to the induced representation on $G$. The theorem of this section is that these two functors are adjoints to each other.

But this is a standard fact about tensor products: let $S \subset R$ be rings (commutative or not) and let $V$ be an $S$-module and $W$ an $R-S$-bimodule. In our case a representation $W$ of $G$ is both a $\C[G]$-module and a $\C[H]$-module by restriction. Then we always have
$$
\Hom_R(R \otimes_S V,W) \simeq \Hom_S(V,W)
$$
Note that as an $S$-module we have $W=\Res_H^G W$. 

If $\chi$ is the character of $\pi$ (a representation of $H$), we denote by $\Ind_H^G \chi$ the character of the induced representation. 

Note that it follows by Schur's lemma and the orthogonality relations that for any two representations, we have
$$
(\chi_{\pi_1}, \chi_{\pi_2}) = \dim_\C \Mor(\pi_1,\pi_2).
$$
Therefore, in terms of characters, Frobenius reciprocity can be stated as 
$$
(\Ind_H^G \chi_\pi, \chi_\theta) = (\chi_\theta, \Res_H^G \chi_\theta).
$$
Thus:
\begin{prop}
Let $V$ be an irreducible representation of $H$ and $W$ an irreducible representation of $G$. Then the number of times that $W$ occurs in $\Ind_H^G V$ is equal to the number of times that $\Res_H^G W$ occurs in $V$.
\end{prop}
\begin{proof}
Recall that if $V$ is an irreducible representation and $U$ is any representation, then $(\chi_V,\chi_U)$ is equal to the number of times that $V$ occurs in $U$. Now use the above remarks.
\end{proof}

\subsection{Characters of induced representations}

\begin{prop}
\label{inducedcharacter}
 Assume $G \supset H$ and that $(\pi,V)$ is a finite-dimensional representation of $H$ and $\chi$ its character. Then the character $\Ind_H^G \chi$ of $\Ind_H^G \pi$ is given by
$$
(\Ind \chi)(g) = \sum_{\bar s \in G/H, s^{-1}gs \in H} \chi(s^{-1}gs) =\frac{1}{\lvert H \rvert} \sum_{s \in G \mid s^{-1}gs \in H} \chi(s^{-1}gs),
$$
where we extend $\chi$ to $G$ by zero.
\end{prop}
\begin{proof}
Consider the imprimitivity system $\{W_x\}_{x \in G/H}$ for the induced representation $\theta = \Ind_H^G \pi$. Then
\begin{align*}
  \chi_\theta(g) &= \tr(g) \\
 &=\sum_{x \in G/H, gx=x} \tr(\restr{\theta(g)}{W_x}) \\
&= \sum_{\bar s \in G/H, s^{-1}gs \in H} \tr( \restr{\theta(g)}{W_{\bar s}})  \\
&= \sum_{\bar s \in G/H, s^{-1}gs \in H} \tr( \restr{\theta(s^{-1})\theta(g)\theta(s)}{W_{\bar e}})   \\
&= \sum_{\bar s \in G/H, s^{-1}gs \in H} \tr( \pi(s^{-1}gs)) \\
&= \sum_{\bar s \in G/H, s^{-1}gs \in H} \chi(s^{-1}gs).
\end{align*}
The second equality follows by noting that $\chi(s^{-1}gs)$ is constant on left cosets as $s$ varies.
\end{proof}

This makes it easy to compute characters of induced representations. In practice one often induces from a normal subgroup containing few conjugacy classes, so that many of the terms in the sum are zero.


%%%%%%%%%%%%%%%%%
\newpage
\section{The Frobenius subgroup theorem}

In this section we will prove a theorem of group theory, the proof of which relies heavily upon representation theory. To this day, no ``elementary'' proof is known.

\begin{thm}
  Assume a finite group $G$ acts transitively on a set $X$ such that every $g \neq e$ has at most one fixed point. Then there exists a unique normal subgroup $K$ of $G$ acting freely transitively on $X$.
\end{thm}

\begin{proof}

Put
$$
K \stackrel{\Delta}{=} \{ e \} \cup \{ g\in G \mid g  \text{ has no fixed points } \}.
$$
We have to show that $K$ is a normal subgroup and that it acts freely transitively on $X$. Once this is done, we are finished, since $K$ is clearly the largest possible subset of $G$ satisfying the conclusion of the theorem.

To show that $K$ acts freely transitively on $X$, it suffices to show that $\# K=\#X$. Let $x \in X$, and consider the stabilizer
$$
H = G_x = \{ g \in G \mid g \cdot x = x \}.
$$
If $g \not \in H$, by definition $g \cdot x \neq x$. Also note that
$$
H \cap g H g^{-1} = G_x \cap G_{gx} = \{ e \},
$$
since otherwise $h$ in the intersection would act trivially on both $x$ and $gx$, but we assumed that every element acted with at most one fixpoint.

Hence we can write $G$ as a disjoint union as follows:
$$
G = K \coprod \left( \coprod_{y \in X} G_y \bs \{ e\} \right) = K \coprod \left( \coprod_{[g] \in G/H} gHg^{-1} \bs \{e \} \right).
$$

Therefore
$$
\# G = \#K  + \frac{ \# G}{\# H}( \# H  -1 )
$$
so that $\# K = \# G / \# H = \# X$.

It remains to show that $K$ is a normal subgroup. We will do this by showing that it is the kernel of some representation of $G$.

For an irreducible representation $\pi$ of $H$, consider the function $\widetilde{\chi_\pi}$ on $G$ defined by
\begin{align*}
\widetilde{\chi_\pi} &= \Ind_H^G \chi_\pi - (\dim \pi) \Ind_H^G \chi_{\epsilon_H} + (\dim \pi) \chi_{\epsilon_G} \\
&= \Ind_H^G \chi_\pi - (\dim \pi) \Ind_H^G 1 + (\dim \pi) 1.
\end{align*}
Recall that in general, if $f:H \to \C$ is any class function, we can induce a new class function on $G$ by the formula
$$
(\Ind_H ^G f)(h) = \sum_{[g] \in G/H, g^{-1}hg \in H} f(s^{-1} g s).
$$
Here the sum chooses one representative from each coset. Now one sees easily that
$$
\widetilde{\chi_\pi}(g) = \begin{cases} \dim \pi & \text{ if } g \in K \\
\chi_\pi(h) & \text{ if $g$ is conjugate to $h \in H$}
\end{cases}
$$
In the first case we get $0-0+\dim \pi$ in the definition, and in the second case we get $\Ind_H^G \chi_\pi(g)-\dim \pi + \dim \pi$. That this completely describes $\widetilde{\chi_\pi}(g)$ is clear because of our description of $G$ as a disjoint union above.

Using this decomposition, we can calculate $(\widetilde{\chi_\pi},\widetilde{\chi_\pi})$:

$$
(\widetilde{\chi_\pi},\widetilde{\chi_\pi}) = \frac{1}{\lvert G \rvert} \sum_{g \in G} \lvert \widetilde{\chi_\pi}(g) \rvert^2 = \frac{1}{\lvert G \rvert} \frac{\lvert G \rvert }{\lvert H \rvert } \dim \pi^2 + \frac{1}{\lvert G \rvert} \frac{\lvert G \rvert }{\lvert H \rvert} \sum_{h \in H\bs \{e\} } \lvert \chi_\pi(h) \rvert^2
$$
But this last expression is equal to $(chi_\pi,\chi_\pi)=1$.

As $\widetilde{\chi_\pi}$ is a linear combination of characters with integer coefficients, we must have $\widetilde{\chi_\pi} = \pm \chi_{\widetilde \pi}$ for some irreducible representation $\widetilde \pi$.\footnote{To see this, note that the norm of $\widetilde{\chi_\pi}$ is a sum of squares of integers, summing to $1$.}. But $\widetilde{\chi_\pi}(e) = \dim \pi > 0$, so that actually $\widetilde{\chi_\pi} = \chi_{\widetilde \pi}$. 

Note that as $\restr{\chi_{\widetilde \pi}}{H} = \chi_\pi$, we have $\Res_H^G \widetilde \pi \sim \pi$.

Now we claim that
$$
K = \bigcap_{[\pi] \in \hat H} \ker \widetilde \pi.
$$
This implies that $K$ is a normal subgroup. This would finishing the proof.

First we look at $\subseteq$: Let $g \in K$. Then $\chi_{\widetilde \pi}(g) = \dim \pi = \dim \widetilde \pi$. But $\widetilde \pi(g)^n = 1$ for some $n \geq 1$. Hence all the eigenvalues of $\widetilde \pi (g)$ are roots of unity. The matrix $\widetilde \pi(g)$ is diagonalizable, hence the sum of $\dim \widetilde \pi$ roots of unity is equal to $\dim \widetilde \pi$. This is only possible if all the eigenvalues are $1$ by the triangle inequality, i.e. that $\widetilde \pi(g)=\id$. Thus $g \in \ker \widetilde \pi$.

Now look at $\supseteq$. Let $g \in G \bs K$. Then $g$ is conjugate to an element $h \in H \bs \{e\}$. Then we can find some irreducible representation $[\pi] \in \hat H$ such that $\pi(h) \neq 1$. Hence $\widetilde \pi(g) \neq 1$. 

This finishes the proof.
\end{proof}

\begin{example}
Let $k$ be a finite field and let $G$ be the translation group of $\Aa^1$, or in other words the ``ax+b''-group. It is isomorphic to the matrix group
$$
G \simeq \left\{ \begin{pmatrix}
a & b \\
0 & 1 
\end{pmatrix} \mid a \in k^\times, b \in k \right\}.
$$
Then $G$ satisfies the conditions of the theorem (we say that $G$ is a \textbf{Frobenius group}). Then the group $K$ in the theorem is the group of translations:
$$
K = \left\{ \begin{pmatrix}
1 & b \\ 0 & 1 
\end{pmatrix}
\mid b \in k \right \}.
$$
\end{example}


%%%%%%%%%%%%%%%%%%%%
\newpage
\section{The theorems of Artin and Brauer and fields of definition}

\begin{thm}[Artin]

Let $G$ be a finite group and $\mathscr C$ a collection of subgroups of $G$. Then the following two conditions are equivalent:
\begin{enumerate}
\item Any character of $G$ is a rational combination of characters of representations induced from subgroups in $\mathscr C$.
\item Any $g \in G$ is conjugate to an element in a subgroup in $\mathscr C$.
\end{enumerate}
\end{thm}

\begin{proof}
The proof is ``surprisingly simple''.

We first prove $1 \Rightarrow 2$. Recall that the characters span the space of central functions on $G$.

At the same time, the characters induced from subgroups in $\mathscr C$ are zero on elements that are not conjugate to elements of subgroups in $\mathscr C$, by Proposition \ref{inducedcharacter}. But any non-identity $g \in G$ have a character with non-zero value at $g$ (take for example the induced regular representation). Thus $g$ must meet some conjugacy class in $\mathscr C$.

Conversely, denote by $R_\C(G)$ the space of central functions on $G$ with the usual scalar product.

Consider the map
\begin{align*}
  T: \bigoplus_{H \in \mathscr C} R_\C(H) &\to R_\C(G) \\
 T\left( {(f)}_{H \in \mathscr C} \right) &= \sum_{H \in \mathscr C} \Ind_H^G f_H.
\end{align*}
Recall that by Frobenius reciprocity, the functor $\Res_H^G$ is adjoint to $\Ind_H^G$. Therefore the adjoint map of $T$ is given by 
$$
T^\ast : R_\C(G) \to \bigoplus_{H \in \mathscr C} R_\C(H)
$$
sending $f$ to ${(\Res_H^G f)}_{H \in \mathscr C}$. 

Now suppose $\Res_H^G f = 0$ for all $H \in \mathscr C$ and some $f \in R_\C(G)$. But as above, since any $g \in G$ is conjugate to an element from some $H$, it follows that $f$ is zero on all conjugacy classes, hence $f$ is zero. So the map $T^ \ast$ is injective, hence $T$ is surjective.\footnote{Quote Sergey: ``we didn't do any real work''.}

This means that we can find a basis in $R_\C(G)$ consisting of characters $\chi_1,\ldots,\chi_n$ of representations induced from subgroups in $H \in \mathscr C$. The characters $\chi_1',\ldots, \chi_n'$ of irreducible representations of $G$ form a basis in $R_\C(G)$ by Prop .... Therefore the transition matrix $A$ from $\{ \chi_1,\ldots, \chi_n \}$ to $\{ \chi_1',\ldots,\chi_n' \}$ has integer entries. Hence $A^{-1}$ has rational entries.

Therefore any character induced from an irreducible representation can be written as a $\Q$-linear combination of the $\chi_i$'s. Therefore this holds for all characters.
\end{proof}

We can take $\mathscr C$ to be the collection of cyclic subgroups of $G$.

\begin{corr}
Any character can be written as a $\Q$-linear combination with rational coefficients of characters induced from cyclic subgroups.
\end{corr}
\begin{proof}
 Any $g \in G$ is contained in some cyclic subgroup, say $\langle g \rangle$. 
\end{proof}

The following is more difficult to prove (we won't):

\begin{thm}[Brauer]
\label{thmbrauer}
For any finite group $G$, the characters of $G$ can be written as a linear combination with integer coefficients of characters $\Ind_H^G \chi_\pi$ of one-dimensional representations $\pi$ of subgroups $H \subset G$.

In fact we can let $H$ range through products of cyclic and $p$-subgroups.
\end{thm}

For a proof, see for example \cite{serre_linrep}. 

\subsection{Field of definition}

Let $\pi:G \to \GL(V)$ be a representation over the complex numbers. Let $K \subset \C$. We say that \textbf{ $\pi$ is defined over $K$} if there exists a vector space $V'$ over $K$ and a representation $\pi' : G \to \GL(V')$ such that $\pi$ is equivalent to $\pi_\C'$, where
$$
\pi_\C'(g) = 1 \otimes \pi(g) : \C \otimes_K V' \to \C \otimes_K V'.
$$

\begin{example}
 Let $G = \Z/nZ$. Representations of $G$ are characters, all of the form $e^{\frac {2 \pi i}{n}}$. Hence any representation of $G$ is defined over $\Q(\zeta_n)$. 
\end{example}

\begin{example}
 If $G=S_n$ (a much more complicated group), every representation is in fact defined over $\Q$.
\end{example}

As long as the field $K$ is of characteristic zero, many of the results are still true with the same proofs. The main difference is that if $\pi$ is an irreducible representation, then $\End(\pi)$ forms a \emph{division algebra} over $K$, instead of being just equal to $K$ itself. There are not many division algebras of characteristic zero, so this is not too bad.

Many results remain true, however. Maschke's theorem (that is, proving complete reducibility) goes through for any field of characteristic zero (or for any group of order not dividing the characteristic). We also have a weaker form of Schur's lemma: if $\pi$ and $\pi'$ are non-equivalent irreducible representations, then $\Mor(\pi,\pi')=0$ and $(\chi_{\pi'},\chi_\pi)=0$. 

Note that ${\chi_{\pi'}}_\C=\chi_{\pi'}$. This implies that disjoint irreducibles stay disjoint when complexifying. What can happen, of course, is that irreducibles split into disjoint representations.

If $G$ is a finite group, then the \textbf{exponent of $G$} is the least common multiple of orders of all elements in $G$. 

\begin{thm}[Brauer]
Let $G$ be a finite group and let $m$ be the exponent of $G$. Then any complex representation of $G$ is defined over $\Q(\zeta_n)$, where $\zeta_n$ is a primitive $n$-th root of unity.  
\end{thm}

\begin{proof}
Let $\pi_1,\ldots, \pi_k$ be representatives of the equivalence classes of irreducible representations of $G$ over $\Q(\zeta_n)$. 

Note that any one-dimensional representation of a subgroup of $G$ is defined over $\Q(\zeta_n)$. Hence the induction of any such representation is defined over $\Q(\zeta_n)$. 

Now let $\pi$ be any finite-dimensional complex representation representation of $G$. It follows by Brauer's Theorem \ref{thmbrauer} that
$$
\chi_\pi = \sum_{i=1}^k m_i {\chi_{\pi_i}}_\C = \sum_{i=1}^k m_i \chi_{\pi_i}
$$
for some $m_i \in Z$, since each $\Ind_H^G \chi$ is a $\Z$-linear combination of the $\pi_i$'s. 

Assume in addition that $\pi$ is irreducible. Then
\begin{align*}
  1 = (\chi_\pi,\chi_\pi) &= \sum_{i,j} (\chi_{\pi_i}, \chi_{\pi_j}) \\
&= \sum_i m_i^2 (\chi_{\pi_i}, \chi_{\pi_i}).
\end{align*}

This is only possible if $\chi_\pi = \chi_{\pi_i}$ for some $i$, hence $\pi \sim (\pi_i)_\C$. So $\pi$ is defined over $\Q(\zeta_n)$. Thus the theorem is true for irreducible representations, and so true for every representation.
\end{proof}


\newpage
\section{Mackey's irreducibility criterion}

When is an induced representation irreducible? In practice, this is almost never the case.

\begin{example}
Let $H \subset G$ be a subgroup. Then $\Ind_H^G \epsilon_H = \C[G/H]$, which is never irreducible.
\end{example}

We need a unpleasant lemma.

Let $G$ be a finite group and let $H,K$ be two subgroups and suppose $\pi$ is a representation of $H$. Define a subgroup $H_g \subset H$ by
$$
H_g = gHg^{-1} \cap K, \qquad g \in G.
$$

Then, for any $g \in G$, define a representation $\pi^g$ of $H_g$ by
$$
\pi^g(h) \stackrel{\Delta}{=} \pi(g^{-1}hg).
$$

\begin{lemma}
 We have 
$$
\Res_K^G \Ind_H^G \pi \sim \bigoplus_{\overline g \in K \bs G / H } \Ind_{H_g}^K \pi^g,
$$
where $K \bs G / H = G/\sim$, where $g_1 \sim g_2$ if $g_1=kg_2h$ with $k \in K$ and $h \in H$.
\end{lemma}
\begin{proof}
To come. 
\end{proof}

\begin{thm}[Mackey]
 
Assume $\pi$ is an irreducible representation of $H \subset G$. Then $\Ind_H^G \pi$ is irreducible if and only if the representations $\Res_{H_g}^H \pi$ and $\pi^g = \pi(g^{-1} - g)$ are disjoint for all $g \in G \bs H$. 
\end{thm}
\begin{proof}
With the lemma in hand, the proof is quite short.

Let $\chi=\chi_\pi$. Then we want to compute $(\Ind_H^G \chi_\pi, \Ind_H^G \chi_\pi)$.  By Frobenius reciprocity, this is the same as $(\Res_H^G \Ind_H^G \chi, \chi)$. Hence, by the lemma, this is (using Frobenius reciprocity again)
$$
\sum_{\overline g \in H \bs G / H} (\Ind_{H_g}^H \chi_{\pi^g}, \chi)  = \sum_{\overline g \in H \bs G / H} (\chi_{\pi^g}, \Res_{H_g}^H \chi).
$$

Thus $\pi$ is irreducible if and only if $(\chi_{\pi^g}, \restr{\chi}{H_g})=0$ for all $g \not \in H$.
\end{proof}

\newpage
\section{Induction from normal subgroups}

We study the following question: let $G$ be a finite group and $H \subset G$ a normal subgroup. Can we describe the representations of $G$ in terms of those of $H$ and those of $G/H$? 

Assume $(W,\theta)$ is an irreducible representation of $G$. Let $(V,\pi)$ be an irreducible representation contained in $\Res_H^G \theta$. Then, for every $g \in G$, $\Res_H^G \theta$ also contains the representation $\pi^g$ on $H$ defined by 
$$
\pi^g(h) = \pi(g^{-1}hg).
$$

Indeed, if $T:V \to W$ intertwines $\pi$ with $\Res \theta$ (meaning that $T \pi(g)= \theta(g) T$ for all $g \in H$), then $\theta(g) T $ intertwines $\pi^g$ with $\theta$:
$$
\theta(g) T \pi^g(h) = \theta(g) T \pi(g^{-1}hg) = \theta(g) \theta(g^{-1} h g) T = \theta(h) \theta(g) T.
$$

Furthermore, if we consider the isotypic components of $W(\pi^g)$ of $\Res \theta$ corresponding to $\pi^g$, then $\theta(g_1) W(\pi^{g_2}) = W(\pi^{g_1g_2})$. 

Thus the space
$$
\sum_{g \in G} W(\pi^g)
$$
is invariant under $G$, hence it coincides with $W$, since $W$ was assummed to be irreducible.

In other words, $\Res \theta$ decomposes into copies of $\pi^g$, $g \in G$.

We can define an action of $G$ on $\widehat H$ by 
$$
[\pi] \mapsto [\pi^g].
$$

What we have shown, is that any irreducible representation $W_\theta$ of $G$ defines a $G$-orbit $S \subset \widehat H$ and that
$$
W = \bigoplus_{x \in S} W(x), \qquad \theta(g) W(x) W(gx).
$$


For $x \in \widehat H$, denote by $\mathcal I(x) \subset G$, the stabilizer of $x$ in $G$. It is called the \textbf{inertia subgroup} of $x$. Note that the action of $H$ on $\widehat H$ is trivial, so that we always have $H \subset \mathcal I(x)$.

Then
$$
\theta \sim \Ind_{\mathcal I(x)}^G \left( \restr{\Res_{\mathcal I(x)}^G \theta }{W(x)}\right).
$$

(why??) 

What can be say about $\Res_{\mathcal I(x)}^G \theta$? In other words, assume we take $[\pi] \in \widehat H$. Then
$$
\mathcal I = \mathcal I([\pi]) = \{ g \in G \, \mid \, \pi^g \sim \pi \}.
$$
Can we describe all irreducible representations $\eta$ of $\mathcal I$ such that $\Res_H^{\mathcal I} \eta$ is isotypic to $\pi$?

It can be shown that all such representations of $\mathcal I$ can be described in terms of \emph{projective} irreducible representations of $\mathcal I/H$. 

In general, a \textbf{projective representation} of a group $K$ on a vector space $V$ is a homomorphism $K \to \PGL(V) \stackrel{\Delta}{=} \GL(V)/ \C^\ast \cdot \id_V$.

Why do projective representations appear? We have that $\pi^g \sim \pi$ for all $g \in \mathcal I$. Thus there exists an invertible $T_g \in \GL(V_\pi)$ such that
$$
\pi(g^{-1}hg) = T_g^{-1} \pi(h) T_g
$$
for all $h \in H$. By irreducibility, $T_g$ is unique up to a scalar factor. This implies that $T_{g_1g_2} = c(g_1,g_2)T_{g_1}T_{g_2}$ for some $c(g_1,g_2) \in \C^\ast$. There the operators $T_g$ define a projective representation of $\mathcal I$ on $V$.

In some cases projective representations do not appear. We will make the following assumptions: $H$ is abelian, so $\pi$ is a character of $H$. Also, $\pi:H \to \GL(\C)$ extends to a homomorphism $\widetilde: \mathcal I \to \C^\ast$.

We want to describe all irreducible representations of $\mathcal I$ such that $\Res_H^{\mathcal I} \sigma$ is isotypic to $\pi$. 

Assume $\sigma$ is such a representation. Then consider $\widetilde \sigma = \overline{\widetilde \pi} \otimes \sigma$ (recall that by assumption, $\pi$ is a character, so conjugation makes sense). Then $\widetilde \sigma$ is a representation on the same space given by $\sigma(g) = \widetilde \pi(g)^{-1} \sigma(g)$. Then $\widetilde \sigma$ is irreducible and $H \subset \ker \widetilde \sigma$. So $\sigma$ is an irreducible representation of $\mathcal I([\pi])/H$.

Conversely, if $\widetilde \sigma$ is an irreducible representation of $\mathcal I/H$, then we can define $\sigma$ by $\sigma = \widetilde \pi \otimes \widetilde \sigma$. 

We have almost proved:

\begin{thm}
 Let $G$ be a finite group and $H \subset G$ a normal abelian subgroup. Assume that every character $\chi \in \widehat H$ extends to a homomorphism $\widetilde \chi: \mathcal I(\chi) \to \C^\ast$, where
$$
\mathcal I(\chi) = \{ g \in G \, \mid \, \chi^g = \chi \}.
$$
Then the irreducible representations of $G$ can be described as follows: for every $G$-orbit in $\widehat H$, fix a representative $\chi$. Then for every $[\sigma] \in \widehat{ \mathcal I(\chi)/H}$, define $\Ind_{\mathcal I(x)}^G(\widetilde \chi \otimes \sigma)$. 

Therefore, as a set, $\widehat G$ can be identified with 
$$
\bigsqcup_{[\chi] \in \widehat H / G} \widehat{\mathcal I(\chi)/H},
$$
where the $[\chi]$ ranges over representatives of $G$-orbits in $\widehat H$.
\end{thm}

\begin{example}
 Consider a finite field $\F_p$ with $p$ prime. Consider the corresponding $ax+b$ group:
$$
G = \left\{ 
\begin{pmatrix}
a & b \\
0 & 1
\end{pmatrix} \, \mid \, a \in \F_p^\ast, b \in \F_p
\right\}.
$$
Let $H$ be the subgroup
$
H = \left\{ 
\begin{pmatrix}
1 & b \\
0 & 1
\end{pmatrix} \, \mid \,  b \in \F_p
\right\} \simeq \Z/p.
$
Then $\widehat H = \Z/p$ consists of characters $\chi_0,\ldots, \chi_{p-1}$ where
$$
\chi_k(b) = e^{\frac{2\pi i}{p} kb}.
$$
We have 
$$
\chi^{\begin{pmatrix} a & b \\ 0 & 1\end{pmatrix}} = \chi(a^{-1} -).
$$
The stabilizer of $\chi_0$ is $G$. The stabilizers of $\chi_k$ for $k=1,\ldots,p-1$ are trivial in $G/H$, and $G$ acts transitively on $\{ \chi_1,\ldots, \chi_{p-1} \}$. 

The assumptions of the theorem are satisfied, as $\chi_0$ extends to $G$ by $\widetilde \chi_0(g) = 1$.

Consider the representatives of the two $G$-orbits on $\widetilde H$:

\underline{$\chi_0$}: Here $\mathcal I(\chi_0)=G$. We have $G/H \simeq \F_p^\times$. This leasds to $\lvert \F_p^\times \rvert = p-1$ one-dimensional representations. Namely, 
$$
G \ni \begin{pmatrix} a & b \\ 0 & 1
\end{pmatrix} \mapsto \chi(a)
$$
for $\chi \in \F_p^\times\simeq \Z/(p-1)$. 

\underline{$\chi_1$}: Here $\mathcal(\chi_1)=H$. Then we get one irreducible $p-1$-dimensional representation $\Ind_H^G \chi_1$. 
\end{example}

\subsection{Divisibility}

\begin{thm}
  Let $G$ be a finite group, $H \subset G$ a normal abelian subgroup, and $\pi$ an irreducible representation of $G$. Then $\dim \pi$ divides $\lvert G \rvert / \lvert H \rvert$. 
\end{thm}

\begin{proof}
Recall that we already proved this for $H=Z(G)$. For general $H$, we know that $\pi \simeq \Ind_I^G \sigma$ for some  $I \supset H$ and $\Res_H^I \sigma$ is isotypic to $\chi \in \widehat H$. 

Consider the groups $\widetilde I = \sigma(I)$ and $\widetilde H = \sigma(H) = \{\chi(h) \cdot 1 \, \mid \, h \in H \}$. 

Then $\widetilde H \subset Z(\widetilde I)$ and the representation of $\widetilde I$ on $V_\sigma$ is irreducible. Hence 
$$
\dim \sigma \mid \lvert \widetilde I / \widetilde H \rvert = \lvert I  / H \cdot \ker \sigma \rvert,
$$
so
$$
\dim \sigma \mid \lvert I / H \rvert.
$$
Hence 
$$
\dim \pi = \lvert G / I \rvert \dim \sigma  \mid \lvert G / I \rvert \lvert G / H \rvert = \lvert  G /H \rvert.
$$
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%55
\newpage
\section{Part II - Compact groups}

When one wants to develop a representation theory for infinite groups, one has to restrict the scope somewhat. Infinite groups are too wild in general, and they can have quite bad representation theory. For example, $\Z$ acting on $\C^2$ by $(x,y) \mapsto (x+ny,y)$ has the $y$-axis as an irreducible subspace, but the representation does not split, because there is no complementary invariant subspace. Thus the property of complete reducibility fails. 

We want to at least restrict to groups with a topology. But what made wonders for the theory of finite groups was the presence of an averaging operator. If we restrict to \emph{compact} groups, we will see that we can mimic this operator in this setting as well.

Recall that a \textbf{topological group} is a group $G$ equipped with a topology such that the group law and the inverse are continous maps. 

Here is a key fact:
\begin{prop}[Haar, von Neumann, Andr Weil]
For any locally compact group $G$ there exists a nonzero left-invariant Radon measure on $G$ which is unique up to scalar. We call such a measure a \textbf{Haar measure}.  
\end{prop}

Recall that a \textbf{Radon measure} $\mu$ on a locally compact space is a Borel measure which is outer regular, inner regular on open subsets and $\mu(K) < \infty$ for all compacts $K$.

To be given a Radon measure on a locally compact space $X$ is equivalent to be given a linear functional $F:C_c(X) \to \C$ such that $F(f) \geq 0$ if $f \geq 0$ (here $C_c(X)$ is the space of compactly supported functions on $X$). Thus we can think of a Radon measure as equivalent to having a way to integrate functions.

If $G$ is compact, we usually normalize so that $\mu(G)=1$. This makes the Haar measure unique.

\begin{example}
  Let $G=(\R,+)$. The the Haar measure is the usual Lebesgue measure.
\end{example}

There are also right-invariant measures. If the left- and right-invariant Haar measures coincide, then we say that the group $G$ is \textbf{unimodular}.

\begin{lemma}
Any compact group is unimodular.  
\end{lemma}
\begin{proof}
 Let $\mu_l$ be the left-invariant measure. Then the measure $\mu_l'(U)= \mu_l(Ug)$ is left-invariant as well. Hence $\mu_l' = \alpha \mu_l$ for some $\alpha \in \R$. But $1=\mu_l'(G) = \alpha \mu_l(G) = \alpha$. Hence $\mu_l'=\mu$.
Thus $\mu_l(Ug)=\mu_l'(U)=\mu_l(U)$, so $\mu_l$ is right-invariant as well.
\end{proof}

Many of the results for finite groups carry over to compact groups: the proofs are the same except expressions like $\frac{1}{\lvert G \rvert}$ are replaced by $\int_G f d \mu$.

A \textbf{representation} of a compact group $G$ is a continous homomorphism $\pi:G \to \GL(V)$.

Here are some results that holds for compact groups as well (with the same proofs):
\begin{itemize}
\item \textbf{Maschke's theorem:} Any finite-dimensional representation of $G$ is completely reducible.
\item Any finite-dimensional representation is unitarizable.
\item The orthogonality relations. In particular, two finite-dimensional representations of a $G$ are equivalent if and only if the characters coincide. 
\end{itemize}

The notion of regular representation is however more complicated. We are forced to (briefly) introduce infinite-dimensional spaces.

A \textbf{unitary} representation of a topological group $G$ on a Hilbert space $\mathcal H$ is a homomorphism $\pi:G \to U(\mathcal H)$, continous in the \textbf{strong operator topology}, meaning the following: if $g_i \to g$ in $G$, then $\pi(g_i) \zeta \to \pi(g)\zeta$ for all $\zeta \in \mathcal H$. This can be rephrased as saying that the map $G \to \mathcal H$ given by $g \mapsto \pi(g) \zeta $ is continous for all $\zeta \in \mathcal H$.

\begin{example}
 Let $G$ be a locally compact group with fixed Haar measure. Consider $L^2(G)$ (the vector space of square integrable functions on $G$) with respect to the given measure. Define
$$
\xymatrix @ R = 0mm{
G \ar[r] &U(L^2(G)) \\
g \ar@{|->}[r] & (\lambda(g)f)(h) = f(g^{-1}h).
}
$$
Then $\lambda$ is continous in the above sense.

We call $\lambda$ the \textbf{regular representation of the compact group $G$}.
\end{example}

\begin{thm}[Peter-Weyl]
\label{thmpeterweyl}
Let $G$ be a compact group and $\pi:G \to U(\mathcal H)$ a unitary representation. Then $\pi$ decomposes into a direct sum (in the sense of Hilbert spaces) of finite-dimensional irreducible representations.
  
\end{thm}

\begin{proof}
A sketch was given in the lecture.
\end{proof}

\begin{corr}
For any compact group $G$, the intersection of all kernels of irreducible representations on $G$ is trivial.
\end{corr}
\begin{proof}
The regular representation decomposes into a sum of irreducibles, and is injective. 
\end{proof}

Denote by $\C[G]$ the linear span of matrix coefficients of finite dimensional representations of $G$. This is an algebra under pointwise multiplication: the product of two matrix coefficients (from representations $\pi_1,\pi_2$, say) is a matrix coefficient in $\pi_1 \otimes \pi_2$. This algebra is closed under complex conjugation, since the matrix coefficient of the dual representation is given by $\overline{\pi(g^{-1})}_{ji}$. Thus $\C[G]$ is a $C^\ast$-algebra.

\begin{corr}
  The algebra $\C[G]$ is norm-dense in $C(G)$.
\end{corr}
\begin{proof}
Recall the Stone-Weierstrass theorem: if $A$ is a subalgebra of $C(G)$ that separates points, then $A$ is norm-dense in $C(G)$. So let $g_1,g_2 \in G$. By Theorem \ref{thmpeterweyl}, there exist non-trivial representations of $G$. Then [[details later]]
\end{proof}

The algebra $\C[G]$ is called the \textbf{algebra of regular/polynomial functions on $G$}.

\begin{thm}[Peter-Weyl]
  For every compact group $G$, we have 
$$
\lambda \sim \bigoplus_{[\pi] \in \widehat G} \pi^{\dim \pi_i},
$$
where $\lambda:G \to U(L^2(G))$ is the regular representation.
\end{thm}

Just as in the case of finite groups, it is shown that the characters of finite-dimensional representations form an orthonormal basis in the space 
$$
\{ f \in L^2(G) \mid f(g - g^{-1}) = f(-) \forall g \in G \} \subseteq L^2(G).
$$

In fact, for any compact group $G$, the characters of finite-dimensional represetantions of $G$ span a \emph{dense} subspace in the space of continous central functions on $G$. 

\begin{thm}
  Suppose $G$ is a compact subgroup of the unitary group $U(n) \subseteq \R^{n\times n}$. Then $\C[G]$ is the unital algebra generated by the matrix coefficients $a_{ij}$ and $\det^{-1}$. 
\end{thm}

\begin{proof}
  LATER
\end{proof}

%%%%%%%%%%%
\newpage
\section{Representation theory of compact Lie groups}

A \textbf{Lie group} is a smooth manifold $G$ equipped with smooth maps $\mu:G \times G \to G$ and $\iota:G \to G$ making $(G,\mu,\iota)$ into a group.

Equivalently, a Lie group is a group object in the category of smooth manifolds.

\begin{example}
 Let $V$ be a finite-dimensional vector space. Then $\GL(V)$ is a Lie group. The smooth structure is inherited from $\End(V) \simeq \R^N$, since $\GL(V)$ is an open subset of $\End(V)$.
\end{example}

Before giving further examples, we will develop some general results.

\subsection{The exponential map}

We recall some results from manifold theory.

Let $M$ be a smooth manifold and let $p \in M$. The tangent space $T_pM$ can be equivalently defined as
\begin{itemize}
\item Equivalence classes of smooth curves passing through $p$: let $\gamma_i:(a_i,b_i) \to M$, with $a_i < 0 < b_i$ and $\gamma_i(0)=p$ be two such curves ($i=1,2$). Then $\gamma_1 \sim \gamma_2$ if and only if 
$$
(x \circ \gamma_1)'(0) = (x \circ \gamma_2)'(0)
$$
for some/any coordinate system $(x,U)$ around $p$. We denote the equivalence class by $\gamma'(0)$, or
\item the more algebraic definition: the space of derivations of $C^\infty (M)$ at $p$. This is by definition a linear map $L:C^\infty(M) \to \C$ such that $L(fg) = f(p)L(g) + g(p)L(f)$.
\end{itemize}
The relation between these two definitions is that the derivation $L$ defined by $\gamma'(0)$ is
$$
L(f) = \restr{\dd{}{f}(f \circ \gamma)}{t=0}.
$$
In local coordinates $(x,U)$, any tangent vector can be written as
$$
\sum a^i(p) \restr{\dd{}{x^i}}{p}.
$$
Recall that a smooth vector field is a smooth section of the tangent bundle. The space of vector fields can be identified with the space of derivations of $C^\infty(M)$, i.e. as linear maps $L:C^\infty(M) \to C^\infty(M)$ satisfying the Leibniz rule.

Given a vector field $X$, an \textbf{integral curve of $X$} is a smooth curve $\gamma:(a,b) \to M$ such that $\gamma'(t) = X_{\gamma(t)}$ for all $t \in (a,b)$. It is a standard fact of manifold theory that for any $p \in M$, there exists a unique maximal integral curve passing through $p$. 

Recall also: if $f:M \to N$ is a smooth map, then the differential at $p$ is the map  $d_pf:T_pM \to T_{f(p)}N$ given by $\gamma'(0) \mapsto (f \circ \gamma)'(0)$. 

Now let $G$ be a Lie group. For each $g \in G$, we have diffeomorphism $l_g:G \to G$ defined by left-multiplication: $h \mapsto gh$. This induces a map on vector fields:
$$
(l_g)_\ast(X)_p = (d_{g^{-1}(p)} l_g)(X_{g^{-1}p}).
$$

We say that a vector field $X$ on a Lie group $G$ is \textbf{left-invariant} if $(l_g)_\ast(X)=X$ for all $g \in G$. Such a vector field is determined by its value at $e \in G$: $X_g = (d_e l_g)(X_e)$.

Thus we get a one to one correspondence between left-invariant vector fields and $v \in T_eG$.
 
\begin{prop}
 Let $G$ be a Lie group, $X \in T_eG$ and $\alpha_X:(a,b) \to G$ be the maximal integral curve of the corresponding left-invariant vector field such that $\alpha_X(0)=e$.

Then $(a,b)=\R$ and $\alpha_X(s+t) = \alpha_X(s)\alpha_X(t)$ for all $s,t \in R$.
\end{prop}

A \textbf{1-parameter subgroup of $G$} is a smooth map $\gamma:\R \to G$ that is a group homomorphism.

\begin{corr}
 The map $X \mapsto \alpha_X$ is a bijection between $T_eG$ and one-parameter subgroups of $G$.
\end{corr}
\begin{proof}
 Assume $\gamma:\R \to G$ is a one-parameter subgroup of $G$. Put $X = \gamma'(0) \in T_eG$. Then $\gamma = \alpha_X$:
$$
\gamma'(t) = (d_e \gamma(t))(\gamma'(0))
$$
as $\gamma(t+s)=\gamma(t)\gamma(s)$. [[why does it follow??]]
\end{proof}

\begin{remark}
 In general $\gamma(\R)$ is not a closed subgroup of $G$. Take for example $\R \to S^1 \times S^1$ given by $t \mapsto (e^{it},e^{i\theta t})$ for some irrational $\theta$. Then the image is dense in $S^1 \times S^1$.
\end{remark}

We define the \textbf{exponential map} as $\exp:T_e G \to G$ given by $X \mapsto \alpha_X(1)$. 

Note that for any $s \in \R$
$$
\dd{}{t} \alpha_X (st) = s \alpha_X'(st) = s X_{\alpha_X(st)}.
$$
Thus $t \mapsto \alpha_X(st)$ is the integral curve of the vector field corresponding to $sX$, hence $\alpha_X(st)= \alpha_{sX}(t)$. 

Therefore $\exp(tX)=\alpha_{tX}(1) = \alpha_X(t)$. So by the corrollary, all 1-parameter subgroups of $G$ have the form $t \mapsto exp(tX)$. 

We have
$$
\restr{\dd{}{t} \exp(tX)}{t=0} = X.
$$
In other words, if we identify $T_0(T_eG)$ with $T_eG$, we conclude that $d_0 \exp = \id_{T_eG}$. Thus $\exp$ gives us a diffeomorphism between a neigbourhood of $0 \in T_eG$ and a neighbourhood of the identity $e \in G$.

That $\exp$ is smooth is a standard result on integral curves (see e.g. Spivak).

\begin{thm}
  The exponential map is the unique smooth map $\exp:T_eG \to G$ such that $\exp((s+t)X) = \exp(sX)\exp(tX)$ and
$$
\restr{\dd{}{t} \exp(tX)}{t=0}=X
$$
\end{thm}
In general, the exponential map is neither injective nor surjective, even for connected groups.
\begin{example}
Let $G = \GL(V)$. As $\GL(V)$ is a open subset of $\End(V)$, the tangent space at every $M \in \GL(V)$ is identified with $\End(V)$ for all $M \in \GL(V)$. Then $\exp:\End(V) \to \GL(V)$ is given by 
$$
X \mapsto e^X = \sum_{n=0}^\infty \frac{X^n}{n!}.
$$
\end{example}

A \textbf{homomorphism of Lie groups} is a homomorphism $\pi:G \to H$ which is also smooth.

\begin{thm}
 Assume that $\pi:G \to H$ is a Lie group homomorphism. Then the diagram
$$
\xymatrix{
G \ar[r]^\pi & H \\
T_eG \ar[u]^{\exp} \ar[r]_{d_e\pi} & T_e H \ar[u]_\exp
}
$$
is commutative.
\end{thm}

\begin{example}
  Let $G = \GL(V)$. Consider the map $\det:\GL(V) \to K^\times = \GL(K)$. Then $d_e \det = trace$. Then the theorem says that $\det(e^A)=e^{trace(A)}$.
\end{example}

\subsection{The Lie algebra of a Lie group}

Recall that for vector field $X,Y$ on a manifold $M$, there is a bracket operation $[X,Y]$. If $L_X,L_Y$ are the corresponding derivations, then $[X,Y]$ is defined as the vector field corresponding to the derivation $L_{[X,Y]} = L_X L_Y - L_Y L_X$.\footnote{It is not immediately obvious that this is a derivation, but it is easily checked.} If $X$ and $Y$ are given in local coordinates as 
$$
X = \sum f^i \dd{}{x^i}, \, Y = \sum g^i \dd{}{x^i},
$$
respectively, then
$$
[X,Y] = \sum_{i,j} \left(f^i \dd{g^j}{x^i} - g_i \dd{f^j}{x^i} \right) \dd{}{x^j}.
$$

The definition of a Lie algebra over a field $k$ is then the following: a \textbf{Lie algebra} is a vector space $\g$ over $k$ equipped with a bilinear form $[-,-]$ called the \textbf{Lie bracket} that is skew-symmetric and satisfies the \textbf{Jacobi identity}:
$$
[X,[Y,Z]] + [Z,[X,Y]] + [Y,[Z,X]] = 0.
$$

The Jacobi identity can be thought of as a "linearization of associativity". 

We want to compute the commutator more explicitly. Let $\gamma_X$ be a curve on $G$ with $\gamma_X(0)=e$ and $\gamma'_X(0)=X$. Then for any $g \in G$, the curve $g \gamma_X$ satisfies $(g \gamma_X)(0)=g$ and $(g \gamma_X)'(0) = (d_e l_g)(X)$. 

Hence we have 
$$
L_X(f)(g) =\restr{ \frac{d}{dt} f(g \gamma_X(t))}{t=0}
$$
Then
\begin{align*}
(L_XL_Y)(f)(g) &= \restr{\frac{d}{dt} L_Y(f) (g \gamma_X(t))}{t=0} \\
&= \restr{\frac{\partial^2}{\partial s \partial t} f(g \gamma_X(t) \gamma_Y(s))}{s=t=0}
\end{align*}
Hence

\begin{align}
(d_ef)([X,Y]) &= (L_X L_Y - L_Y L_X)(f)(e) \nonumber \\
&= \restr{
\frac{\partial^2}{\partial s \partial t} (f(\gamma_X(t)\gamma_Y(s))-f(\gamma_Y(s)\gamma_X(t)))}{s=t=0} \label{eqLie}
\end{align}

This is reasonable explicit, but can be made more so by working with coordinates. Choose local coordinates $(x,U)$ around the identity $e \in G$. We can identity the codomain of $x:U \to \R^n$ with $\mathfrak g=T_eG$. We require that $x(e)=0$ and $d_ex = \id$ (we can for example choose $x = \exp^{-1}$). 

Now consider the map $m:V \times V \to \mathfrak g$ given by $(X,Y) \mapsto x(x^{-1}(X)x^{-1}(Y))$. This is well-defined for a small enough neighbourhood $U$ of $0 \in \mathfrak g$. 

Applying equation \eqref{eqLie} to $f = \ell \circ x$ (where $\ell$ is a linear functional on $\g$) we get 

\begin{align*}
\ell([X,Y]) &= \restr{\frac{\partial^2}{\partial s \partial t} \ell(x(x^{-1}(tX)(x^{-1}(sY))-x(x^{-1}(sY)x^{-1}(tX)))}{t=s=0} \\
&= \ell \left(\restr{\frac{\partial^2}{\partial s \partial t} (m(tX,sY) - m(sY,tX))}{t=s=0} \right)
\end{align*}

Therefore 

$$
[X,Y] = \restr{\frac{\partial^2}{\partial s \partial t} (m(tX,sY) - m(sY,tX))}{t=s=0} 
$$

This can further be written as follows: as $m(0,0)=0$, $m(X,0)=X$ and $m(0,Y)=Y$, the Taylor expansion of $m$ at $(0,0)$ has the form
$$
m(X,Y) = X+Y + B(X,Y) + h.o.t
$$
where $B: \g \times \g \to \g$ is a bilinear map [[WHY?????]] 

Hence in this description, we have that
$$
[X,Y] = B(X,Y) - B(Y,X).
$$

\begin{example}
Let $G= \GL(V)$ for $V$ a finite-dimensional real or complex vector space. Choose coordinates by setting $x(A) = A-I$. Then
$$
m(X,Y) = (1+X)(1+Y)-1  = XY+ X+Y.
$$
Hence in this case $[X,Y]= XY-YX$. 
\end{example}

The space $\End V=T_e \GL(V)$ with this bracket is denoted by $\mathfrak {gl}(V)$. 

With this done, here's a theorem:

\begin{thm}
If $\pi:G \to H$ is a Lie group homomorphism, then $\pi_\ast= d_e \pi: \g \to \mathfrak h$ is a Lie algebra homomorphism. That is, the map $\pi_\ast$ preserves the bracket:
$$
\pi_\ast([X,Y]) = [\pi_\ast(X), \pi_\ast(Y)].
$$
Also, if $G$ is connected, $\pi$ is completely determined by $\pi_\ast$.
\end{thm}
\begin{proof}
Recall first that for all $X \in \g$ we have $\pi(\exp(t X)) = \exp(t \pi_\ast (X))$. First off, we have, as above, that
$$
L_X(f)(g) = \restr{ \frac{d}{dt} (f(g \exp(tX)))}{t=0}
$$
and one also verifies that
$$
L_{\pi_\ast(X)} (f) (\pi(g)) = L_X(f \circ \pi)(g).
$$
This follows more or less directly from the definitions. 

[[[[ rest of the proof here ]]]]
\end{proof}



%%%%%%%%%%%
\newpage
\section{Lie subgroups}

By definition, a \textbf{closed Lie subgroup} of a Lie group $G$ is a subgroup $H$ which is also a closed submanifold. 

Then $H$ is itself a Lie group. We can identify $\mathfrak h = T_e H$ with a subspace of $\g = T_eG$. As the embedding $H \hookrightarrow G$ is a Lie group homomorphism, this gives an identification of $\mathfrak h$ with a Lie subalgebra of $\g$. 

In fact, the condition that $H$ is a submanifold is redundant:

\begin{thm}
\label{thmclosedlie}
If $G$ is a Lie group and $H$ is a closed subgroup, then $H$ is a closed submanifold.
\end{thm}
\begin{proof}
This will be a bit tricky.

Consider the subspace $V \subset \g$ consisting of $X \in \g$ such that there exist $X_n \in \g$ and $\alpha_n \in \R$ such that $\exp(X_n) \in H$ and $\alpha_n X_n \to X$ as $ n \to \infty$ and also $\lvert \alpha_n \rvert \to \infty$ as $n \to \infty$.

Note that $V$ is non-empty since $0 \in V$.

\textbf{Claim 1:} If $X \in V$, then $\exp(tX) \in H$ for all $t \in \R$.

Take $t \in \R$. As $\lvert \alpha_n \rvert \to \infty$, we can find $m_n \in \Z$ such that 
$$
\frac{m_n}{\alpha_n} \to t
$$
as $t \to \infty$ (take for example $m_n=[\alpha_n t ]$). Then
\begin{align*}
H \ni   \exp(X_n)^{m_n} &= \exp(m_n X_n) \\
&= \exp(\frac{m_n}{\alpha_n} \alpha_n X_n) \\
&\to \exp(t X). 
\end{align*}
This implies that $\exp(tX) \in H$, since $H$ is closed.

\textbf{Claim 2:} The set $V \subset \g$ is a sub-vector space of $\g$.

By Claim 1, $V$ is closed under multiplication by scalars. Therefore, we have to show that if $X,Y \in V$, then $X+Y \in V$. 

Consider the map $\gamma(t)=\exp(tX)\exp(tY)$. Then $\gamma'(0)=X+Y$. For small $\lvert t \rvert$, we can write $\gamma(t)=\exp(f(t))$, where $f$ is a smooth $\g$-valued function, since $\exp$ is a local diffeomorphism at $e$. 

Then $f'(0) = X+Y$, so $\frac{f(t)}{t} \to X+Y$ as $t \to 0$. In particular, $nf(\frac{1}{n}) \to X+Y$ as $n \to \infty$. By definition of $\gamma$, we have $\exp(f(\frac 1 n)) \in H$. Now by definition of $V$, letting $\alpha_n=n$ and $X_n = f(\frac 1n)$, we conclude that $X+Y \in V$. 

\textbf{Claim 3:} $H$ is a closed submanifold with $T_e H = V$. 

Let $V'$ be a complementary subspace to $V$ in $\g$, i.e. a subspace such that $\g = V \oplus V'$. 

Consider the map $\varphi: V \times V' \to G$ defined by
$$
\varphi(X,Y) = \exp(X) \exp(Y).
$$
Then $d_{(0,0)}\varphi = \id_\g$, so we can find a neighbourhood $\Omega$ of $0$ in $V$ and $\Omega'$ of $0 \in V'$ such that $\varphi$ is a diffeomorphism of $\Omega \times \Omega'$ onto $\varphi(\Omega \times \Omega')$. 

We will show that for $n$ large enough, we have the equality
$$
\varphi \left(\Omega \times \frac 1n \Omega' \right) \cap H  = \exp(\Omega \times \{0\} ).
$$

Suppose this is not true. Then we can find $(X_n,X_n') \in \Omega \times \Omega'$ such that $\exp(X_n) \exp(\frac{X_n'}{n}) \in H$ and $X_n' \neq 0$. 

Take any limit point $X'$ of the sequence $\frac{X_n'}{\| X_n \| }$. Then $X' \neq 0$ and $X' \in V'$ and $\exp(\frac{X_n'}{n}) \in H$ (since the left term is in $H$). Then also
$$
\frac{n}{\| X_n' \|} \cdot \frac{X_n'}{n} \to X',
$$
Then by definition of $V$, we must have $X' \in V$. This is a contradiction, since $V'$ is closed. 

Hence for some $n$, we have $\varphi \left (\Omega \times \frac 1n \Omega' \right) \cap H = \exp(\Omega \times \{0\})$. Thus, letting $U= \varphi \left(\Omega \times \frac 1n \Omega'\right)$, we can take as coordinate system $x=\varphi^{-1}$, such that
$$
U \cap H = \{ g \in H \, \mid \, \text{ the coordinates $x(g)$ in $V'$ are zero } \}.
$$
Thus $H$ satisfies the definition of a closed submanifold near the identity $e$. For arbaitry $h \in H$, we can take the neigbourhood $hU$ of $h \in G$ and coordinates $x_h(-) = x(h^{-1}-)$.
\end{proof}

Here's a surprising consequence:

\begin{thm}
If $\pi:G \to H$ is a continous homomorphism of Lie group, then $\pi$ is smooth.
\end{thm}

In particular, for Lie groups, there is no difference between finite-dimensional representations of locally compact groups and Lie groups. 

\begin{proof}
  Consider the graph $\Gamma$ of $\pi$:
$$
\gamma = \{ (g,\pi(g) ) \, | \, g \in G \} \subset G \times H.
$$
Since $\pi$ is continous, $\Gamma$ is a closed subgroup of the product. Hence by the theorem, $\Gamma$ is a also a closed submanifold.

Consider the projection $\pi:G \times H \to G$, and let $q = \restr{p_1}{\Gamma}$. Then $q$ is a smooth isomorphism between $\Gamma$ and $G$. 

Since $q$ is a Lie group homomorphism, it has constant rank (proof: for all $x \in \Gamma$, we have $q=l_{q(x)^{-1}} \circ q \circ l_x$). 

Now, by the constant rank theorem, a constant rank bijection is a diffeomorphism. Thus $q^{-1}$ is smooth.

But $\pi$ is the composition of $q^{-1}$ with $p_2:G \times H \to H$, so $\pi$ is smooth.
\end{proof}

\begin{prop}
  If $\pi:G \to H$ is a Lie group homomorphism, then $\ker \pi$ is a closed Lie subgroup of $G$ with Lie algebra $\ker( \pi_\ast: \g \to \mathfrak h)$.
\end{prop}

\begin{proof}
 Clearly $ \ker \pi$ is closed, hence it is a closed subgroup. Hence it is a closed Lie subgroup by Theorem \ref{thmclosedlie}. So we need to show the second statement (let $\catname{LieAlg}(G)$ for the moment denote the Lie algebra of a Lie group):
 \begin{align*}
 X \in \catname{LieAlg}(\ker \pi) &\Leftrightarrow \exp(tX) \in \ker \pi \, \forall t \\
&\Leftrightarrow \pi(\exp tX) = e \, \forall t \\
&\Leftrightarrow \exp(t \pi_\ast X) = e \, \forall t \\
&\Leftrightarrow \pi_\ast(X) = 0,
 \end{align*}
so $X \in \ker \pi_\ast$.
\end{proof}

\begin{prop}
  Let $\pi:G \to \GL(V)$ be a representation of a Lie group $G$ on a finite-dimensional real or complex vector space $V$. Take any $v \in V$. Then
$$
H = \{ g \in G \, | \, \pi(g) v = v \}
$$
is a closed Lie subgroup of $G$ with Lie algebra
$$
\mathfrak h = \{ X \in \g \, | \, \pi_\ast(X) v = 0 \}.
$$
\end{prop}

\begin{proof}
 That $H$ is closed Lie subgroup is immediate.

Suppose $X \in \mathfrak h$. Then $\exp(t X) \in H$ for all $t$. But this means that $\pi(\exp(tX))v=\exp(t\pi_\ast( X))v = v$ for all $t$. But differentiating both sides give $\pi_\ast(X)v = 0$. 
\end{proof}


%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Classical Lie groups}

This section will give examples of Lie groups and their Lie algebras explicitly in terms of matrices. The \textbf{classical Lie groups} are groups of linear transformations preserving some symmetric/skew-symmetric/Hermitian form on a real/complex/quaternionic space, as well as their special versions. 

\subsection{The general linear group}

Let $V$ be a real or complex vector space of dimension $n$. Then $\GL(V)$ is the set of invertible linear operators $V \to V$. If a basis for $V$ is given, then $\GL(V)$ is realized as the set of invertible $n \times n$ matrices with determinant non-zero.

If $V$ is a real vector space, then the dimension of $\GL(V)$ is $n^2$, and if $V$ is complex, then the dimension is $2n^2$. 

The Lie algebra of $\GL(V)$, denoted by $\mathfrak {gl}(V)$ is identified with $\End(V)$, or, if a basis is given, with $\mathrm{Mat}_{n,n}(V)$, since $\GL(V)$ is an open subset in $\End(V)$.

\subsection{The special linear group}

Again, let $V$ be a real or a complex vector space. The \textbf{special linear group} $\SL(V)$ is by definition the subset of $\GL(V)$ consisting of operators of determinant $1$. Since this is a closed condition, it follows from Theorem \ref{thmclosedlie} that $\SL(V)$ is actually a closed Lie subgroup of $\GL(V)$. Note that $\dim \SL(V) = n^2-1$ if $V$ is a real vector space.

I claim that the Lie algebra of $\SL(V)$ is the space of matrices in $\End(V)$ of trace zero. To prove this, let $\{ A_t \}$ be an arc in $\SL(V)$ such that $A_0 = I_n$ and $\restr{\frac{d}{dt}}{t=0} A_t =X$. Then $A_t e_1 \wedge \ldots  \wedge A_t e_n = e_1 \wedge \ldots \wedge e_n$ for all $t$ since the determinant of $A_t$ is $1$ for all $t$. Thus, taking derivatives of both sides, we get

\begin{align*}
0 &= \restr{ \frac{d}{dt}}{t=0} A_t e_1 \wedge \ldots \wedge A_t e_n \\
&= \sum_{i=1}^n e_1 \wedge \ldots \wedge X e_i \wedge \ldots e_n \\
&= \tr(X) e_1 \wedge \ldots \wedge e_n.
\end{align*}

In the second line we used the product rule for the derivative (which works just as well with exterior products (in fact, with any bilienar operator). Hence $\tr(X)=0$. Counting dimensions, we see that the space of traceless matrices have the same dimension as $\SL(V)$, so that we can identify $\mathfrak{sl}(V)$ with the space of traceless matrices.


\subsection{Groups defined by preservation of a bilinear form}

Let $V$ be as above and consider a bilinear $B$ form on $V$. Let
$$
G = \{ g \in \GL(V) \, \mid  \, B(gv,gw)=B(v,w) \, \text{for all } v,w \in V \}.
$$

Then $G$ is a subgroup of $\GL(V)$. We shall verify that it is indeed a Lie subgroup.

Let $W$ be the space of bilinear forms on $V$. Note that $W \simeq V^\ast \otimes V^\ast$. Define a representation $\pi$ of $\GL(V)$ on $W$ by
$$
(\pi(g)B')(v,w) = B'(g^{-1}v,g^{-1}w).
$$
Then $G$ is exactly the set $\{  g \in \GL(V) \mid \pi(g) B = B \}$, so that from [[Propostion 2]], it follows that $G$ is a Lie group. with Lie algebra
$$
\g = \{X \in \mathfrak{gl}(V) \, \mid \, \pi_\ast(X) B = 0 \}.
$$

We want this a little more explicit. So note that we can write
$$
\pi_\ast(X) = \restr{\frac{d}{dt}\pi(\exp(t X)) }{t=0}
$$

This is a simple calculation using the functoriality of $\pi_ast$ and the definition of the exponential map. Hence we get

\begin{align*}
(\pi_\ast(X) B)(v,w) &= \restr{\frac{d}{dt} (\pi(\exp(tX))B)(v,w) }{t=0} \\
&= \restr{\frac{d}{dt}  B(e^{-tX} v, e^{-tX} w)}{t=0} \\
&= -B(Xv,w)-B(v,Xw).
\end{align*}
We used the chain rule in the last line.

Thus
$$
\g =  \{X \in \mathfrak{gl}(V) \, \mid \, B(Xv,w)+B(v,Xw) = 0 \}.
$$

The dimension of $G$ will of course depend upon the choice of $B$.

\subsection{The orthogonal group $O(n)$}

Let $V=\R^n$ and let $B$ be the usual scalar product
$$
(v,w) = \sum_{i=1}^n v_i w_i.
$$
Then the group $G$ for $B$ as above is called the \textbf{orthogonal group} and denoted by $O(n)$, $O_n(\R)$ or $O(n; \R)$. 

Note that we have
$$
(Xv,w)+(v,Xw) = (Xv,w)+(X^Tv,w)= ((X+X^T)v,w).
$$
This inner product should be zero for all $v,w$. Hence we have:
$$
\mathfrak{o}(n) = \{ X \in \mathfrak{gl}_n(\R) \, \mid \, X+X^T = 0 \},
$$
by the previous section. Thus the Lie algebra consists of all skew-symmetric matrices. We see that $O(n)$ have dimension $1+2+\ldots+(n-1)=n(n-1)/2$. The \textbf{special orthogonal group} $SO(n)$ is the subgroup of $O(n)$ whose determinants are $1$. In other words: $SO(n) = O(n) \cap \SL_n(R)$. This is the connected component of the identity of $O_n(\R)$. Then the Lie algebra is given by
$$
\mathfrak{sl}_n(\R) = \{ X \in \mathfrak{gl}_n(\R) \, \mid \, X^T=-X, \, \tr X = 0 \},
$$
which is equal to $\mathfrak{o}(n)$ since the condition $X^T=-X$ implies that $\tr X = 0$.

We can do the same for $V= \C^n$ with the same bilinear form. The corresponding groups are denoted by $O_n(\C)$ and $SO_n(\C)$. These are however not compact anymore, since $B \otimes \C$ is not positive definite.

\subsection{Indefinite orthogonal groups $O(k,l)$}

Let $V=\R^{k+l}$ for $k, l \geq 1$ and consider the bilinear form
\begin{align*}
B(v,w) &= \sum_{i=1}^k v_iw_i - \sum_{i=k+1}^{k+l} v_iw_i \\
&= (Av,w)
\end{align*}
where
$$
A = \begin{pmatrix} \
I_k &  0 \\
0 & -I_l \end{pmatrix}.
$$
The group of transformations preserving this form is then by definition the indefinite orhogonal group $O(k,n)$. As above, we see that its Lie algebra is
$$
\mathfrak{o}(k,l) = \{ X \in \mathfrak{gl}_{k+l}(\R) \, \mid \, AX+X^TA=0 \} . 
$$

This can be made even more explicit. Let $X \in \mathfrak{o}(k,l)$. Write
$$
X = \begin{pmatrix}
P & Q \\
R & S
\end{pmatrix},
$$
where $P$ is a $k \times k$ matrix, $Q$ is a $l \times k$ matrix, $R$ is a $k \times l$ matrix, and $S$ is a $l \times l$ matrix. Then one can check that the condition for $X$ to be in $\mathfrak{o}(k,l)$ is equivalent to $P=-P^T$, $S=-S^T$ and $R=Q$. Thus the dimension of $\mathfrak{o}(k,l)$ is
$$
\frac{k(k-1)}{2} + \frac{l(l-1)}{2} + kl = \frac{n(n-1)}{2}.
$$

We also have its special version: $SO(k,l) := O(k,l) \cap SL_{k+l}(\R)$. 

\subsection{The unitary group $U(n)$}

Let $V=\C^n$, and consider the Hermitian form
$$
\langle v, w \rangle = \sum_{i=1}^n v_i \overline{w_i}.
$$
The \textbf{unitary group} $U(n)$ is the group of $\C$-linear transformations preserving this form. The proof in the case of $O(n)$ goes through here as well, so that we see that
$$
\mathfrak{u}(n) = \{ X \in \mathfrak{gl}_n(\C) \, \mid \, X = -X^\ast \},
$$
where the star means the conjugate transpose of $X$. We also have the \textbf{special unitary group} $SU(n) = U(n) \cap \SL_n(\C)$ with Lie algebra
$$
\mathfrak{su}(n) = \{ X \in \mathfrak{gl}_n(\C) \, \mid \, X = -X^\ast, \, \tr X = 0\}.
$$
The dimension of $U(n)$ is $n^2$. To see this, note that matrices in the Lie algebra consist of real entries along the diagonal, plus free entries above the diagonal (the entries below the diagonal are minus conjugate those above). Thus the total freedom is $n+2 \cdot \frac{n(n-1)}{2} = n^2$.

\subsection{The symplectic Lie groups $Sp(2n,\R)$}

Let $V= \R^{2n}$ or $V= \C^{2n}$ and consider
\begin{align*}
  (v,w) &= \sum_{i=1}^{2n} (v_i w_{i+n}-v_{i+n}w_i) \\
&= (Jv,w)
\end{align*}
where
$$
J =
\begin{pmatrix}
  0 & -I_n \\
-I_n & 0 
\end{pmatrix}.
$$
The group of transformations preserving this form are the \textbf{symplectic groups} $Sp(2n,\R)$ or $Sp(2n,\C)$. The Lie algebra is
$$
\mathfrak{sp}(2n;k) = \{ X \in \mathfrak{gl}_n(k) \, \mid \, JX+X^TJ = 0 \}.
$$
In the same way is with the indefinite orthogonal group, one can compute the Lie algebra more explicitly. Namely, write $X$ as 
$$
X =
\begin{pmatrix}
  A & B \\
C & D
\end{pmatrix}.
$$
Here $A,B,C$ and $D$ are $n \times n$ matrices. Here the condition on $X$ to be in the Lie algebra is equivalent to $D=-A^T$ and $B$ and $C$ being skew-symmetric matrices. Hence the dimension is $n^2+2 \cdot \frac{n^2-n}{2} = 2n^2-n$.

\subsection{The compact symplectic group $Sp(n)$} 

This is the group defined by $Sp(n) = Sp(2n; \C) \cap U(2n)$ with Lie algebra
$$
\mathfrak{sp}(n) = \{ X \in \mathfrak{gl}_n(\C) \, \mid \, X+X^*=0, \, JX+X^TJ = 0\},
$$
where $J$ is as above. It can be shown that $Sp(n)$ is the subgroup of $\GL_n(\mathbb H)$ preserving $\sum_{i=1}^n v_i \overline{w_i}$. 

%&&&&&%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Simply connected Lie groups}

There is a close correspondence between the category of Lie algebras and the category of Lie groups. In particular, it is true that any Lie subalgebra correspond to a Lie subgroup with that Lie algebra.

To show this, we need to remind ourselves of some definitions from manifold theory, however.

\subsection{Some facts about distributions}

Let $M$ be a smooth manifold and let $D \subset TM$ be a subbundle of the tangent bundle of $M$. Then an immersed submanifold $N \subset M$ is called an \textbf{integral manifold} if $T_pN = D_p$ for all $p \in N$.

Here's the so-called \emph{Frobenius integrability condition}:

\begin{thm}
Suppose $D$ is integrable. Then for all $p_0 \in M$ , there exists a coordinate chart $(x,U)$ around $p_0$ such that
$$
D_p = \Span \left\{ \restr{\frac{\partial}{\partial x^1}}{p}, \ldots, \restr{\frac{\partial}{\partial x^k}}{p} \right\} 
$$
for all $p \in U$.
\end{thm}
The proof is surprisingly short, and can be found in for example Spivak's book \cite{spivak_manifolds}. 

This implies that there locally exists a unique integral manifold passing through a given point:
$$
N = \{ p \in U \, | \, x^{j}(p) = x^{j}(p_0) \text{ for } j=k+1,\ldots,n \}.
$$
Uniqueness means that if $N'$ is another integral manifold passing through $p$, then $N \cap N'$ is open in $N$ and $N'$. 

This implies that every point lies in a unique maximal connected integral manifoold, called \textbf{a leaf of the foliation defined by $D$}.

\subsection{Back to Lie subgroups}

\begin{thm}
Let $G$ be a Lie group with Lie algebra $\g$. Let $\mathfrak h \subset \g$ be a Lie subalgebra.

Then there exists a unique Lie subgroup $H \subset G$ with Lie algebra $\mathfrak h$.
\end{thm}
\begin{proof}
  Consider the distribution $D$ on $G$ defined by $D_g = (d_el_g)(\mathfrak h)$. As $\mathfrak h$ is a Lie subalgebra of $\g$, it is integrable.

Since left translations map $D$ into itself, they map leaves into leaves of the corresponding foliation. 

It follows that if $H$ is the leaf passing through $e$, then $H=hH$ for all $h \in H$. Therefore $H \subset G$ is a subgroup. It is a Lie subgroup because it is locally closed.

Now, if $K \subset G$ is another connected Lie subgroup with Lie algebra $\mathfrak h$, then $K$ would be an integral manifold of $D$ passing through $e$. Thus $K$ is an open subgroup. But $K$ is closed as well, since
$$
K = H \bs \left(\bigcup_{h \not \ in K} h K\right).
$$
As $H$ is connected, $H \subset K$.
\end{proof}

\begin{prop}
  Assume $\pi:G \to H$ is a Lie group homomorphism with $G$ connected. Then $\pi$ is completely determined by $\pi_\ast: \g \to \mathfrak h$. 
\end{prop}

\begin{proof}
  Consider the subgroup $K$ generated by $\exp \g \subset G$. Since $\exp \g$ contains an open neighbourhood of $e$, $K$ is an open subgroup of $G$. But this implies that $K$ is closed, hence $K=G$.

Thus, the statement is proved since  $\pi(\exp X) = \exp \pi_\ast X$, and every element of $G$ is a product of terms looking like $\exp X$.
\end{proof}

\begin{remark}
  This proves that the functor $\catname{LieAlg}:\catname{LieGrps} \to \catname{LieAlgs}$ sending a Lie group to its Lie algebra is faithful.

It is \emph{not} full, however. Take for example $G=S^1$ and $H=\R$. Then $\g = \mathfrak h = \R$, so $\Hom_{\catname{LieAlg}}(\g,\mathfrak h)=\R$, but $\Hom_{\catname{LieGrp}}(S^1,\R) = \{ 0\}$\footnote{Proof: Any such homomorphism must have compact image. But the image must also be discrete, since otherwise it would generate $\R$. Hence the image is a finite subgroup of $\R$. But the only finite subgroup of $\R$ is $\{0 \}$.}.
\end{remark}

Recall that a map $p:N \to M$ is a \textbf{covering map} if $p$ is continous and surjective and a local homeomorphism such that each point $p \in M$ has a neigbourhood $U$ such that the pre-image decomposes as
$$
p^{-1}(U) = \bigsqcup_{i \in I} U_i
$$
with $\restr{p}{U_i}:U_i \to U$ a homeomorphism.


\begin{example}
\label{excovering}
 Suppose that $\pi:G \to H$ is a Lie group homomorphism with $H$ connected, such that $\pi_\ast: \g \to \mathfrak h$ is an isomorphism. Then $\pi$ is a covering map:

We can find $U \supset e \in G$ such that $\restr{\pi}{U}:U \to \pi(U)$ is a diffeomorphism. Choose a neighbourhood $V$ of $e \in G$ such that $VV^{-1} \subset U$. 

Note that as $\pi$ is open and $H$ is connected, $\pi$ is surjective. Then for all $g \in G$, we have
$$
\pi^{-1}(\pi(g) \pi(V)) = \bigsqcup_{h \in \ker \pi} hg V.
$$
We have to check that $h_1g \cap h_2gV = \emptyset$ for $h_1,h_2 \in \ker \pi$ and $h_1 \neq h_2$. 

Suppose $h_1gv_1 = h_2gv_2$ for some $v_1,v_2 \in V$. This is equivalent to $g^{-1}h_2^{-1}h_1g = v_2v_1^{-1} \in VV^{-1} \subset U$. The left hand side is in the kernel of $\pi$, but $\pi$ is a homeomorhpism on $U \ni e$, hence $g^{-1}h_2^{-1}h_1 g = e$, hence $h_1= h_2$. 
\end{example}

If a topological space is nice enough (for example a connected manifold), then there exists a unique universal covering space $\pi:\widetilde M \to M$ such that if $p:P \to N$ is any covering space and $f:M \to N$ is a continous map, then there exists a unique map $\widetilde f: \widetilde M \to P$ such that $p \circ \widetilde f = f \circ \pi$.

Furthermore, a covering map $\lambda: Q \to M$ is universal if and only if $Q$ is simply connected. 

We can define $\widetilde M$ as the set of equivalence classes $[\gamma]$ of curves $\gamma:[0,1] \to M$ such that $\gamma_1 \sim \gamma_2$  if $\gamma_1(1)=\gamma_2(1)$ and $\gamma_1$ and $\gamma_2$ are homotopic within the class of  curves ending at $\gamma_1(1)=\gamma_2(1)$. This set has a natural topology. See e.g. Munkres \cite{hatcher_algtop}.

\begin{prop}
Let $G$ be a connected Lie group and $\pi: \widetilde G \to G$ a universal covering space. Then $\widetilde G$ has a natural structure of a Lie group so that $\pi$ becomes a Lie group homomorphism.
\end{prop}

\begin{proof}
  Suppose $\overline e \in \pi^{-1}(e)$. Then using that $\pi \times \pi: \widetilde G \times \widetilde G \to G \times G$ is a universal cover, there exist a unique lift of the product map $m: G \times G \to G$ to a map $\widetilde m: \widetilde G \times \widetilde G \to \widetilde G$ such that $\widetilde m (\widetilde e, \widetilde e) = \widetilde e$.

Then $\widetilde G$ is a Lie group with inverse given by the lift of the inverse such that $\widetilde e \mapsto \widetilde e$.
\end{proof}

\begin{thm}
  Let $G,H$ be Lie groups with $G$ simply connected. Then any Lie algebra homomorphism $\rho:\g \to \mathfrak h$ integrates to a Lie group homomorphism $\pi:G \to H$.
\end{thm}

\begin{proof}
  Consider the Lie subalgebra
$$
\pp = \{ (X,\rho(X)) \, | \, X \in \g \} \subset \g \oplus \mathfrak h.
$$
Let $P \subset G \times H$ be the connected Lie subgroup with Lie algebra $\pp$. Consider the projection $p_1: G \times H \to G$ and let $q = \restr{p_1}{P}$. Then $q_\ast : \pp \to \g$ is an isomorphism. 

Hence $q:P \to G$ is a covering map. As $G$ is already simplyconnected, $q$ must be an isomorphism. Then we can define $\pi$ as the composition of $q^{-1}:G \to P \subset G \times H$ with the projection $p_2:G \times H \to H$.
\end{proof}

\begin{corr}
 Two simply connected Lie groups are isomorphic if and only if $\g \simeq \mathfrak h$.
\end{corr}

Here is a theorem we will only state, known as \textbf{``Lie's third fundamental theorem''}, which is due to Cartan.

\begin{thm}
For any finite-dimensional real Lie algebra $\g$, there exists a simply connected Lie group $G$ with Lie algebra $\g$.
\end{thm}

Thus the category of simply connected Lie groups is equivalent to the category of Lie algebras.

One possible proof is based on \textbf{Ado's theorem}, which we also will not prove: 

\begin{thm}
Any finite-dimensional real Lie algebra is isomorphic to a Lie subalgebra of $\mathfrak{gl}_n(\R)$.
\end{thm}

Given this, to prove Lie's theorem, we can take $\mathfrak g \subset \mathfrak{gl}_n(\R)$. Then let $G$ be the Lie subgroup of $\GL_n(\R)$ corresponding to $\g$, and then pass to the universal covering.

\begin{remark}
  It is not true that every Lie group can be embedded in $\GL_n(\R)$. Here's an example: let $G$ be the universal cover of $\SL_2(\R)$. 

One can show that $\SL_2(\R)$ is a fiber bundle with fiber $\R$ over $\R^2 \bs \{ 0\}$, hence its fundamental group is $\Z$.

Complexifying, we get the corresponding group $\SL_2(\C)$, which in the same manner can be shown to have trivial fundamental group.

Now suppose we have a representation $p:G \to \GL_n(\R)$ for some $n$. By inclusion, we have a representation $G \stackrel{p}\to \GL_n(\R) \to \GL_n(\C)$. 

This is the same map as the composition
$$
G \stackrel{\pi}{\to} \SL_2(\R) \to \SL_2(\C) \to \GL_n(\C)
$$
where $\pi$ is the covering space map. Since the map on tangent spaces is the same. But $\pi$ is not injective, hence $p$ cannot be injective.
\end{remark}

If $G$ is connected but not simply-connected, then $G \simeq \widetilde G / \Gamma$ for some discret normal subgroup $\Gamma \subset \widetilde G$. It is easy to show that any such subgroup must be contained in the center of $G$.

\begin{example}
  The groups $SU(n), Sp(n)$ are simply connected.

The group $SO(n;\R)$ is connected, but not simply connected. We have $\pi_1(SO(2)) = \pi_1(S^1) = \Z$. For $n > 2$, we have $\pi_1(SO(n)) = \Z/2$. The universal covers are the ``spin groups''.

We will prove (almost) this later.
\end{example}

%%%%%%%%%%%%%%%%%%
\newpage
\section{The adjoint representation}

Let $G$ be a Lie group. For any $g \in G$ consider the automorphism $h \mapsto ghg^{-1}$ of $G$. Thus we have a map $G \to \Aut(G)$. Its differential at the identity is denoted by $\Ad$, and we get a homomorphism ${\Ad:G \to \GL(\mathfrak g)}$, called the \textbf{adjoint representation}.

The differential of $\Ad$ at $e \in G$ gives a Lie algebra homomorphism
$$
\ad:\g \to \mathfrak{gl}(\g),
$$
also called the \textbf{adjoint representation}. 

We can compute this map more explicitly. Let $X,Y \in \g$. Then by definition
$$
(\ad X)(Y) = \restr{\frac{d}{dt}( \Ad \exp(tX))(Y)}{t=0}.
$$
Let $f \in C^\infty(G)$. Then
\begin{align*}
  (d_ef)(\ad(X)(Y)) &= \restr{\frac{d}{dt} (d_ef)( \Ad \exp(tX)(Y))}{t=0} \\
&= \restr{\frac{\partial^2}{\partial t \partial s}f(\exp(tX)\exp(sY)\exp(-tX) )}{s=t=0} \\
&= \restr{\frac{\partial^2}{\partial s \partial t} f(\exp(tX)\exp(sY))-f(\exp (sY)\exp(tX))}{s=t=0} \\
&= (d_ef)([X,Y]).
\end{align*}

Therefore, $(ad X)(Y) = [X,Y]$.

Of course  the adjoint map $\ad: \g \to \mathfrak{gl}(\g)$ defined by $X \cdot Y = [X,Y]$ is well-defined for any Lie algebra $\g$. Note that the identity 
$$
\ad [X,Y] = [\ad (X), \ad(Y)] = \ad(X) \ad (Y) - \ad(Y) \ad(X)
$$
is exactly the Jacobi identity.

The kernel of $\ad$ is exactly the \textbf{center of $\g$}:
$$
\mathfrak{z}(\g) = \{ X \in \g \, | \, [X,Y] = 0 \, \forall \, Y \in \g \}.
$$

\begin{prop}
\label{propcommute}
  Let $G$ be a Lie group and let $X,Y \in \g$. Then the following two conditions are equivalent:
  \begin{enumerate}
  \item $[X,Y]=0$.
\item $\exp(tX)\exp(sY) = \exp(sY) \exp(tX)$ for all $s,t \in \R$.
  \end{enumerate}

Furthermore, if these conditions are satisfied, then $\exp(X+Y) = \exp(X)\exp(Y)$.
\end{prop}

\begin{proof}
This is Lemma 13 on page 157 in \cite{spivak_manifolds}.
\end{proof}

\begin{prop}
\label{propabelian}
  Any connected abelian Lie group $G$ is isomorphic to $\R^k \times \mathbb T^l$ for some natural numbers $k,l \geq 0$.
\end{prop}

\begin{proof}
 Consider $\g$ as a Lie group under addition. Then $\exp:\g \to G$ is a Lie group homomorphism by Proposition \ref{propcommute}.

By Example \ref{excovering}, $\exp$ is a covering map. Let $\Gamma$ be its kernel, which must a discrete subgroup of $\g$. 

But for any discrete subgroup $\Gamma$ of $\g \simeq \R^n$, there is a basis $\{e_1,\ldots,e_k \}$ such that $\Gamma$ is equal to $\Z e_1 \oplus \ldots \oplus \Z e_k$. Hence
$$
\g / \Gamma \simeq \R^n/(\Z e_1 \oplus \ldots \oplus \Z e_k) \simeq \mathbb T^k \times \R^{n-k}.
$$
\end{proof}



%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Structure and representation theory of $SU(2)$}

Recall that the Lie algebra of $SU(2)$ is
$$
\mathfrak{su}(2) = \{ X \in \mathfrak{gl}_2(\C) \mid X^\ast + X = 0, \quad \tr(X) = 0\} .
$$

Recall also that
$$
\mathfrak{sl}_2(\C) = \{ X \in \mathfrak{gl}_2(\C) \mid \tr(X) = 0 \}.
$$

The latter is a complex vector space, and in fact we have $\mathfrak{su}(2) \otimes_\R \C = \mathfrak{sl}_2(\C)$. Note that one inclusion is obvious. For the other, note that any $X \in \mathfrak{sl}_2(\C)$ can be written as 
$$
X = \frac{X-X^\ast}{2} + i \frac{X+X^\ast}{2i},
$$
namely as a sum of two matrices in $\mathfrak{su}(2)$. 

There is a standard $\C$-basis for $\mathfrak{sl}_2(\C)$ given by the following three matrices:
\begin{align*}
E = \begin{pmatrix}
0 & 1 \\
0 & 0 
\end{pmatrix} &&
F = \begin{pmatrix} 
0 & 0 \\
1 & 0 
\end{pmatrix} &&
H = \begin{pmatrix}
1 & 0 \\
0 & - 1
\end{pmatrix}.
\end{align*}

A basis for $\mathfrak{su}(2)$ over $\R$ is $E-F$, $i(E+F)$ and $iH$. Note that $\R i H \subset \mathfrak{su}(2)$ is the Lie algebra of the torus 
$$
T = \left\{ \begin{pmatrix}
z & 0 \\
0 & \overline z 
\end{pmatrix} \mid z \in S^1
\right\}.
$$
The Lie algebra structure on $\mathfrak{sl}_2(\C)$ is defined by the rules
\begin{align*}
[H,E] = 2E && [H,F] = -2F && [E,F] = H.
\end{align*}

\begin{lemma}
The group $SU(2)$ is simply-connected.
\end{lemma}
\begin{proof}
One checks that
$$
SU(2) =\left\{ \begin{pmatrix}
\alpha & \beta \\
-\overline \beta & \overline \alpha 
\end{pmatrix}
\mid \alpha, \beta \in \C, \quad |\alpha|^2 + |\beta|^2 = 1
\right\}
$$
This is clearly homeomorphic to $S^3$, which is simply-connected.
\end{proof}

Therefore, from [[Prop]], we have a one to one correspondence between Lie group homomorphisms $SU(2) \to \GL(V)$ and Lie algebra homomorphisms $\mathfrak{su}(2) \to \mathfrak{gl}(V)$.

Recall that if $\g$ is Lie algebra, then a \textbf{representation} of $\g$ on $V$ is the same thing as a Lie algebra homomorphism $\g \to \mathfrak{gl}(V)$. We also say that $V$ is a \textbf{$\g$-module}. We say that a subspace $W \subset V$ is \textbf{invariant} if $X \cdot W \subset W$ for all $X \in \g$. A representation $\g \to \mathfrak{gl}(V)$ is \textbf{irreducible} (or the $\g$-module $V$ is \textbf{simple}) if there are no proper invariant subspaces.

It is an exercise to see that if $G$ is connected Lie group, then these notions coincide with those of representation of $G$ (as opposed to representations of $\g$).

We now want to classify irreducible representations of $SU(2)$ on \emph{complex} finite-dimensional vector spaces $V$. Equivalently, we want to classify irreducible representations of $\mathfrak{su}(2)$ on complex finite-dimensional vectors spaces $V$. 

Now, since $\mathfrak{su}(2) \otimes_\R \C \simeq \mathfrak{sl}_2(\C)$, any Lie algebra homomorphism $\mathfrak{su}(2) \to \mathfrak{gl}(V)$ (where we consider $V$ as a real vector space) extends uniquely to a Lie algebra homomorphism $\mathfrak{sl}_2(\C) \to \mathfrak{gl}(V)$ of complex vector spaces.

So classifying $\mathfrak{su}(2)$-modules is the same as classifying $\mathfrak{sl}_2(\C)$-modules.

Assume now $V$ is a finite-dimensional $\mathfrak{sl}_2(\C)$-module. Then this exponentiates to a representation of $SU(2)$:
$$
\xymatrix{
\mathfrak{su}(2) \ar@{^{(}->}[r] \ar[ddr] &  \mathfrak{sl}_2(\C) \ar[dd] & & SU(2) \ar[ddr]  \ar@{^(->}[r] & SL_2(\C) \ar[dd] \\
& & \ar[r]^{\exp}   & \\
&  \mathfrak{gl}(V) && & \GL(V)
}
$$

By restriction, the torus $T \subset SU(2)$ act on $V$ as well, and by complete reducibility, $V$ decomposes as a sum of $T$-invariant subspaces. Since $\{ ti H \mid t \in \R \}$ exponentiates to the torus $\mathbb T$, $V$ decomposes in as a sum of $H$-invariant subspaces as well.

\begin{lemma}
  The eigenvalues of $H$ are integers.
\end{lemma}
\begin{proof}
Note that each irreducible eigenspace must be one-dimensional, since on each eigenspace, $H$ is sent to a diagonal matrix. Thus on each one-dimensional irreducible representation, $itH$ is sent to multiplication by $it\alpha$.

Also note that $\exp(itH) \in S^1$, so that the action of $H$ on $(\C,+)$ lifts to a representation of $S^1$. These are all of the form $\theta \ mapsto n \theta$.

Exponentiating, we get that $e^{itH}=e^{it\alpha}$ for all $t \in \R$. Put $t=2\pi$. Then we get that $1=e^{2\pi i \alpha}$, which is possible only if $\alpha \in \Z$.
\end{proof}

It is common to index the eigenvalues of $H$ on $V$ by half-integers. Thus for $s \in \frac 12 \Z$, we put
$$
V(s) = \{ \xi \in V \mid H \xi = 2s \xi \}.
$$

Elements of $V(s)$ are called vectors of \textbf{weight} $s$. Thus we write
$$
V = \bigoplus_{s \in \frac 12 \Z} V(s).
$$

Now recall the commutation relations in $\mathfrak{sl}_2(\C)$: $[H,E]=2E$ and $[H,F]=-2F$.

\begin{lemma}
  We have 
$$
EV(s) \subset V(s+1)
$$
and 
$$
FV(s) \subset V(s-1).
$$
In particular, if $v \neq 0$ is a vector of heighest weight, then $Ev = 0$.
\end{lemma}
\begin{proof}
Let $v \in EV(s)$. Then $v=Ew$ for $w \in V(s)$. Then
\begin{align*}
  Hv &= HEw \\
&= HEw - EHw + EHw \\
&= [H,E]w + EHw \\
&= 2Ew + 2sEw\\
&=2v+2sv \\
&=2(v+1)v.
\end{align*}
So $v \in V(s+1)$. The calculation with $F$ is similar.
\end{proof}

These two observations are enough to prove quite a lot about representations of $\mathfrak{sl}_2(\C)$.

\begin{lemma}
Suppose $V$ is a finite-dimensional $\mathfrak{sl}_2(\C)$-module and $\xi \in V(s) \bs \{ 0 \}$ satisfies $E \xi = 0$ (that is, $\xi$ is of heighest weight). Then
\begin{enumerate}
\item $s \geq 0$.
\item $F^{2s+1}\xi = 0$ and $F^k\xi \neq 0$ for $k=0,\ldots,2s$. 
\item $EF^k \xi = (2s-k+1)kF^{k-1} \xi$ for all $k \geq 0$.
\end{enumerate}
\end{lemma}
\begin{proof}

We start by proving number 3 by induction. The base case $k=0$ gives $E \xi = 0$, which is clear by assumption.

Note that $EF = [E,F]+FE = H+FE$. Note also that since $\xi \in V(s)$, we have $F^k \xi \in V(s-k)$ by the previous lemma. Then:
\begin{align*}
  EF^{k+1} \xi &= EF F^k \xi \\
&= (H+FE) F^k \xi \\
&= H(F^k \xi) + F(EF^k \xi) \\
&= 2(s-k)F^k \xi + (2s-k+1)k F^k \xi \\
&= (2s-k+2sk-k^2) F^k \alpha.
\end{align*}
Now note that this is what we want, because
$$
(2s-(k+1)+1)(k+1) = (2s-k)(k+1) = 2sk+2s-k^2-k.
$$
Thus number 3) is proved.

Now let $n$ be the smallest number such that $F^n \xi = 0$ (the \textbf{smallest weight}). Then $F^n \xi = 0$. Then it follows from 3) that
$$
0 = EF^n \xi = (2s-n+1)nF^{n-1} \xi.
$$
Hence $2s-n+1=0$, so $n=2s+1$. This proves $2)$. This also proves $1)$ because $n \geq 1$. 
\end{proof}

In view of this lemma, for any $s \in \frac 12 \Z_+ = \{ 0, \frac 12, 1, \ldots \}$, define an $\mathfrak{sl}_2(\C)$-module $V_s$ as follows: it is a vector space with basis elements denoted by
$$
\{ \xi_{-s}^s, \xi_{-s+1}^s, \ldots, \xi_s^s \},
$$
and where $H,F$ and $E$ act as follows:
\begin{align*}
 H \xi_t^s = 2t \xi_t^s && F\xi_t^s = \xi_{t-1}^s && E \xi_{s-k}^s = (2s-k+1)k\xi_{s-k+1}^s.
\end{align*}

Then it is an (easy) exercise to see that this defines an $\mathfrak{sl}_2(\C)$-module. It is also easy to see that $V_s$ is irreducible, since, starting from any basis vector $\xi_t^s$, we can use $F$ and $E$ to raise and lower the indices to generate all of $V_s$.

The number $s$ in $V_s$ is called the \textbf{heighest weight} or \textbf{spin} of $V_s$.

The lemma tells us that if $V$ is a finite-dimensional $\mathfrak{sl}_2(\C)$-module and $\xi \in V(s)$, $\xi \neq 0$ and $E \xi = 0$ (that is, $\xi$ is a heighest weight vector), then we get an embedding $V_s \hookrightarrow V$ given by $\xi_{s-k}^s \mapsto F^k \xi$.

\begin{thm}
 The $\mathfrak{sl}_2(\C)$-modules $V_s$ ($s \in  \frac 12 \Z$) are irreducible, pairwise non-isomorphic, and any finite-dimensional irreducible $\mathfrak{sl}_2(\C)$-module is isomorphic to some $V_s$.
\end{thm}
\begin{proof}
The first statement is clear, since all the $V_s$ have different dimension. Now, if $V$ is an irreducible $\mathfrak{sl}_2(\C)$-module, the observation above tells that us we have an embedding $V_s \hookrightarrow V$.

Since $\mathfrak{sl}_2(\C)$ is semisimple, it follows from Weyls theorem ([[ref??]]), that every invariant subspace has a complementary invariant subspace. But $V$ was irreducible, so $V_s=V$.
\end{proof}

\begin{example}
  Let $s=0$. Then $\dim V_0 = 1$, and this corresponds to the trivial representation of $SU(2)$. 
\end{example}

\begin{example}
 Let $s=\frac 12$. Then $\dim V_{\frac 12} = 2$, and one sees that $H,F$ and $E$ act precisely as the canonicial representation of $\mathfrak{sl}_2(\C)$ on $\C^2$. 
\end{example}

\begin{example}
Let $s=1$. Then $\dim V_1 = 3$, and $V_1$ is the adjoint representation of $\mathfrak{sl}_2(\C)$ with heigest weight vector $\xi_1^1 = E$. 
\end{example}



%%%%%%%%%%%%%%%%%%
\newpage
\section{Tori}

A \textbf{torus} is a Lie group isomorphic to $\mathbb T^n = {(S^1)}^n$. 

Recall that by Proposition \ref{propabelian}, any compact connected abelian Lie group is a torus.

\begin{prop}
 Let $G$ be a compact connected Lie group. Then there exists a maximal torus in $G$. Furthermore, we have a one to one correspondence between the maximal tori in $G$ and the maximal abelian Lie subalgebras of $\g$.
\end{prop}
\begin{proof}

As $\g$ is finite-dimensional, maximal abelian subalgebras exist. Let $\ia$ be such a maximal abelian subalgebra of $\g$. Then $\exp \ia$ is a connected abelian Lie subgroup of $G$.

Then $H = \overline {\exp \ia  }$ is a closed connected abelian subgroup of $G$. Hence $H$ is a torus. Then $\mathfrak h$ is abelian and $\mathfrak h \supset \ia$, hence $\mathfrak h = \ia$. Hence $H = \exp \ia$, since the inclusion $\ia \subset H$ must be a covering map.  

On the other hand, if $T \subset G$ is a maximal torus, then $\mathfrak t \subset \g$ is an abelian Lie algebra, hence it is contained in a maximal abelian Lie algebra $\ia$. Then $\exp \ia$ is a torus containing $T$, hence $T= \exp \ia$, and $\mathfrak t = \ia$.
\end{proof}

Our goal in this section is to show that any two maximal tori are conjugate. This is not so difficult in concrete cases:

\begin{example}
 Let $G=U(n)$. Then
$$
T = \left\{ \begin{pmatrix} 
z && \\
& \ddots & \\
&& z 
\end{pmatrix} \, \mid \, z \in \C \right\} \cap SU(n) \simeq \mathbb T^n
$$
is a torus. 

Suppose $H \subset G$ is a torus. As $H$ is compact abelian, $\C^n$ decomposes into a direct sum of $1$-dimensional $H$-invariant subspaces. Hence there exist $g \in U(n)$ such that $gHg^{-1} \subset T$, hence there exist $g \in U(n)$ such that $gHg^{-1} \subset T$. 

This shows that $T$ is maximal and any maximal torus is conjugate to $T$.
\end{example}

We need some preparations to prove the general case.

If $T$ is a torus, we say that $g \in T$ is a \textbf{topological generator} if
$$
\overline{ \{ g^n \, \mid \, n \in \Z \} } = T.
$$
\begin{thm}[Kronecker]
 Consider $T=\mathbb T^n$. Then
$$
g = \left(e^{2\pi i t_1}, \ldots, e^{2 \pi i t_n}  \right)
$$
is a topological generator if and only if the numbers $1, t_1, \ldots, t_n$ are linearly independent over $\Q$.
\end{thm}
\begin{proof}
Consider $H= \overline{\{ g^n \mid n \in \Z \}}$. If $H \neq T$, then $T/H$ is a nontrivial compact abelian group, hence $T/H$ has a nontrivial character. By composition, $T$ has a non-trivial character $\chi$ such that $\restr{\chi}{H} = 1$ (constantly the identity). 

Note that $G/H$ is trivial if and only if for every character $\chi$, the implication $\chi(g) = 1 \Rightarrow \chi =1 $ holds.

We have, for any character,
$$
\chi\left(e^{2 \pi i t_1}, \ldots, e^{2 \pi i t_n} \right) = e^{2 \pi i m_1t_1} \cdot \ldots \cdot e^{2 \pi i m_n t_n}
$$
for some $m_1,\ldots,m_n \in \Z$. So suppose $\chi(g)=1$. Then the $m_i$'s constitute an equation of linear dependence among the $t_i$. But if these are linearly independent, then $\chi$ is trivial. 
\end{proof}

\begin{corr}
  For any torus $T$, the set of topological generators is dense in $T$.
\end{corr}

\begin{prop}
  Let $G$ be a compact connected Lie group. Let $T \subset G$ be a maximal torus. Then
  \begin{enumerate}
  \item $N(T)  \stackrel{\Delta}{=} \{ g \in G \, \mid \, gTg^{-1} = T\}$ is a closed subgroup of $G$.
\item $N(T)^\circ = T$.
\item The group $W(G,T) \stackrel{\Delta}{=} N(T)/T$ is finite. It is called the \textbf{Weyl group of $G$}.
  \end{enumerate}
\end{prop}

\begin{proof}
It is obvious that $N(T)$ is a closed subgroup, since the definition is a closed condition.

For 2., consider the homomorphism
\begin{align*}
  \alpha:N(T) &\to \Aut(T) \\
g &\mapsto \alpha(g)(h)=ghg^{-1}.
\end{align*}
The automorphism group of the torus $T$ is $\GL_n(\Z) \subset \GL_n(\R)$.

The group $\GL_n(\Z)$ is discrete, so $\alpha$ is locally constant. Thus $N(T)^\circ \subset \ker \alpha$. Thus $gh=hg$ for all $g \in N(T)^\circ$ and all $h \in T$. Let $X$ be an element of the Lie algebra of $N(T)$. Then $\exp(tX)$ commutes with $T$ for all $t \in \R$. It follows that the closure of the group generated by $\exp(tX)$ is a compact connected abelian Lie group, hence must be a torus. 

But since $T$ is maximal, this torus coincides with $T$, so $\exp(tX) \in T$ for all $t \in \R$. 

Thus the Lie algebra of $N(T)$ coincides with the Lie algebra of $T$. Hence $N(T)^\circ =T$.

For 3., note that by 2., we have
$$
N(T)/T = N(T) /N(T)^\circ ,
$$
which is finite because $N(T)$ is compact.
\end{proof}

\begin{example}
Let $G=U(n)$. And let $T$ consist of the unitary diagonal matrices. It is a maximal torus. 

Then $N(T)$ consist of all the matrices $g \in U(n)$ such that every row and column contains exactly one nonzero coefficient. 

Thus $N(T)/T \simeq S_n$. 
\end{example}

\subsection{Digression on Riemannian geometry}

Recall that a Riemannian manifold is a manifold $M$ such that $T_pM$ is equipped with a scalar product depending smoothly on $p$. Thus, if $X,Y \in \mathfrak X(M)$ are two vector fields, then the map $p \mapsto (X_p,Y_p)$ is a smooth map. Equivalently, a scalar product defines a smooth section of $S^2(T^\ast M)$.

Given a piecewise $C^1$-curve, we define its \textbf{length} by 
$$
L(\gamma) \stackrel{\Delta}{=} \int_a^b \| \gamma'(t) \| dt.
$$

Then we can define a metric on $M$ (assuming $M$ is connected), by
$$
d(a,b) = \inf \{ L(\gamma) \, \mid \, \gamma:[0,1] \to M \text{ piecewise $C^1$ }, \gamma(0)=a, \gamma(1)=b \}.
$$

It is not difficult to see that the topology on $M$ coincides with the topology defined by this metric $d$.

A map $\gamma:[a,b] \to M$ is called a \textbf{geodesic} if for any $t_0 \in (a,b)$, there exists a $\delta > 0$ such that $d( \gamma(s), \gamma(t) ) = \lvert s - t \rvert$ for $s,t \in (t_0-\delta, t_0+\delta)$. 

We also allow linear change of variables, so $t \mapsto \gamma(a t)$ is also called a geodesic for $\alpha > 0$. 

Here is a theorem we wont prove.

\begin{thm}
\label{theoremgeodesic}
  Let $M$ be a Riemannian manifold. Then:
  \begin{enumerate}
  \item Any geodesic is smooth.
\item For any $p \in M$ and $X \in T_pM$, $X \neq 0$, there is a unique maximal geodesic $\gamma$ such that $\gamma(0)=e$ and $\gamma'(0)=X$.
\item (Hopf-Rinow theorem) If $M$ is compact and connected, then any maximal geodesic is defined on the whole line $\R$, and ofr any $p,q \in M$, there exist a geodesic passing through $p$ and $q$. 
  \end{enumerate}
\end{thm}

\subsection{Returning to tori}

From now on, assume $G$ is a compact connected Lie group. Consider the adjoint representation $\Ad \colon  G \to \GL(\g)$. Since $\g$ is compact, there exists an $\Ad G$-invariant scalar product (see Lemma \ref{lemmainnerproduct}). Then $\Ad:G \to O(\g, \langle-,- \rangle)$. Recall that the Lie algebra of $O(\g)$ consist of $T \in \mathfrak{gl}(\g)$ such that
$$
\langle Tx,y  \rangle  + \langle X,Ty \rangle = 0
$$
for all $x,y \in \g$ (???). 

Now we use left- or right-translations to define a scalar product on $T_gG$ for any $g \in G$. Thus $G$ becomes a Riemannian manifold such that both left and right translations preserve the Riemannian structure.

\begin{thm}
 The geodesics in $G$ are translations (left or right) of one-parameter subgroups of $G$.
\end{thm}

\begin{proof}

The result is true for tori, since they are of the form $\R^n/\Z^n$, so their geodesics are lines.

In general, it suffices to consider geodesics passing through $e$. Let $\gamma: \R \to G$ be a geodesic with $\gamma(0)=e$. Put $X= \gamma'(0) \in \g$. For any $g \in G$, the map $t \mapsto g \gamma(t) g^{-1}$ is also a geodesic (because the scalar product is invariant under left- and right-translations). Its derivative at $0$ is $Ad(g)(X)$.

Let $T$ be a maximal torus containing $\exp(tX)$ for $t \in \R$. Then $\Ad(g)(X)=X$ for all $g \in T$. 

It follows that $\gamma(t)=g \gamma(t) g^{-1}$ for all $t \in \R$, since by point 2. in Theorem \ref{theoremgeodesic}, geodesics are unique. In particular, $\gamma(t) \in N(T)^\circ =T $. 

Thus $\mathrm{Im} \gamma \subset T$. Hence $\gamma$ is a geodesic in $T$, where the result is true. 

Hence $\gamma(t) = \exp(tX)$, since the theorem is true for $T$.
\end{proof}

The next result is completely fundamental for everything that follows.

\begin{thm}[Cartan]
Let $G$ be a compact connected Lie group and $T \subset G$ a maximal torus. Then for all $g \in G$ there exists $h_g \in G$ such that $h_g g h_g^{-1} \in T$.
\end{thm}
\begin{proof}
Let $t_0 \in T$ be a topological generator of $T$. Let $H_0 \in \mathfrak t$ be such that $\exp(H_0)=t_0$. 

Choose a geodesic $\gamma$ passing through $e$ and $g$ with $\gamma(0)=e$. Let $X=\gamma'(0)$. Then $\gamma(t) = \exp(tX)$ by the previous theorem.

Consider the function 
$$
G \ni h \mapsto \langle \Ad(h)(X),H_0 \rangle \in \R
$$

This is a function from a compact space to the real line, so it has a maximum, achieved at, say, $h \in G$.

\textbf{Claim: } $[(\Ad h)(X), H_0] = 0$. 

Assuming the claim, it follows that $\exp(t(\Ad h)(X))=h \exp(tX)h^{-1}$ commutes with $\exp(H_0)=t_0$ for $t \in \R$. As $t_0$ is a topological generator of $T$, it follows that $h \exp(tX)h^{-1}$ commutes with $T$. 

As $T$ is a maximal torus, it follows that $h \exp(tX) h^{-1} \in T$ for all $t \in \R$. In particular, $hgh^{-1} \in T$.

Thus it remains to prove the claim. Take $Y \in \g$. Then the function 
$$
t \mapsto \langle \Ad(\exp(tY)h)(X), H_0 \rangle 
$$
attains its maximum at $t=0$. Therefore its derivative at $0$ is zero. So
\begin{align*}
0 &= \langle \ad(Y)( \Ad h)(X), H_0 \rangle \\
&= - \langle [\Ad(h)X,Y], H_0 \rangle \\
&= \langle Y, [\Ad(h)(X),H_0] \rangle.
\end{align*}

This is true for all $Y \in \g$, hence $[\Ad(h)(X),H_0]  = 0$. 
\end{proof}


\begin{corr}
Any two maximal tori in $G$ are conjugate.
\end{corr}
\begin{proof}
Let $T_1$ and $T_2$ be two maximal tori. Let $t_1 \in T_1$ be a topological generator. Then there exists $h \in G$ such that $ht_1 h^{-1} \in T_2$. But then $hT_1 h^{-1} \subset T_2$, since $t_1$ was a topological generator.  So $T_1 \subset h^{-1} T_2 h$. But $T_1$ was maximal, so $T_1=T_2$.
\end{proof}

\begin{corr}
Any element of $G$ is contained in a maximal torus. In particular, $\exp \colon \g  \to G$ is surjective.
\end{corr}

\begin{proof}
Let $g \in G$, and let $T$ be a maximal torus. Then $hgh^{-1} \in T$ for some $h \in G$. But then $g \in h^{-1}T h$, which is maximal. 

To see the "in particular", let $g \in G$. Then $g$ is contained in a maximal torus. Then the geodesic starting at $e$ and ending at $g$ must have the form $\exp(tX)$. 
\end{proof}

\begin{corr}
Any maximal torus $T$ in $G$ is a maximal abelian subgroup of $G$.
\end{corr}
\begin{proof}
Assume $g$ commutes with $T$. Consider
$$
H = C(g)^\circ = \{ h \in G \, \mid \, hg = gh \}^\circ,
$$
the centralizer of $g$ in $G$. This is a compact connected Lie group. Since $T$ is connected, $T \subset H$. As $g$ is contained in a maximal torus in $G$, we have $g \in H$. 

As $T$ is a maximal torus in $G$, it is a maximal torus in $H$. Hence there exists $h \in H$ such that $hgh^{-1} \in T$. But $g \in Z(H)$, hence $g \in T$.
\end{proof}


\begin{remark}
This implies that the map 
\begin{align*}
\alpha: W(G,T) &\to \Aut(T) \\
g &\mapsto  \alpha(gT)(h) = ghg^{-1}
\end{align*}
has trivial kernel. Hence $W(G,T) \hookrightarrow \Aut(T)$.
\end{remark}




%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Roots}

First we recall some notation from the representation theory of $SU(2)$. There we had a maximal torus, called $T$, which was given by
$$
T = \left\{ \begin{pmatrix} z & 0 \\ 0 & \overline z
\end{pmatrix} \mid z \in S^1 \subset \C \right\}.
$$
The Lie algebra of $SU(2)$ is contained in the Lie algebra of $SL(2,\C)$. We write $\mathfrak{su}(2) \subset \mathfrak{sl}_2(\C)$. The Lie algebra is spanned by [[write basis here]]

\subsection{Roots}

Let $G$ be a compact connected Lie group. Fix a maximal torus $T$. Consider $\g_\C=\C \otimes_\R \g$, the complexification of the Lie algebra of $G$. This is a complex Lie algebra, where we define $[c \otimes X, d \otimes Y]=cd \otimes [X,Y]$.

We can identifty $\g$ with the elements of the form $1 \otimes X$ for $X \in \g$. Sometimes we write elements of $\g_\C$ as $X+iY$.

The adjoint representation $\Ad:G \to \GL(\g)$ extends to a representation $G \to \GL(\g_\C)$, which we will also denote by $\Ad$.

Consider $\restr{\Ad}{T}$. As $T$ is a compact abelian group, we have a decomposition of $\restr{\Ad}{T}$ into isotypic components corresponding to characters $\chi \in \widehat T$. Thus we can write
$$
\g_\C = \bigoplus_{\chi \in \widehat T} \g_\C(\chi)
$$
where
$$
\g_\C(\chi) = \{ X \in \g_\C \mid (\Ad g)X = \chi(g) X \, \text{for all } g \in T \}
$$
is the eigenspace of $X$.

We view $\mathbb T = S^1$ as a subgroup of $\C^\ast = \GL_1(\C)$. In  this picture, the Lie algebra of $\mathbb T$ is $i \R \subset \C$.

Let $\mathfrak t$ be the Lie algebra of $T$. Any character $\chi \in \widehat T$ is determined by its differential $\chi_\ast:\mathfrak t \to i \R$, so we can think of $\chi_\ast \in i \mathfrak t^\ast$, where $\mathfrak t ^\ast = \Hom_\R(\mathfrak t,\R)$.

Then the set of differentials $\chi_\ast$ ($\chi \in \widehat T$) form a subset $X^\ast(T) \subset i \mathfrak t ^\ast$. It is a \emph{lattice} in $i \mathfrak t^\ast$ (here by \emph{lattice} we mean a discrete subgroup of maximal rank), called the \textbf{character lattice}.

Indeed, if $T=\R^n/\Z^n$, then $\mathfrak t=\R^n$, so $\mathfrak t^\ast=\R^n$, and an element $(it_1,\ldots,it_n) \in i\mathfrak t^\ast$ defines a character $\R^n \to \mathbb T$ by 
$$
(a_1, \ldots,a_n) \mapsto e^{ia_1t_1}\cdot \ldots \cdot e^{ia_nt_n},
$$
which factors through $\R^n/\Z^n$ if and only if all the $a_i \in 2\pi \Z$. Then $X^\ast(T) = 2\pi i \Z^n \subset i \R^n$.

Consider now the complexification $\mathfrak t_\C$ of the Lie algebra of $T$. We denote this complexification by $\mathfrak h$. It is called a \textbf{Cartan subalgebra} of $\mathfrak g_\C$. It is a ``maximal toral subalgebra''.

Functionals $\mathfrak t \to \R$ can be extended by complex linearity to $\mathfrak h$, so $i \mathfrak t^\ast \subset \mathfrak h^\ast = \Hom_\C(\mathfrak h, \C)$. We then have

\begin{align*}
  \g_\C(\chi) &= \{ X \in \g_\C \mid (\Ad g)X = \chi(g), X\, \forall g \in T \} \\
&= \{ X \in \g_\C \mid [Y,X] = \chi_\ast(Y)X, \,\forall Y \in T \} \\
&= \{ X \in \g_\C \mid [Y,X] = \chi_\ast(Y)X, \, \forall Y \in \mathfrak h \}.
\end{align*}

For $\alpha \in \mathfrak h^\ast$, denote
$$
\g_\alpha = \{ X \in \g_\C \mid [H,X] = \alpha(H)X \forall H \in \mathfrak h \}.
$$
In our earlier notation, this says that $\g_{\alpha} = \g_\C(\chi)$, where we think of $\chi \in i \mathfrak t^\ast \subset \mathfrak h^\ast$.

\begin{lemma}
We have that $\g_0 = \mathfrak h$.
\end{lemma}
\begin{proof}
 This follows because $\mathfrak t$ is a maximal abelian Lie subalgebra of $\g$, so that
$$
\mathfrak t = \{ X \in \g \mid [H,X] = 0 \quad \forall H \in \mathfrak t \}.
$$
Now complexify both sides.
\end{proof}

The elements $\alpha \in X^\ast(T) \bs \{ 0 \}$ such that $\g_\alpha \neq 0$ are called the \textbf{roots} of $G$ or $\g_\C$. We denote the set of roots by $\Delta$. 

To remember the position of $\g$ inside $\g_\C$, it is convenient to introduce an involution on $\g_\C$, defined by
$$
(X+iY)^\star = -X + iY,
$$
so that $\g = \{ X \in \g_\C \mid X = -X^\star \}$.\footnote{Note that I have chosen to write $X^\star$ for the involution to distinguish it from taking duals.}

This induces an involution $\alpha \mapsto \overline \alpha$ in $\mathfrak h^\ast$ as well. Namely, define
$$
\overline \alpha(H) \stackrel{\Delta}{=} \overline{\alpha(H^\star)}.
$$
This is actually an involution, because
$$
\overline{\overline \alpha}(H) = \overline{\overline \alpha(H^\star)} = \overline{\overline{\alpha(H^{\star \star})}} = \alpha(H).
$$
In fact, this definition ensures that $\overline{\alpha}$ is \emph{complex linear} as well: we have that $\overline \alpha (zH) = z \overline \alpha(H)$ (this is an easy but somewhat tedious exercise).

This lets us recognice $i \mathfrak t^\ast$ inside $\mathfrak h^\ast$ as well. Namely,
\begin{align*}
  i \mathfrak t^\ast &= \{ \alpha \in \mathfrak h^\ast \mid \alpha(\mathfrak t) \subset i \R \} \\
&= \{ \alpha \in \mathfrak h ^\ast \mid \overline \alpha = \alpha \},
\end{align*}
as those functionals fixed by the involution.

\subsection{Some lemmas}

The rest of this lecture will consist of several small results.

\begin{lemma}
\label{lemmadecomp}
  \begin{enumerate}
We have a decomposition $$\g_\C = \mathfrak h \oplus \bigoplus_{\alpha \in \Delta} \g_\alpha.$$ Also:
  \item $[\mathfrak g_\alpha, \mathfrak g_\beta] \subset \mathfrak g_{\alpha+\beta}$.
\item $\g_\alpha^\star = \g_{-\alpha}$. Here $g_\alpha^\star = \{ X^\star \mid X \in \g_\alpha \}$.
  \end{enumerate}
\end{lemma}
\begin{proof}
The decomposition of $\g_\C$ is just that of writing $\g_\C$ into simultaneous eigenspaces for the action of $\mathfrak h$, where $\mathfrak h$ has zero eigenvalues, as proved above.

We have that 
$$
\g_\alpha = \{ X \in \g_\C \mid [H,X] = \alpha(H) X \text{ for all } H \in \mathfrak h\}.
$$
Let $Y \in \g_\beta$. Then we look at $[H,[X,Y]]$. By the Jacobi identity, this is equal to
\begin{align*}
  [H,[X,Y]] &= -[Y,[H,X]] - [X,[Y,H]] \\
&= -\alpha(H)[Y,X] +[X,[H,Y]] \\
&= \alpha(H)[X,Y] +\beta(H)[X,Y] \\
&= (\alpha(H)+\beta(H))[X,Y].
\end{align*}
Hence $[X,Y] \in \g_{\alpha+\beta}$.

For ii), note that $\alpha(\mathfrak t) \subset i \R$. Hence for $H \in \mathfrak t$, we have 
$$
\overline \alpha(H) = \overline{\alpha(H^\star)} = - \overline{\alpha(H)} = \alpha(H).
$$
Hence we have
\begin{align*}
\g_\alpha &= \{ X \in \g_\C \mid [H,X] = -\alpha(H)X \quad \forall H \in \mathfrak h \} \\
&= \{ X \in \g \mid [H,X] = -\alpha(H) X \quad \forall H \in i \mathfrak t^\ast \} \otimes_\R \C  \\
&= \{ X \in \g \mid [H,X^\star] = \alpha(H) X^\star \quad \forall H \in i \mathfrak t ^\ast \} \otimes_\R \C.
\end{align*}
In the last equality, we used the involution on both sides to get
$$
[H^\star, X^\star] = -[H,X] = \alpha(H) X = \alpha(H) X^\star,
$$
since $X$ is real (which gives $X^\star = -X$) and $H^\star=H$.
\end{proof}

\subsection{An Ad-invariant inner product}

To proceed, it is convenient to introduce an $\Ad$-invariant scalar product on $\g$. These exist:

\begin{lemma}
\label{lemmainnerproduct}
Let $G$ be compact Lie group. Then there is an $\Ad$-invariant inner product $( -,-)$ on $\g$, in the sense that $$( g \cdot X, g\cdot Y )  = (X , Y )$$
for $X,Y \in \g$, where $g \cdot X$ denotes the adjoint action $\Ad:G \to \GL(\g)$. This condition further implies that
$$
( [X,Y],Z )  +(Y,[X,Z]) = 0.
$$
\end{lemma}
\begin{proof}
 Choose any scalar product $\langle -,-\rangle $ on $\g$ and a normalized Haar measure $dg$ on $G$. Then 
$$
(X,Y ) = \int_G (g \cdot X, g\cdot Y) dg
$$
is an $\Ad$-invariant scalar product on $\g$. To see the other equality, let $\gamma:\R \to G$ be an arc with $\gamma(0)=e \in G$ and $\gamma'(0)=Z \in \g$. Then
\begin{align*}
0 &=   \restr{\frac{d}{dt}\left( \gamma(t) \cdot X, \gamma(t) \cdot Y \right)}{t = 0} \\
 &= \lim_{t \to 0} \frac 1t \left(( \gamma(t) \cdot X, \gamma(t) \cdot Y ) - ( X, \gamma(t) \cdot  Y ) + ( X, \gamma(t) \cdot Y ) - ( X, Y ) \right) \\
&= \lim_{t \to 0} \left( \frac 1t (\gamma(t) \cdot X - X), Y \right) + \left( X, \frac 1t (\gamma(t) \cdot Y - Y) \right) \\
&= ( [Z,X],Y ) + ( X, [Z,Y])
\end{align*}
Renaming $X,Y,Z$ appropriately gives the required identity.
\end{proof}

It will be convenient to require that $( -,- )$ is \emph{negative} definite on $\g$. We can extend this form to $\g_\C$ be complex linearity. It will no longer be definite (it doesn't make sense), but it will still be non-degenerate (easy exercise). 

\begin{lemma}
We have
$$
(\g_\alpha, \g_\beta ) = 0
$$
if $\alpha \neq -\beta$.
\end{lemma}
\begin{proof}
This follows from the invariance of $\Ad$. Namely let $X \alpha \in \g_\alpha$ and $X_\beta \in \g_\beta$. Then:
\begin{align*}
  0 &= ([H,X_\alpha],X_\beta ) + (X_\alpha,[H,X_\beta]) \\
&= (\alpha(H)X_\alpha, X_\beta) + (X_\alpha,\beta(H)X_\beta) \\
&= (\alpha(H)+\beta(H)) (X_\alpha,X_\beta)
\end{align*}
for all $H \in \mathfrak h$.
\end{proof}

By duality we can define a symmetric bilinear form on $\mathfrak h^\ast$ as well: let $\alpha \in \mathfrak h^\ast$. Define its \textbf{dual element} $h_\alpha$ by the equality
$$
(h_\alpha, H) = \alpha(H)
$$
for all $H \in \mathfrak h$. To see that this is well-defined, choose an orthogonal basis $\{ e_i \}$ for $h$ and let $\{ e_i^\ast \}$ be the dual basis (with respect to the form $(-,-)$). Then the vector $h_\alpha$ is just the transpose of the vector $\alpha$. 

We define 
$$
(\alpha,\beta) = (h_\alpha, h_\beta).
$$


\begin{lemma}
We have
$$
h_\alpha^\star = h_{\overline \alpha}
$$
for every $\alpha \in \mathfrak h^\ast$.
\end{lemma}
\begin{proof}
First off, note that by definition $(X^\star, Y) = \overline{(X,Y^\star)}$. Then
$$
(h_\alpha^\star, H) = \overline{(h_\alpha, H^\star)} = \overline{\alpha(H^\star)} = \overline{\overline{\overline \alpha(H)}} = \overline{\alpha} (H) = (h_{\overline \alpha},H).
$$
Here we used the definition of $\overline \alpha$ as $\overline \alpha(H) = \overline{\alpha(H^\star)}$.
\end{proof}

It follows that if for $\alpha \in \mathfrak h$ satisfies $\overline \alpha = \alpha$, then $h_\alpha^\star = h_\alpha$, which implies that $h_\alpha \in i \mathfrak t$. Since $(-,-)$ was negative definite on $\g$, it follows that $(-,-)$ is positive definite on $i \mathfrak t$. 

Now let $\alpha \in \Delta$ be a root. Then define
$$
H_\alpha = \frac{2 h_\alpha}{(\alpha,\alpha)} \in i \mathfrak t \subset \mathfrak h.
$$

\begin{lemma}
Let $\alpha \in \Delta$, and $X \in \g_\alpha$. Then
$$
[X,X^\star] = (X,X^\star) h_\alpha.
$$
\end{lemma}
\begin{proof}
We know already from Lemma \eqref{lemmadecomp} that $[X,X^\star] \in \mathfrak h$. Let $\beta \in \mathfrak h^\star$. Then
\begin{align*}
\beta([X,X^\star]) &= (h_\beta,[X,X^\star]) \\
&= -(h_\beta,[X^\star,X]) \\
&= ([X^\star, h_\beta],X) \\
&= (\alpha(h_\beta) X^\star,X) = (X^\star,X) \beta(h_\alpha).
\end{align*}
This holds for all $\beta \in \mathfrak h^\ast$, which implies the statement.
\end{proof}

Choose a non-zero $E_\alpha \in \g_\alpha$, and normalize it so that $(E_\alpha,E_\alpha^\star) = \frac{2}{(\alpha,\alpha)}$. Then
$$
[E_\alpha,E_\alpha^\star] = H_\alpha.
$$
Put $F_\alpha = E_\alpha^\star$. We thus get
\begin{align*}
  [H_\alpha, E_\alpha] = 2 E_\alpha && [H_\alpha ,F_\alpha] = -2F_\alpha && [E_\alpha, F_\alpha] = H_\alpha.
\end{align*}

This defines a sub-Lie-algebra isomorphic to $\mathfrak{sl}_2(\C)$. Thus for every $\alpha \in \Delta$ we get an inclusion of Lie algebras
$$
i_\alpha: \mathfrak{sl}_2(\C) \to \g_\C.
$$

\begin{lemma}
\begin{enumerate}[a)]
\item The spaces $\g_\alpha$ are one-dimensional.
\item Suppose $\alpha \in \Delta$ and $c \in \R^\star$ and $c\alpha \in \Delta$. Then $c = \pm 1$.
\end{enumerate}
\end{lemma}
\begin{proof}
We can suppose $\alpha$ to be minimal, in the sense that $c \alpha \not \in \Delta$ if $c \in (0,1)$. Now consider the space
$$
V = \C F_\alpha \oplus \mathfrak h \oplus \left( \bigoplus_{c > 0, c \alpha \in \Delta} \mathfrak g_{c \alpha} \right).
$$
This is a finite-dimensional $\mathfrak{sl}_2(\C)$-module. But we know from [[previous lecture]] that their spectra are symmetric about the origin, counted with multiplicities. Hence the big term must be isomorphic with the left term, which is one-dimensional, and we must have $c=1$.
\end{proof}

\begin{remark}
  The \emph{isomorphism} $\mathfrak h ^\ast \simeq \mathfrak h$ depends upon the choice of invariant form, but the elements $H_\alpha$ ($\alpha \in \Delta$) does not depend upon this choice: $H_\alpha$ is the unique element of the one-dimensional space $[\g_\alpha, g_{-\alpha}] \subseteq \mathfrak h$ such that $\alpha(H_\alpha)=2$. Then $E_\alpha \in \g_\alpha$ is defined up to scalar of modulus $1$, such that $[E_\alpha, E_\alpha^\ast] = H_\alpha$. 
\end{remark}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Root systems}

In this lecture, we study root systems in the abstract.

Let $V$ be a finite-dimensional Euclidean vector space (that is, a real vector space equipped with a scalar product). A finite subset $\Delta \subset V \bs \{ 0 \}$ is a \textbf{root system} if:

\begin{enumerate}
\item For every $\alpha \in \Delta$, the reflection $s_\alpha$ with respect to the hyperplane $\alpha^\perp$ leaves $\Delta$ invariant.

Note that
$$
s_\alpha(v) = v - \frac{2(\alpha,v)}{(\alpha,\alpha)} \alpha.
$$
\item The number 
$$
\frac{2(\alpha, \beta)}{(\alpha,\alpha)}
$$
is an integer for all $\alpha, \beta \in \Z$.
\newcounter{enumTemp}
    \setcounter{enumTemp}{\theenumi}
\end{enumerate}
A root system is called \textbf{reduced}, if in addition the following condition is satisfied:
\begin{enumerate}
\setcounter{enumi}{\theenumTemp}
\item If $\alpha, c\alpha \in \Delta$, where $ c \in \R^\times$, then we must have $c = \pm 1$.
\end{enumerate}

\begin{remark}
 Number 3. is often assumed to be part of the definition of a root system. It is also often assumed that $\Delta$ span the whole space $V$, but for us it will be important not to assume this.
\end{remark}

\begin{example}
  Here's an example of a root system. It is denoted by $A_2$ (and is associated with $SU(3)$). It is a regular hexagon.

\begin{figure}[h!]
\centering
\begin{tikzpicture}
\foreach \a in {0,60,...,300} { %\a is the angle variable
\draw [->] (0,0) -- (\a:2cm);
}
\end{tikzpicture}
\caption{The root system $A_2$.}
\end{figure}

To check the conditions, we can choose the vertices $\alpha_i$ to be given by $\alpha_i=(\cos \theta_i, \sin \theta_i)$ with $\theta_i = \frac{\pi}{3} i$ for $i=0,\ldots,5 \pi/3$. Then we have that
$$
(\cos \theta_i, \sin \theta_i) \cdot (\cos \theta_j, \sin \theta_j)  = \cos(\theta_i - \theta_j) = \pm \frac 12 \text{ or } \pm 1,
$$
since $\theta_i-\theta_j$ is a multiple of $\pi/3$. In either case, the second condition is fulfilled. 
\end{example}

We say that two root systems $(V_1, \Delta_1)$ and $(V_2, \Delta_2)$ are \textbf{isomorphic} if there exists a linear isomorphism $T:V_1 \to V_2$ such that $T(\Delta_1)=\Delta_2$ and $T s_\alpha = s_{T(\alpha)} T$ for all $\alpha in \Delta_1$. Note that we do not require the isomorphism to preserve the scalar product.

\begin{thm}
 Let $G$ be a compact connected Lie group and $T \subset G$ a maximal torus. Then $(i \mathfrak  t^\ast, \Delta)$ is a reduced root system. Up to isomorphism, this root system does not depend upon the choice of $T$ and the choice of an invariant form on $\g$.
\end{thm}

\begin{proof}
LONG PROOF HERE
\end{proof}

\subsection{Simple roots, Weyl chambers, and the Weyl group}

Let $(V,\Delta)$ be a root system and assume that $\mathrm{Span} \Delta = V$.  We say that a subset $\Pi \subset \Delta$ is a \textbf{system of simple roots} if (1) $\Pi$ is a basis of $V$ and (2) that if $\beta = \sum_{\alpha \in \Pi} c_\alpha \alpha \in \Delta$, then either all $c_\alpha \geq 0$ or all $c \alpha \leq 0$.

Reflections $s_\alpha$ defined by simple roots are called \textbf{simple reflections}.

If we fix a set of simple roots $\Pi$, then the roots $\beta = \sum_\alpha c_\alpha \alpha \in \Delta$ with positive $c_\alpha$ are called \textbf{positive roots}. We denote the set of positive roots by $\Delta_+$. Thus $\Delta = \Delta_+ \cup -\Delta_-$.

It is a fact that every root system possesses systems of simple roots. In order to construct $\Pi$, we introduce the notion of a \textbf{Weyl chamber}: consider the set 
$$
V \bs \bigcup_{\alpha \in \Delta} \alpha^\perp.
$$
The connected components of this set are called the (open) \emph{Weyl chambers} of $\Delta$. Let $C$ be such a Weyl chamber. 

By definition, $(\alpha, \beta) \neq 0$ for all $\alpha \in \Delta$ and $\beta \in C$. Hence for any $\alpha \in \Delta$ we have either $(\alpha, \beta) > 0$ for all $\beta \in C$ or $(\alpha, \beta) < 0$ for all $\beta \in C$. In the first case, we say that $\alpha$ is \textbf{$C$-positive}. Let $\Pi(C) \subset \Delta$ be the set of $C$-positive roots $\alpha$ such that it is impossible to write $\alpha$ as $\beta+\gamma$ for $C$-positive roots $\beta, \gamma \in \Delta$.


\begin{prop}
  The set $\Pi(C)$ is a system of simple roots. Furthermore, the assignment $C \mapsto \Pi(C)$ is a bijection between Weyl chambers and systems of simple roots.
\end{prop}
\begin{proof}
By definition of $\Pi(C)$, any $C$-positive root $\beta$ has the form $\sum_{\alpha \in \Pi(C)} c_\alpha \alpha$ for $c_\alpha \in \Z_+$ (why: because $\Pi(C)$ is exactly the set of $C$-positives that are such sums with only one term).

It follows that $\Span \Pi(C)=V$, since any cone of the same dimension as $V$ span $V$.

Therefore we have to show that $\Pi(C)$ is a basis. Observe first that if $\alpha,\beta \in \Pi(C)$, then $\alpha-\beta \not \in \Delta$. Indeed, otherwise, either $\alpha-\beta$ og $\beta-\alpha$ is $C$-positive. But then $\alpha=\beta +(\alpha-\beta)$ or $\beta=\alpha+(\beta-\alpha)$ gives a contradiction to the definition of $\Pi(C)$. 

Observe next that if $\alpha,\beta \in \Pi(C)$, $\alpha \neq \beta$, then $(\alpha,\beta) \leq 0$. Indeed, assume that $(\alpha,\beta) > 0$. We may assume that $\| \alpha \| \geq \|\beta \|$. Then
$$
\frac{2 (\alpha,\beta)}{(\alpha,\alpha)} < \frac{2 \| \alpha \| \|\beta \|}{\| \alpha \|^2} = \frac{2 \| \beta \|}{\| \alpha \|} \leq 2.
$$
But the left term is a positive integer, hence it must be equal to $1$.  But then $s_\alpha(\beta) = \beta - \alpha \in \Delta$, contradicting the previous observation.

Assume now that $\sum_{\alpha \in \Pi(C)} c_\alpha \alpha = 0$ for some $c_\alpha \in \R$. Consider the sets
\begin{align*}
  A = \{ \alpha \in \Pi(C) \mid c_\alpha \geq 0 \}, &&  B = \Pi(C) \bs A,
\end{align*}
and let $v = \sum_{\alpha \in A} c_\alpha \alpha = \sum_{\alpha \in B} (-c_\alpha) \alpha$. Then
$$
(v,v) = \sum_{\alpha \in A, \beta \in B} c_\alpha (-c_\beta) \cdot (\alpha,\beta) \leq 0,
$$
Since the coefficients are positive by definition of $A$ and $B$, and $(\alpha,\beta) \leq 0$ by the above observation. Hence $v=0$. 

Now take any $\beta \in C$. Then
$$
0 = (v,\beta) = \sum_{\alpha \in A} c_\alpha (\alpha, \beta) = \sum_{\alpha \in B} (-c_\alpha)(\alpha,\beta)
$$
The coefficients in the last equality are strictly negative and the dot products are strictly positive (since the $\alpha$s are $C$-positive). Hence we must conclude that $B = \emptyset$. Likewise we conclude that $c_\alpha=0$ for all $\alpha \in A$. Thus $\Pi(C)$ is a basis, and $\Pi(C)$ is a set of simple roots.

Now we have to show that the assignment is a bijection.

Note that for any Weyl chamber $C$, we have
\begin{align*}
  C &= \{ \beta \in V \, \mid \,  (\alpha,\beta) > 0 \text{ for any $C$-positive root } \alpha \} \\
&= \{ \beta \in V \, \mid \, (\alpha,\beta) > 0 \quad  \forall \quad \alpha \in \Pi(C) \}.
\end{align*}
Thus we see that we can recover $C$ from $\Pi(C)$, so $C \mapsto \Pi(C)$ is injective.

Assume now that $\Pi$ is a system of simple roots. As $\Pi$ is a basis, there exists a $\beta_0 \in V$ such that $(\alpha,\beta_0)=1$ for all $\alpha \in \Pi$. Then $(\alpha,\beta_0) \neq 0$ for all $\alpha \in \Delta$, hence $\beta_0$ lies in a uniquely defined Weyl chamber $C$.

Then we claim that $\Pi \subset \Pi(C)$. Indeed, if $\alpha \in \Pi$ is equal to $\alpha=\beta+\gamma$ for some $C$-positive $\beta,\gamma$, then $\beta$ and $\gamma$ are positive with respect to $\Pi$, as $(\beta,\beta_0)  >0$ and $(\gamma, \beta_0) > 0$. 

As $\beta, \gamma$ cannot be proportional to $\alpha$ ($\Delta$ is reduced), decomposing $\beta$ and $\gamma$ as 
$$
\beta = \sum_{\delta \in \Pi} c_\delta' \delta
$$
and 
$$
\gamma = \sum_{\delta \in \Pi} c_{\delta}'' \delta
$$
with $c_\delta',c_\delta'' \geq 0$, we have 
$$
\alpha = \sum_{\delta \in \Pi} (c_\delta' + c_\delta'') \delta,
$$
contradicting the assumption that $\Pi$ is a basis. But $\# \Pi = \# \Pi(C)$, so they must be equal.
\end{proof}

Given a Weyl chamber $C$, we say that a hyperplane $L \subset V$ is a \textbf{wall of $C$} if $L \cap \overline C$ has nonempty interior in $L$ (in the subspace topology). 

Here are som facts that were given as an exercise:
\begin{itemize}
\item For all $\alpha \in \Delta$, the hyperplane $\alpha^\perp$ is a wall of some Weyl chamber.
\item For any Weyl chamber $C$, the walls of $C$ are the hyperplanes $\alpha^\perp$ with $\alpha \in \Pi(C)$. 
\item This implies that any $\alpha \in \Delta$ lies in $\Pi(C)$ for some Weyl chamber $C$.
\end{itemize}

By definition, the \textbf{Weyl group} $W$ of $(V,\Delta)$ is the subgroup of $O(V)$ generated by the reflections $s_\alpha$ for $\alpha \in \Delta$. 

Since the reflections preserve the root system (which is a finite set), the Weyl group is itself finite. Note also that since the Weyl group act on the set of hyperplanes $\alpha^\perp$ for $\alpha \in \Delta$, it also act on the set $C$ of Weyl chambers. 

\begin{thm}
 We have:
 \begin{enumerate}
 \item $W$ acts freely and transitively on the set of Weyl chambers.
\item For any Weil chamber $C$, $W$ is generated by the reflections $s_\alpha$ for $\alpha \in \Pi(C)$. 
 \end{enumerate}
\end{thm}
\begin{proof}[Half-proof:]
 Let $C$ be a Weyl chamber and let $C'$ be an adjacent Weyl chamber, meaning that there exists a hyperplane $L$ such that $L \cap \overline C \cap \overline {C'}$ has nonempty interior in $L$.

Then $L = \alpha^\perp$ for some $\alpha \in \Pi(C)$ and $C' = s_\alpha(C)$. Then $s_\alpha(\Pi(C))=\Pi(C')$. 

It follows that for any $\beta'= s_\alpha(\beta) \in \Pi(C')$ (with $\beta \in \Pi(C)$), we have $$s_{\beta'} = s_{s_\alpha(\beta)} = s_\alpha s_\beta s_\alpha,$$
since for any orthogonal transformation we have $s_{T\alpha} = Ts_\alpha T^{-1}$. 

Thus reflections $s_{\beta'}$ for $\beta' \in \Pi(C')$ lie in the group generated by $s_\alpha$, $\alpha \in \Pi(C)$. 

Now, for any Weyl chamber $C''$ we can find a sequence of adjacent Weyl chambers $C=C_0, \ldots,C_n=C''$. Using the above argument and induction on $n$, we conclude that there exists $w \in W$ such that $w(C)=C''$ and the reflections $s_{\beta''}$ for $\beta'' \in \Pi(C'')$ lie in the group generated by $s_\alpha$ for $\alpha \in \Pi(C)$. 

As any root in $\Delta$ lies in some $\Pi(C'')$, we see that $W$ is generated by $s_\alpha$, $\alpha \in \Delta$.

This proves that the action of $W$ on the set of Weyl chambers is transitive. We will not prove nor need the fact that the action is free. For root spaces coming from compact groups, this will follow from the proof of the next result.
\end{proof}

\begin{example}
 Again consider the root system $A_2$. One computes that the Weyl chambers are the interiors of the rays generated by $\left(\cos( \frac{\pi}{6}+\frac{\pi}{3}k), \sin( \frac{\pi}{6} + \frac{\pi}3 k)\right)$ and $\left(\cos( \frac{\pi}{6}+\frac{\pi}{3}(k+1)), \sin( \frac{\pi}{6} + \frac{\pi}3 (k+1))\right)$ for $k=0,\ldots,5$.

Let $C$ be the Weyl chamber generated by the first two rays. Then one computes that there are three $C$-positive roots: $\alpha_1,\alpha_2,\alpha_3$ (the first one of angle zero, next of angle $60^\circ$, and last of angle $120^\circ$). But $\alpha_2=\alpha_1+\alpha_3$, so we discard it. Thus the $C$-simple roots of $A_2$ are $\alpha_1$ and $\alpha_2$.

We see that the Weyl group is the group $D_6$ of symmetries preserving the hexagon.
\end{example}

\subsection{Root systems of compact Lie groups}

Assume now that $G$ is a compact connected Lie group and $T \subset G$ a maximal torus. Consider the corresponding root system $\Delta$ on $V=i \mathfrak t ^\ast \subset \mathfrak h^\ast$. 

Let $V_0 = Span_\R \Delta$. Then for any $\alpha \in \Delta$, the reflection $s_\alpha$ act trivially on $V_0^\perp$. It follows that the group generated by $s_\alpha$ ($\alpha \in \Delta$) can be identified with the Weyl group $W$ generated by $\restr{s_\alpha}{V_0}$.

We have a homomorphism
$$
r \colon N(T) / T \to O(V)
$$
defined by $r(gT)(\alpha) \alpha \circ (\Ad g^{-1})$.

\begin{thm}
 The homomorphism $r$ defines an isomorphism 
$$
N(T) / T \simeq W,
$$
so that the two definitions of the Weyl group are equivalent.
\end{thm}


\newpage
\section{Cartan matrices and Dynkin diagrams}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\section{Exercises}

\subsection{Lecture 1}
\begin{exc}
Suppose $V_1 \oplus \ldots \oplus V_r \sim W_1 \oplus \cdots \oplus W_s$ as representations and that the $V_i,W_i$ are irreducible. Show that $n=m$ and there exists $\rho \in S_r$ such that $V_i \sim W_{\rho(i)}$ for all $i$.
\end{exc}
\begin{sol}
First note a lemma: If $V \oplus W \sim V' \oplus W'$ and $V \sim V'$, then $W \sim W'$. This follows from the five-lemma.

Now consider the composition $V_1 \hookrightarrow V_1 \oplus \cdots \oplus V_r  \to W_1 \oplus \cdots \oplus W_s \to W_i$. This has to be non-zero for at least one $i$. Hence $V_1 \sim W_i$ for some $i$. By rearring, we are in the situation of the lemma above. Hence, inductively, if $r \leq s$, we find $0 \sim W_{r+1} \oplus \cdots \oplus W_s$, which is impossible. Similarly for $s \leq r$. Hence $r=s$ and we conclude.
\end{sol}

\begin{exc}
 Let $\hat G$ denote the Pontryagin dual of $G$. Then, for any finite abelian group $G$, we have $G \simeq \hat G$.
\end{exc}
\begin{sol}
We first do this when $G= \Z/n$ for some $n \geq 0$. Let $\varphi:\Z/n \to \C^\ast$ be a character. Then $\phi(1)=z$ for some $z$, but $\phi(n)=\phi(0)=z^n=1$, so $z$ must be a $n$'th root of unity.

Realize $\Z/n$ as the $n$th roots of unity. Then we can define a homomorphism $\widehat {\Z/n} \to \Z/n$ by $\varphi \mapsto \varphi(1)$. Similarly, we can define an inverse map by sending $e^{2\pi i/n}$  to the character $m \mapsto e^{2\pi i m /n}$. These are inverses.

Now, every finite abelian group is a product of these groups. So it remains to show (by induction) that if $G,H$ are two finite abelian groups, then $\widehat {G \times H} \simeq \widehat G \times \widehat H$. The inclusion maps $G \to G \times H$ and $H \to G \times H$ induces a map $\widehat G \times \widehat H \to \widehat{G \times H}$ by $(\varphi_1, \varphi_2) \mapsto ((g,h) \mapsto \varphi_1(g)\varphi_2(h))$.

It is easy to see that this map is injective. To see that it is surjective, let $\varphi:G \times H \to \C$ be a character. Write $\varphi$ as $gh \mapsto \varphi(gh)$. Then we can define characters on $G,H$ by $g \mapsto \varphi(g \cdot 1)$ which maps to $\varphi$. Ok.
\end{sol}

\subsection{Exercises 3}

\begin{exc}
Show that the $S$ defined in the proof of Theorem \eqref{thmdensity} lies in $\Theta(G)''$.
\end{exc}
\begin{sol}
 This is, I think, definition-hunting. DETAILS COME LATER
\end{sol}

\begin{exc}
  \begin{enumerate}
  \item Check that indeed 
$$
\langle v, w \rangle := \frac{1}{\lvert G \rvert} \sum_{g \in G} \langle \pi(g) v, \pi(g) w \rangle'
$$
is an invariant Hermitian scalar product.
\item Show that if $\pi$ is irreducible, then an invariant scalar product is unique up to a factor.
  \end{enumerate}
\end{exc}

\begin{sol}
i). Invariant means that $\langle \pi(h) v, \pi(h) w \rangle = \langle v, w \rangle $. This is clear from the definitions, since if $g \in G$ ranges over all of $G$, then so does $gh$.

The only (slightly) nontrivial thing to check is that $\langle,\rangle$ is positive definite. But this is so.


ii) If $\langle,\rangle$ is invariant, then it is an element of $\Hom_G(V \otimes V, \C)$. This is canonically isomorphism to $\Hom_G(V \otimes, V^\ast)$. If we can show that $V$ and $V^\ast$ are isomorphic as representations, then we are done by Schur's lemma. 

But we are given an inner product $\langle , \rangle$. We can define a map $V \to V^\ast$ by sending $v \in V$ to the function $w \mapsto \langle v, w \rangle$. This is clearly a linear isomorphism, and it is also a map of representations. Recall that the action of $G$ on $V^\ast$ is defined by $\pi^\ast(g) \varphi (v) = \varphi(g^{-1} v)$. Then in the diagram

$$
\xymatrix{
V \ar[r] \ar[d]^{\cdot g} & V^\ast \ar[d]^{\cdot g} \\
V \ar[r]  & V^\ast 
}
$$
we want $\langle gv, w \rangle$ to be equal to $\langle v, g^{-1} w \rangle$. But this is true since $\langle ,\rangle$ is invariant:
$$
\langle gv, w \rangle = \langle gv g g^{-1} w \rangle = \langle v, g^{-1}w \rangle.
$$
Hence $V$ and $V^\ast$ are isomorphic as representations and then $\Hom_G(V,V^\ast)$ (space of bilinear forms) is one-dimensional. 
\end{sol}

\begin{exc}
 Let $(V,\pi)$ be an irreducible representation and $(W,\theta)$ a finite-dimensional representation. Let $W(\pi)$ be the isotypic component corresponding to $\pi$.
 \begin{enumerate}
 \item Show that the operator
$$
P = \frac{\dim \pi}{\lvert G \rvert} \sum_{g \in G} \chi_\pi(g^{-1})\theta(g)
$$
on $W$ is the projection onto $W(\pi)$ along $\sum_{\pi' \not \simeq \pi} W(\pi')$. In particular:
$$
W = \bigoplus_{[\pi] \in \widehat G} W(\pi).
$$
\item Show that $\restr{\theta}{W(\pi)} \sim \pi^{n_\pi}$ where $n_\pi = \dim \Mor(\pi,\theta)$.
 \end{enumerate}
\end{exc}

\begin{sol}
dddd
\end{sol}


\subsection{Lecture 4}

\begin{exc}
  \begin{enumerate}
  \item $\pi^{cc}=\pi$.
\item $(pi \otimes \theta)^c = \pi^c \otimes \pi^c$.
\item $\pi$ is irreducible if and only if $\pi^c$ is.
  \end{enumerate}
\end{exc}

\subsection{Exercises 5}

\begin{exc}[Excercise 1]
Show that the ring $\Z$ is integrally closed.
\end{exc}
\begin{sol}
Let $x=p/q \in \Q$ and suppose that we have an equation of integral dependence:
$$
x^n + b_1x^{n-1} + \ldots + b_n = 0
$$
with $b_i \in \Z$. We can suppose that $p$ and $q$ have no common factor. Multiply by $q^n$ on both sides to get
$$
p^n+b_1qp^{n-1} + \ldots+b_nq^n = 0.
$$
Looking modulo $q$ we see that $q \mid p^n$, which to avoid contradiction, must imply that $q=\pm 1$, hence $x \in \Z$.
\end{sol}

\begin{exc}[Exercise 2]
Suppose $R$ is a unital commutative ring and $S \subset R$ a subring with $1 \in S$. Show that the set of elements of $R$ integral over $S$ form a subring of $R$.
\end{exc}
\begin{sol}
 Note that $x$ is integral over $S$ if and only if $S[x]$ is a finitely generated $S$-module. Hence if $x$ and $y$ are integral over $S$, it follows that $S[x] \otimes_S S[y] = S[x,y]$ is a finitely generated $S$-module. Hence $x \pm y$ and $xy$ are integral over $S$
\end{sol}

\subsection{Exercises 11}

\subsection{Lecture 13}
\begin{exc}
 Let $G = \GL_n(\C)$. Show that $\exp:\frak g \to G$ is surjective. Show also that it is not open.
\end{exc}
\begin{sol}
Note that for matrix groups, the exponential is given by
$$
\exp(A) = I + A + \frac 12 A^2 + \frac 16+A^3 + \ldots+\frac{1}{n!}A^n + \ldots
$$
Also note that
$$
\exp(P^{-1}AP) = P^{-1} \exp(A) P
$$
for invertible matrices $P \in \GL_n(\C) \subset \frak g=\End(\C^n)$. Thus, if $A$ is diagonalizable with eigenvalues $\lambda_i$, the matrix $P^{-1}\text{diag}(\log \lambda_1,\ldots,\log \lambda_n)P$ (with $P$ diagonalizing $A$) is mapped to $A$. Here $\log$ is of course the complex logarithm (which is multi-valued, so choose one value for each).

Now if $A$ is not diagonalizable, the matrix $A+\epsilon I$ is invertible and diagonalizable for generic $\epsilon$. Thus we can find a sequence $A_n$ with all $A_n$ diagonalizable and $A_n \to A$ with corresponding $B_n$ with $\exp(B_n)=A_n$. I claim that the $B_n$ can be chosen such that $\{ B_n \}$ is a convergent sequence.  This is because the eigenvalues of $A_n$ are continous with respect to $\epsilon$, and as $\log:\C^\ast \to \C$ is continous, we can choose the logs of close eigenvalues to be close. Thus we get a convergent sequence $B_n \to B$, and by continuity of $\exp$, $\exp(B)=A$.

Now we explain why $\exp$ is not open. [[[HOW????]]
\end{sol}

\subsection{Exercises 13}

\begin{exc}[Exercise 1]

Let $G$ be a group that is also a compact space (that is, a compact Hausdorff topological space) such that the multiplication map $\mu:G \times G \to G$ is continous. Show that $G$ is a compact group, that is, that the inverse map $\iota:G \to G$ is continous as well.
\end{exc}

\begin{sol}
 Let $U \subset G$. Then
 \begin{align*}
   \iota^{-1}(U) &= \{ g \in G \, \mid \, \iota(g) \in U \} \\
&= \{ g \in G \, \mid \, g^{-1} \in U \}  \\
&= \{ g \in G \, \mid \, e \in gU \}
 \end{align*}

[[[[[[[[[ something about bijection between compact spaces is a homeomorphism ]]]]]]]]]
\end{sol}

\begin{exc}
  \begin{enumerate}
  \item Find a Haar measure on $\GL_2(\R)$ and check that this group is unimodular\footnote{Recall that this means that the left and right invariant Haar measures coincide}.
\item Find left- and right-invariant Haar measures on the $ax+b$ group over $\R$, and check that this group is not unimodular.
  \end{enumerate}
\end{exc}
\begin{sol}
For the first part, we start with the usual measure on $\R^4$, and see what happens when we translate by an element of $\GL_2(\R)$. 

We assume the Haar measure has the following form, for some yet unknown function $f$.
$$
\mu(S) = \int_S f(A) \lvert da_{11} d a_{12} da_{21} da a_{22} \rvert.
$$

Now consider the same formula on the set $gS$. Then the change of variable formula tells us that
$$
\mu(gS) = \int_S f(gS) \lvert \det D(g) \rvert \lvert da_{11} d a_{12} da_{21} da a_{22} \rvert.
$$
where $\det D(g)$ is the Jacobian matrix of the map $A \mapsto gA$. A computation shows that this map is row-equivalent to a block diagonal matrix with two copies of $g$. Thus $\det D(g) = (\det g)^2$. Hence a left-invariant Haar measure on $\GL_2(\R)$ is given by
$$
\mu(S) = \int_S \frac{1}{\lvert \det A\rvert ^2} dA.
$$
The same computation with $g \mapsto Ag$ gives the same result. Hence $\GL_2(\R)$ is unimodular.

ii). For the second part, note that the $ax+b$-group is topologically $\R \times \R^\ast$, so that we can use the measure $dxdy$ here. Then a similar computation gives that a left-invariant Haar measure is given by
$$
\mu(S) = \int_S \frac{1}{a^2} da\, db
$$
and a right-invariant Haar measure is given by
$$
\mu(S) = \int_S \frac{1}{a} da\, db.
$$
\end{sol}

\subsection{Exercises 16}

\begin{exc}[Exc 2]
Let $G$ be a topological group and $\Gamma \subset G$ a discrete normal subgroup. Show that $\Gamma \subset Z(G)$.
\end{exc}
\begin{sol}
Since $\Gamma \subset G$ is normal, we have that $h^{-1}gh \in \Gamma$ for all $g \in G$. So we get a continous map $c:G \to \Gamma$ given by conjugation. But a continous map from a connected topological space to a discrete space must be constant (let $\{g_1, g_2\} \subset c(G)$, which is open (in the discrete topology), hence $c^{-1}(\{ g_1,g_2 \})$ is open, which is only possible if  the inverse image is $\emptyset$ or all of $G$). 

Since $c$ is constant it must be constantly equal to $e$, since $c(e)=e$.
\end{sol}

\subsection{Exercises 18}

\begin{exc}[Exc 2]
Let $\pi:G \to \GL(V)$ and $\eta:G \to \GL(W)$ be two finite-dimensional representations of a Lie group $G$. Show that 
$$
(\pi \otimes \eta)_\ast(X) = \pi_\ast(X) \otimes 1 + 1 \otimes \eta_\ast(X)
$$
for all $X \in \frak g$.
\end{exc}

\begin{sol}
We have that $\GL(V \otimes W)$ is an open subset of $\End(V \otimes W)$, so that we may use the additive structure when computing the derivative.

Thus, let $\gamma:\R \to \GL(V \otimes W)$ be a continous curve with $\gamma(0)=e$ and $\gamma'(0)=X$. Then
\begin{align*}
  \pi_\ast(\pi \otimes \eta)(X) &= \dd{}{t}\restr{\left(
\pi(\gamma(t)) \otimes\eta(\gamma(t))
\right)}{t = 0} \\
&= \lim_{t \to 0} \frac 1t \left( \pi(\gamma(t)) \otimes \eta(\gamma(t)) - \pi(\gamma(t)) \otimes 1 + \pi(\gamma(t)) \otimes 1 - 1 \otimes 1 \right)\\
&= 1 \otimes X + X \otimes 1
\end{align*}
by the usual proof of the product rule.
\end{sol}

\subsection{Exercises 19}

\begin{exc}[Exercise 3]
Let $G$ be a closed subgroup of $U(n)$. Verify that the formula
$$
\langle X,Y \rangle = -Tr(XY)
$$  
defines an $\Ad$-invariant scalar product on $\g \subset \mathfrak u(n)$.
\end{exc}
\begin{sol}
We first verify that the formula defines a scalar product. It is clearly symmetric and bilinear. We only need to check that it is positive definite. Note that every $X \in \g$ satisfies $X = -X*$. Hence $\langle X,X \rangle  = -Tr(XX)=-Tr(-X^\ast X)=Tr(XX^\ast)$. An easy computation shows that the $i$th diagonal entry of $XX^\ast$ is the sum of the moduli of the elements in the $i$th row of $X$. Hence $tr(XX^\ast)$ is the sum of the lengths of all the entries in $X$ (with certain repetitions). But this is zero if and only if $X=0$. Hence $\langle X,Y \rangle$ is positive definite.

Recall the adjoint action of $G$ on $\g$, given by $g \cdot X = gXg^{-1}$. Being $\Ad$-invariant means that $\langle g \cdot X, g \cdot Y \rangle = \rangle X,Y \langle$ for all $X,Y \in \g$. But the action on the Lie algebra is given by conjugating, and we have
$$
\langle g \cdot X, \cdot Y \rangle = \langle gXg^{-1},gYg^{-1}\rangle = -\tr(gXg^{-1}gYg^{-1}) = -\tr(gXYg^{-1})=-tr(XY).
$$
And we are done.
\end{sol}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Some worked examples}

\subsection{The representation ring of $S_3$}

We want to explicitly compute the representation ring of $S_3$.

First we find all irreducible representations. The first one is the trivial representation $\epsilon:S_3 \to \C$. We also have the \emph{sign} representation given by $g \mapsto  sgn(g) \in \C $. Both of these are one-dimensional representations.

$S_3$ have a natural action on $\R^3$ given by permuting the basis vectors. But it acts trivially on the subspace spanned by $e_1+e_2+e_3$, hence $\R^3$ decomposes as $\epsilon \oplus V$, where $V$ is some $2$-dimensional representation. It is irreducible: if not, any $v \in V$ would be sent to a scalar multiple of itself, but this is not the case. This representation is called the \textbf{standard representation} of $S_3$. We have also now found all representations of $S_3$, since $1^2+1^2+2^2=6$. 

Thus the representation ring $R(S_3)$ is $\Z[A,S]$ modulo some relations to be found. Here $A$ is the alternating representation and $S$ is the standard representation. The trivial representation is $1$. It is easy to see that $A \otimes A \sim \epsilon$, so $A^2=1 \in R(S_3)$. The representation $S \otimes S$ is $4$-dimensional. To compute how it decomposes, we use character theory.

Recall that characters are class functions on $G$, that is, they only depend on the conjugacy classes of $G$. So we write a character table:

\begin{center}
\begin{tabular}{ r | c c  c }
 & $e$ & $\tau$ & $\sigma$ \\
\hline
$\epsilon$ & 1 & 1 & 1  \\
$A$ & 1 & 1 & -1 \\
$S$ & 2 & -1 & 0 \\
\end{tabular}
\end{center}

To compute the character of the standard representation, we first note that it is, as a vector space given by $\R^3/(1,1,1)$. A basis is then given by the images of $e_1,e_2$. Let $\tau$ be the transposition $(123)$, sending $e_i$ to $e_{i+1}$. Let $\sigma$ be reflection fixing $e_1$ and exchanging $e_2$ and $e_3$. In this basis that means $e_2$ is sent to $-e_1-e_2$. Writing up the corresponding matrices lets us find the value of the character.

Now one can compute by hand (or use a result on characters), that the character of $S \otimes S$ is given by $\chi_S \cdot \chi_S$, so that its entry in the character table is $(4,1,0)$. If $S \otimes S = V_1 \oplus V_2$, then $\chi_S = \chi_{V_1} + \chi_{V_2}$. Using also that the characters are linearly independent, we see that the only option is $S \otimes S \sim \epsilon \oplus A \oplus S$.

Hence $S^2=1+A+S$ in the representation ring. Similarly, we find that $AS=S$ in the representation ring. All in all
$$
R(S_3) = \Z[A,S]/(A^2-1,S^2-1-A-S,AS-S).
$$

\subsection{Explicit Specht modules}

Again, we work with $G=S_3$. Consider the following Young-diagram:
$$
T = \young(12,3)
$$

We want to use the theorem from Lecture 5 (?) to find the standard representation of $S_3$. The elements of $S_3$ are generated by $\rho=(123)$ and $s=(23)$ with the relations $\rho^3=e$ and $s\rho s =\rho^2$. 

First off, the elements permuting the rows is the subgroup consisting of $e$ and $(12)=s\rho^2$. The elements permuting the columns is the subgroup generated by $(13)=s\rho$. Then
$$
a_T = \frac 12 \left(e+(12)\right)
$$
and
$$
b_T = \frac 12 \left( e+(13) \right).
$$
Thus
$$
c_T= \frac 14 \left( e -(13) + (12)-(132) \right).
$$

This gives us as in the lecture a map
$$
\C[S_3] \to V \subseteq \C[S_3]
$$
given by $g \mapsto g c_T$, whose image is supposed to be an irreducible representation of $S_3$. Let's find this. One computes the action of $c_T$ on the basis elements of $\C[S_3]$ by direct computation:
\begin{align*}
  c_T      &= \frac 14 \left( e -s\rho  + s\rho^2-\rho^2 \right) \\
 \rho c_T  &= \frac 14 \left( \rho - s + s\rho -e \right) \\
 \rho^2 c_T &= \frac 14 \left( \rho^2 - s\rho^2 + s -\rho \right) \\
sc_T &= \frac 14 \left( s-\rho + \rho^2-s\rho^2 \right)  \\
s \rho c_T &= \frac 14 \left( s \rho -e+\rho-s \right) \\
s \rho^2 c_T &= \frac 14 \left( s\rho^2-\rho^2+e - s\rho \right) \\
\end{align*}
We claim that $c_T$ and $\rho c_T$ span the image. For we have $\rho^2 c_T = -c_T-\rho c_T$. And $sc_T=\rho^2 c_T$. And $s \rho c_T = \rho c_T$. And $s \rho^2 c_T = c_T$. 

Thus we have some $2$-dimensional representation of $S_3$ with basis $c_T, \rho c_T$. We compute the matrices of $S_3$ with respect to this basis:
\begin{align*}
e =
\begin{pmatrix}
1 & 0 \\ 0 & 1
\end{pmatrix}
&&
\rho = 
\begin{pmatrix}
0 & -1 \\ 1 & -1
\end{pmatrix}
&&
\rho^2 =
\begin{pmatrix}
-1 & 1 \\ -1 & 0
\end{pmatrix} \\
s =
\begin{pmatrix}
-1 & 0 \\ -1 & 1
\end{pmatrix}
&&
s\rho  =
\begin{pmatrix}
0 & 1 \\ 1 & 0
\end{pmatrix}
&&
s\rho^2  =
\begin{pmatrix}
1 & -1 \\ 0 & -1
\end{pmatrix}.
\end{align*}
As expected, the character of this representation is exactly the character of the standard representation from the previous example (as is seen by computing traces). This is no coincidence.


\bibliographystyle{plain}
\bibliography{bibliografi} 

\end{document}